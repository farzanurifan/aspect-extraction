{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk import Tree\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "import json\n",
    "import nltk\n",
    "from nltk.sem.logic import *\n",
    "import requests\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "nltk.download('sentiwordnet')\n",
    "\n",
    "\n",
    "read_expr = nltk.sem.Expression.fromstring\n",
    "nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "dependency_parser = nlp.annotate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Babelfy WSD method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def babelfy(sentence):\n",
    "    response = []\n",
    "    token_word = nltk.word_tokenize(sentence)\n",
    "    url = 'https://babelfy.io/v1/disambiguate?text='+sentence+'&annRes=WN&lang=en&key=76787a80-1771-41d6-8879-2e5064008923'\n",
    "    r = requests.get(url)\n",
    "    \n",
    "    res = r.json()\n",
    "    \n",
    "    for x in res:\n",
    "        tokenFragment = x['tokenFragment']\n",
    "        startTknFragment = tokenFragment['start']\n",
    "        endTknFragment = tokenFragment['end']\n",
    "        babelSynsetID = x['babelSynsetID'];\n",
    "        response.append((token_word[startTknFragment], babel_info(babelSynsetID)))\n",
    "    \n",
    "    return response\n",
    "\n",
    "def babel_info(synset_id):\n",
    "    url = 'https://babelnet.io/v5/getSynset?id='+synset_id+'&key=76787a80-1771-41d6-8879-2e5064008923'\n",
    "    r = requests.get(url)\n",
    "    res = r.json()\n",
    "    return '.'.join(res['mainSense'].split('#'))\n",
    "    \n",
    "\n",
    "def get_score(sentence):\n",
    "    resp = babelfy(sentence)\n",
    "    ss = []\n",
    "    for (w, sy) in resp:\n",
    "        swn_senti = swn.senti_synset(sy)\n",
    "        ss.append((w, sy, swn_senti.pos_score(), swn_senti.neg_score() ))\n",
    "        \n",
    "    return ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intensifier_adverb = ['absolutely', 'completely', 'extremely', 'highly', 'rather', 'really', 'very', 'so', 'too', 'totally', 'utterly', 'at all']\n",
    "negate_adverb = ['no', 'not', 'never', 'none', 'nobody']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser(expression):\n",
    "    old = expression.replace(' ', '_').replace('>_(', '> (').replace(')_(', ') (').replace(')_)', ') )').replace(')_)', ') )').replace(' (', '(')\n",
    "    new = ''\n",
    "    flag = False\n",
    "    for x in range(0, len(old) - 1):        \n",
    "        if old[x] == '<':\n",
    "            flag = True\n",
    "        if old[x] == '>':\n",
    "            flag = False\n",
    "            \n",
    "        if flag == True:\n",
    "            if old[x] == '(':\n",
    "                new += '{'\n",
    "            elif old[x] == ')':\n",
    "                new += '}'\n",
    "            else:\n",
    "                new += old[x]\n",
    "        else:\n",
    "            new += old[x]\n",
    "    new += old[len(old)-1]\n",
    "    return new\n",
    "\n",
    "\n",
    "def pos_tag(sentence):\n",
    "    result = dependency_parser(sentence, properties={\"outputFormat\": \"json\", \"annotators\": \"pos\"})['sentences']\n",
    "\n",
    "    res = []\n",
    "    for iterate in result:\n",
    "        for pos in iterate['tokens']:\n",
    "            res.append(pos['pos'])\n",
    "    return res\n",
    "\n",
    "\n",
    "def insert_pos_tag(exp, pos, nltk_pos):\n",
    "    count = 0\n",
    "    res = ''\n",
    "    for x in range(0, len(exp)):\n",
    "        if exp[x] == 'S' and exp[x+1]==' ' and exp[x+2] == 'P':\n",
    "            res += 'S '\n",
    "            res += pos[count] + ' ' + nltk_pos[count][1]\n",
    "            count += 1\n",
    "            x += 4\n",
    "        else:\n",
    "            res += exp[x]  \n",
    "    return res\n",
    "\n",
    "\n",
    "def direction(exp):\n",
    "    cont = False\n",
    "    for x in exp:\n",
    "        if x == '{':\n",
    "            cont = True\n",
    "        elif x == '}':\n",
    "            cont = False\n",
    "            continue\n",
    "        if cont == True:\n",
    "            continue\n",
    "        if x == '/':\n",
    "            return '/'\n",
    "        elif x == '\\\\':\n",
    "            return '\\\\'\n",
    "    return False\n",
    "\n",
    "\n",
    "def is_type_raising(tree):\n",
    "    tree_string = str(tree)\n",
    "    \n",
    "    # check type raising\n",
    "    exp = tree_string.split('_')[1]\n",
    "    pattern_1 = r'(.*?)\\\\(.*?){(.*?)/(.*?)}'\n",
    "    pattern_2 = r'(.*?)/(.*?){(.*?)\\\\(.*?)}'\n",
    "    \n",
    "    match = False\n",
    "    if re.search(pattern_1, exp):\n",
    "        match = True\n",
    "    elif re.search(pattern_2, exp):\n",
    "        match = True\n",
    "        \n",
    "    sub = []\n",
    "    for subtree in tree:\n",
    "        sub.append(subtree)\n",
    "    if len(sub) == 1 and match:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "    \n",
    "def map_wnpos_to_pennpos(pos):\n",
    "    if(pos == 'n'):\n",
    "        return 'NN'\n",
    "    elif(pos == 'a'):\n",
    "        return 'JJ'\n",
    "    elif(pos == 'v'):\n",
    "        return 'VB'\n",
    "    elif(pos == 'r'):\n",
    "        return 'RB'\n",
    "    \n",
    "    \n",
    "def find_word_in_swn(swn_score, word):\n",
    "    for (wordd, synset, pos_score, neg_score) in swn_score:\n",
    "        if(wordd == word):\n",
    "            return (wordd, synset, pos_score, neg_score)\n",
    "    \n",
    "    \n",
    "def polarity_with_score(pos, neg):\n",
    "    if(pos > neg):\n",
    "        return ('P', round(pos * 10))\n",
    "    elif(pos == neg):\n",
    "        return ('Ne', round(neg*10))\n",
    "    else:\n",
    "        return ('N', 0)\n",
    "    \n",
    "    \n",
    "def adverb_type(word):\n",
    "    if (word in intensifier_adverb):\n",
    "        return 'I'\n",
    "    elif(word in negate_adverb):\n",
    "        return 'Ne'\n",
    "    else:\n",
    "        return 'N'\n",
    "    \n",
    "    \n",
    "def pos_majority_voting(corenlp, nltk, babelfy):\n",
    "    pos = {}\n",
    "    \n",
    "    if(corenlp in pos):\n",
    "        pos[corenlp] += 1\n",
    "    else:\n",
    "        pos[corenlp] = 0  \n",
    "    \n",
    "    if(nltk in pos):\n",
    "        pos[nltk] += 1\n",
    "    else:\n",
    "        pos[nltk] = 0\n",
    "        \n",
    "    if(babelfy in pos):\n",
    "        pos[babelfy] += 1\n",
    "    else:\n",
    "        pos[babelfy] = 0\n",
    "        \n",
    "    #find biggeest counter in pos \n",
    "    return sorted(pos.items(), key=lambda x: x[1], reverse=True)[0][0];\n",
    "\n",
    "\n",
    "def chunk(tree):\n",
    "    # ini cuma masukin ke array \n",
    "    chunk = []\n",
    "    chunk3 = []\n",
    "    \n",
    "    for subtree in tree:\n",
    "        if type(subtree) == nltk.tree.Tree:\n",
    "            \n",
    "            # chunk temp array\n",
    "            subtree_str_array = str(subtree).split('_')\n",
    "            if subtree_str_array[0][2] == 'L':\n",
    "                if subtree_str_array[3] == 'NN':\n",
    "                    chunk.append(subtree_str_array[6])\n",
    "                    if len(chunk3) == 0:\n",
    "                        chunk3.append(subtree_str_array[6])\n",
    "                    \n",
    "            if len(chunk3) == 1:\n",
    "                for sub in subtree:\n",
    "                    # chunk temp array\n",
    "                    subtree_str_array3 = str(sub).split('_')\n",
    "                    if subtree_str_array3[0][2] == 'L':\n",
    "                        if subtree_str_array3[3] == 'NN':\n",
    "                            chunk3.append(subtree_str_array3[6])\n",
    "    \n",
    "    # chunk noun phrase\n",
    "    if len(chunk) == 2:\n",
    "        chunk_str = '+'.join(chunk)\n",
    "        return True, read_expr(r'(' + chunk_str + '_NN_Ne_0)')\n",
    "    \n",
    "    if len(chunk3) == 3:\n",
    "        chunk_str = '+'.join(chunk3)\n",
    "        return True, read_expr(r'(' + chunk_str + '_NN_Ne_0)')\n",
    "    \n",
    "    return False, None\n",
    "\n",
    "\n",
    "def one_child(tree, swn_score):    \n",
    "    sub = []\n",
    "    for subtree in tree:\n",
    "        if type(subtree) == nltk.tree.Tree:\n",
    "            sub.append(subtree)\n",
    "    if len(sub) == 1:\n",
    "        return True, lambda_calculus(sub[0], swn_score), sub\n",
    "    \n",
    "    return False, None, sub\n",
    "\n",
    "\n",
    "def type_raising(first, second, swn_score):    \n",
    "    if is_type_raising(first):\n",
    "        if direction(str(first).split('_')[1]) == '/':\n",
    "            x = read_expr(r'\\F x.F(x, ' + str(lambda_calculus(first, swn_score)) + ')')\n",
    "            y = lambda_calculus(second, swn_score)\n",
    "            return True, ApplicationExpression(x, y).simplify()\n",
    "    \n",
    "    if is_type_raising(second):\n",
    "        if direction(str(second).split('_')[1]) == '/':\n",
    "            x = lambda_calculus(first, swn_score)\n",
    "            y = read_expr(r'\\F x.F(x, ' + str(lambda_calculus(second, swn_score)) + ')')\n",
    "            return True, ApplicationExpression(x, y).simplify()\n",
    "        \n",
    "    return False, None\n",
    "    \n",
    "\n",
    "def rule_var(ccg, corenlp_pos, nltk_pos, word, swn_score):\n",
    "    r_word = ['PRP', 'FW', 'NN', 'LS']\n",
    "    word_swn_score = find_word_in_swn(swn_score, word)\n",
    "    pos_score = 0\n",
    "    neg_score = 0\n",
    "    babelfy_pos = None;\n",
    "\n",
    "    if(word_swn_score):\n",
    "        word, synset, pos_score, neg_score = word_swn_score\n",
    "        splitted_syns = synset.split('.');\n",
    "        babelfy_pos = map_wnpos_to_pennpos(splitted_syns[1]);\n",
    "    polarity, score = polarity_with_score(pos_score, neg_score)\n",
    "    pos = pos_majority_voting(corenlp_pos, nltk_pos, babelfy_pos)\n",
    "    \n",
    "    return rule_leaf(ccg, r_word, pos, word, polarity, score)\n",
    "\n",
    "\n",
    "def rule_leaf(ccg, r_word, pos, word, polarity, score):\n",
    "    if pos == 'CC':\n",
    "        return read_expr(r'CC')\n",
    "    elif pos in r_word:\n",
    "        return read_expr(word + '_' + pos + '_' + polarity + '_' + str(score))\n",
    "    elif 'JJ' in pos:\n",
    "        return read_expr(word + '_' + 'JJ' + '_' + polarity + '_' + str(score))\n",
    "    elif 'VB' in pos:\n",
    "        if '{S[dcl]\\\\NP}/{S[adj]\\\\NP}' in ccg:\n",
    "            return read_expr(r'\\x.x')\n",
    "        else:\n",
    "            return read_expr(r'\\X.' + word + '_' + 'VB' + '_' + polarity + '_' + str(score) +'(X)')\n",
    "    elif 'RB' in pos:\n",
    "        #Adverb has three types. I: Intensifier, Ne: Negation, N: no affection toward sentiment\n",
    "        return read_expr(word + '_' + 'RB' + '_' + adverb_type(word))\n",
    "    elif word == 'of':\n",
    "        return read_expr('of')\n",
    "    else:\n",
    "        return read_expr(r'\\x.x')\n",
    "\n",
    "    \n",
    "def lambda_calculus(tree, swn_score):\n",
    "    tree_string = str(tree)\n",
    "    \n",
    "    # leaf\n",
    "    if tree_string[2] == 'L':\n",
    "        splitted = tree_string.split('_')\n",
    "        ccg = splitted[1]\n",
    "        corenlp_pos = splitted[3]\n",
    "        nltk_pos = splitted[4]\n",
    "        word = splitted[6]\n",
    "        \n",
    "        return rule_var(ccg, corenlp_pos, nltk_pos, word, swn_score)\n",
    "    \n",
    "    # chunk noun phrase\n",
    "    is_true, res = chunk(tree)\n",
    "    if is_true:\n",
    "        return res\n",
    "            \n",
    "    # anak 1\n",
    "    is_true, res, sub = one_child(tree, swn_score)\n",
    "    if is_true:\n",
    "        return res\n",
    "                        \n",
    "    # urutan operasi lambda calculusnya    \n",
    "    first = sub[0]\n",
    "    second = sub[1]\n",
    "    \n",
    "    # type raising\n",
    "    is_true, res = type_raising(first, second, swn_score)\n",
    "    if is_true:\n",
    "        return res    \n",
    "    \n",
    "    # urutan\n",
    "    length_1 = len(str(sub[0]).split('_')[1].replace('\\\\', '/').split('/'))\n",
    "    length_2 = len(str(sub[1]).split('_')[1].replace('\\\\', '/').split('/'))\n",
    "    if length_2 > length_1:\n",
    "        first = sub[1]\n",
    "        second = sub[0]\n",
    "    \n",
    "    # rekursi\n",
    "    return deduction(lambda_calculus(first, swn_score), lambda_calculus(second, swn_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bool_var(str_a, str_b):\n",
    "    is_adj_in_a = re.search(r'JJ.*', str_a)\n",
    "    is_adj_in_b = re.search(r'JJ.*', str_b)\n",
    "    \n",
    "    is_adverb_in_a = re.search(r'RB.*', str_a)\n",
    "    is_adverb_in_b = re.search(r'RB.*', str_b)    \n",
    "    \n",
    "    is_noun_in_b = re.search(r'\\w*?\\+?\\w*?\\+?\\w*_NN_\\w*', str_b)\n",
    "    is_verb_in_b = re.search(r'VB_\\w*_\\d*', str_b)\n",
    "    \n",
    "    return is_adj_in_a, is_adj_in_b, is_adverb_in_a, is_adverb_in_b, is_noun_in_b, is_verb_in_b\n",
    "\n",
    "\n",
    "def rule_and(str_a, str_b, is_noun_in_b, is_adj_in_a):\n",
    "    a, b = None, None\n",
    "    if (str_b == 'CC'):\n",
    "        a = read_expr(str_b)\n",
    "        b = read_expr(str_a)\n",
    "        return True, a, b\n",
    "        \n",
    "    if (re.search(r'CC\\(', str_a)) and (re.search(r',', str_a)) and is_noun_in_b and is_adj_in_a:\n",
    "        a = read_expr(r'\\x.x')\n",
    "        b = read_expr(replacer(str_a, str_b))\n",
    "        return True, a, b\n",
    "    \n",
    "    return False, None, None\n",
    "\n",
    "\n",
    "def rule_seq(str_a, str_b):\n",
    "    a, b = None, None\n",
    "    r_word = ['PRP', 'FW', 'NN', 'LS', 'JJ']\n",
    "    if '(' not in str_a and '(' not in str_b:\n",
    "        x = False\n",
    "        y = False\n",
    "        for r in r_word:\n",
    "            if r in str_a:\n",
    "                x = True\n",
    "            if r in str_b:\n",
    "                y = True\n",
    "\n",
    "        if x and y:\n",
    "            a = read_expr(r'\\x.x')\n",
    "            b = read_expr('seq(' + str_a + ',' + str_b + ')')\n",
    "            return True, a, b\n",
    "    \n",
    "    return False, None, None\n",
    "\n",
    "\n",
    "def ini_fungsi_mas_ari_tolong_dipecah_lagi(str_a, str_b, is_adj_in_a, is_adj_in_b, is_adverb_in_a, is_adverb_in_b, is_noun_in_b, is_verb_in_b):\n",
    "    a, b = None, None\n",
    "    \n",
    "    if is_adverb_in_a and (is_noun_in_b and (not is_verb_in_b)):\n",
    "        print(is_adverb_in_a)\n",
    "        print(is_noun_in_b)\n",
    "        print(is_verb_in_b)\n",
    "        a = read_expr(r'\\x.x')\n",
    "        b = read_expr(r'\\x.x')\n",
    "        return True, a, b\n",
    "\n",
    "#     elif(is_adj_in_a and is_noun_in_b):\n",
    "# #         adjective_score = str_a.split('_')[3]\n",
    "# #         sentiment_polarity = str_a.split('_')[2]\n",
    "#         pattern = '(\\w*_JJ_\\w_\\d)'\n",
    "#         adjective_score = re.findall(pattern, str_a)[0].split('_')[3]\n",
    "#         sentiment_polarity = re.findall(pattern, str_a)[0].split('_')[2]\n",
    "#         print(adjective_score, sentiment_polarity)\n",
    "#         #get noun\n",
    "#         noun_str_b = str_b[ is_noun_in_b.start() : is_noun_in_b.end() ]\n",
    "#         #change sentiment and polarity\n",
    "#         def mapFunction(data):\n",
    "#             idx, x = data\n",
    "#             if(idx == 2):\n",
    "#                 return sentiment_polarity\n",
    "#             elif(idx == 3):\n",
    "#                 return adjective_score\n",
    "#             else:\n",
    "#                 return x\n",
    "            \n",
    "#         noun_update_str_b = '_'.join( list(map(mapFunction, enumerate(noun_str_b.split('_')))) )\n",
    "#         #change str_b for noun filtered with x\n",
    "#         list_str_b = list(str_b)\n",
    "#         list_str_b[is_noun_in_b.start() : is_noun_in_b.end()] = 'x'\n",
    "#         str_b = \"\".join(list_str_b)\n",
    "#         print(str_b, noun_update_str_b, is_noun_in_b)\n",
    "#         str_b = str(ApplicationExpression(read_expr(r\"\\x.\" + str_b), read_expr(noun_update_str_b)).simplify())\n",
    "        \n",
    "#         a = read_expr(r'\\x.x')\n",
    "#         b = read_expr(str_b)\n",
    "        \n",
    "#         return True, a, b\n",
    "    \n",
    "    #change identity function for adverb.\n",
    "    elif( is_adverb_in_a and is_adj_in_b or is_adverb_in_a and is_verb_in_b ):\n",
    "        #adverb modify adjective\n",
    "        #example very_RB_I excellent_JJ_P_10 = excellent_JJ_P_20\n",
    "        #I Intensifier must *2\n",
    "        #N Negate must *-1\n",
    "        adverb_type = str_a.split('_')[2]\n",
    "        if(adverb_type == 'N'):\n",
    "            a = read_expr(r'\\x.x')\n",
    "            b = read_expr(str_b)\n",
    "        elif(adverb_type == 'Ne'):\n",
    "            #for negation adverb just change polairty\n",
    "            def mapFunction(data):\n",
    "                idx, x = data\n",
    "                if(idx == 2):\n",
    "                    if(x == 'P'):\n",
    "                        return 'Ne'\n",
    "                    elif(x == 'Ne'):\n",
    "                        return 'P'\n",
    "                    else:\n",
    "                        return x\n",
    "                else:\n",
    "                    return x\n",
    "\n",
    "            str_b = '_'.join( list(map(mapFunction, enumerate(str_b.split('_')))) )\n",
    "            a = read_expr(r'\\x.x')\n",
    "            b = read_expr(str_b)\n",
    "            \n",
    "        elif(adverb_type == 'I'):\n",
    "            #for intensifier adverb. scale adjective value \n",
    "            \n",
    "            # ini bener?\n",
    "            jj_s = re.findall('JJ_\\w*_\\d*', str_b)\n",
    "            vb_s = re.findall('VB_\\w*_\\d*', str_b)\n",
    "            score = '0'\n",
    "            \n",
    "            if jj_s:\n",
    "                score = jj_s[0].split('_')[2]\n",
    "            else:\n",
    "                score = vb_s[0].split('_')[2]\n",
    "                \n",
    "            def mapFunction(data):\n",
    "                if(data == score):\n",
    "                    return str(int(score) * 2);\n",
    "                else:\n",
    "                    return data;\n",
    "            list_str_b = list(str_b)\n",
    "            str_b = ''.join( list(map(mapFunction, list_str_b)))\n",
    "#             print(str_b)\n",
    "            a = read_expr(r'\\x.x')\n",
    "            b = read_expr(str_b)\n",
    "\n",
    "        return True, a, b\n",
    "        \n",
    "#     elif( is_adverb_in_a and is_adverb_in_b):\n",
    "#         #adverb modify other adverb\n",
    "#         #kondisi yang jarang bertemu\n",
    "#         pass\n",
    "\n",
    "    return False, None, None\n",
    "\n",
    "\n",
    "def deduction(a, b):\n",
    "    str_a = str(a)\n",
    "    str_b = str(b)\n",
    "    print('old a ' + str_a)\n",
    "    print('old b ' + str_b)\n",
    "    \n",
    "    if str_a == 'of':\n",
    "        a = read_expr(r'\\x.x')\n",
    "        b = read_expr(r'\\x.x')\n",
    "    \n",
    "    #change identity function for adjective    \n",
    "    is_adj_in_a, is_adj_in_b, is_adverb_in_a, is_adverb_in_b, is_noun_in_b, is_verb_in_b = bool_var(str_a, str_b)\n",
    "    \n",
    "    # fungsi mas ari\n",
    "    is_true, a_temp, b_temp = ini_fungsi_mas_ari_tolong_dipecah_lagi(str_a, str_b, is_adj_in_a, is_adj_in_b, is_adverb_in_a, is_adverb_in_b, is_noun_in_b, is_verb_in_b)\n",
    "    if is_true:\n",
    "        a = a_temp\n",
    "        b = b_temp\n",
    "    \n",
    "    # and\n",
    "    is_true, a_temp, b_temp = rule_and(str_a, str_b, is_noun_in_b, is_adj_in_a)\n",
    "    if is_true:\n",
    "        a = a_temp\n",
    "        b = b_temp\n",
    "                \n",
    "    # sequence\n",
    "    is_true, a_temp, b_temp = rule_seq(str_a, str_b)\n",
    "    if is_true:\n",
    "        a = a_temp\n",
    "        b = b_temp  \n",
    "    \n",
    "    str_a = str(a)\n",
    "    str_b = str(b)\n",
    "    print('new a ' + str_a)\n",
    "    print('new b ' + str_b)\n",
    "    print('hasil ' + str(ApplicationExpression(a, b).simplify()))\n",
    "    print()\n",
    "    return ApplicationExpression(a, b).simplify()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replacer(a, b):\n",
    "    print(a,b)\n",
    "    pattern = '(\\w*?\\+?\\w*?\\+?\\w*_NN_)'\n",
    "    mereplace = re.findall(pattern, b)\n",
    "    res = ''\n",
    "    for mreplace in mereplace:\n",
    "        pattern = '(\\w*_JJ_)'\n",
    "        hasil = re.findall(pattern, a)\n",
    "        if hasil:\n",
    "            res = a.replace(hasil[0], mreplace)\n",
    "        if len(hasil) > 1:\n",
    "            for index in range(1, len(hasil)):\n",
    "                res = res.replace(hasil[index], mreplace)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glue_process(sent):\n",
    "    url = \"http://localhost:5000/ccgParsing\"\n",
    "    data = {\"sent\": sent}\n",
    "    r = requests.post(url, data=data)\n",
    "\n",
    "    res = r.json()\n",
    "    \n",
    "    from_res = res['tree']\n",
    "    \n",
    "    text = nltk.word_tokenize(sent)\n",
    "    nltk_pos = nltk.pos_tag(text)\n",
    "    \n",
    "    pos_tagged = insert_pos_tag(from_res, pos_tag(data['sent']), nltk_pos)\n",
    "\n",
    "    hasil = parser(pos_tagged)\n",
    "\n",
    "    tree = Tree.fromstring(hasil)\n",
    "    swn_score = get_score(sent)\n",
    "    \n",
    "#     print(tree)\n",
    "    return lambda_calculus(tree, swn_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glue_process('the breakfast that the restaurant served daily was excellent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_process('i bought my canon g3 about a month ago and i have to say i am very satisfied')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_process(\"The bowl of squid eyeball stew is hot and delicious\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = '(\\w*?\\+?\\w*_NN_[P|N]_\\d)'\n",
    "re.findall(pattern, 'CC(bowl_NN_P_8,bowl_NN_N_0)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def semua(collection):\n",
    "    df = pd.DataFrame(columns=['sentence', 'lambda', 'raw_aspect', 'aspect', 'sentiment'])\n",
    "    for data in collection:\n",
    "        hasil = glue_process(data)\n",
    "        \n",
    "        pattern = '(\\w*?\\+?\\w*_NN_[P|N]_\\d)'\n",
    "        aspek = re.findall(pattern, str(hasil))\n",
    "        aspect = []\n",
    "        sentiment = []\n",
    "        temp = ''\n",
    "        for asp in aspek:\n",
    "            temp = asp.split('_')\n",
    "            aspect.append(temp[0].replace('+', ' '))\n",
    "            sentiment.append(1 if temp[2] == 'P' else 0)\n",
    "            \n",
    "        df = df.append({'sentence': data, 'lambda': hasil,'raw_aspect': aspek, 'aspect': aspect, 'sentiment': sentiment}, ignore_index=True)\n",
    "    df.to_csv('hasil_ccg.csv')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# input file\n",
    "df = pd.read_csv(\"dataset.csv\")\n",
    "# preprocess\n",
    "sentences = df['review']\n",
    "labels = df[\"target\"]\n",
    "\n",
    "semua(sentences[4:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "glue_process('i recently purchased the canon powershot g3 and am extremely satisfied with the purchase')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = re.search(r'VB_\\w*_\\d*', 'purchased_VB_Ne_0(canon+powershot+g3_NN_N_0)')\n",
    "print(p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
