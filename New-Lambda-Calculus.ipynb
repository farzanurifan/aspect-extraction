{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package sentiwordnet to C:\\Users\\Farza\n",
      "[nltk_data]     Nurifan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk import Tree\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "import json\n",
    "import nltk\n",
    "from nltk.sem.logic import *\n",
    "import requests\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "nltk.download('sentiwordnet')\n",
    "\n",
    "\n",
    "\n",
    "read_expr = nltk.sem.Expression.fromstring\n",
    "nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "dependency_parser = nlp.annotate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Babelfy WSD method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def babelfy(sentence):\n",
    "    response = []\n",
    "    token_word = nltk.word_tokenize(sentence)\n",
    "    url = 'https://babelfy.io/v1/disambiguate?text='+sentence+'&annRes=WN&lang=en&key=76787a80-1771-41d6-8879-2e5064008923'\n",
    "    r = requests.get(url)\n",
    "    \n",
    "    res = r.json()\n",
    "    \n",
    "    for x in res:\n",
    "        tokenFragment = x['tokenFragment']\n",
    "        startTknFragment = tokenFragment['start']\n",
    "        endTknFragment = tokenFragment['end']\n",
    "        babelSynsetID = x['babelSynsetID'];\n",
    "        response.append((token_word[startTknFragment], babel_info(babelSynsetID)))\n",
    "    \n",
    "    return response\n",
    "\n",
    "def babel_info(synset_id):\n",
    "    url = 'https://babelnet.io/v5/getSynset?id='+synset_id+'&key=76787a80-1771-41d6-8879-2e5064008923'\n",
    "    r = requests.get(url)\n",
    "    res = r.json()\n",
    "    return '.'.join(res['mainSense'].split('#'))\n",
    "    \n",
    "\n",
    "def get_score(sentence):\n",
    "    resp = babelfy(sentence)\n",
    "    ss = []\n",
    "    for (w, sy) in resp:\n",
    "        swn_senti = swn.senti_synset(sy)\n",
    "        ss.append((w, sy, swn_senti.pos_score(), swn_senti.neg_score() ))\n",
    "        \n",
    "    return ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "intensifier_adverb = ['absolutely', 'completely', 'extremely', 'highly', 'rather', 'really', 'very', 'so', 'too', 'totally', 'utterly', 'at all']\n",
    "negate_adverb = ['no', 'not', 'never', 'none', 'nobody']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser(expression):\n",
    "    old = expression.replace(' ', '_').replace('>_(', '> (').replace(')_(', ') (').replace(')_)', ') )').replace(')_)', ') )').replace(' (', '(')\n",
    "    new = ''\n",
    "    flag = False\n",
    "    for x in range(0, len(old) - 1):        \n",
    "        if old[x] == '<':\n",
    "            flag = True\n",
    "        if old[x] == '>':\n",
    "            flag = False\n",
    "            \n",
    "        if flag == True:\n",
    "            if old[x] == '(':\n",
    "                new += '{'\n",
    "            elif old[x] == ')':\n",
    "                new += '}'\n",
    "            else:\n",
    "                new += old[x]\n",
    "        else:\n",
    "            new += old[x]\n",
    "    new += old[len(old)-1]\n",
    "    return new\n",
    "\n",
    "def pos_tag(sentence):\n",
    "    result = dependency_parser(sentence, properties={\"outputFormat\": \"json\", \"annotators\": \"pos\"})['sentences'][0]['tokens']\n",
    "    res = []\n",
    "    for pos in result:\n",
    "        res.append(pos['pos'])\n",
    "    return res\n",
    "\n",
    "\n",
    "def insert_pos_tag(exp, pos, nltk_pos):\n",
    "    count = 0\n",
    "    res = ''\n",
    "    for x in range(0, len(exp)):\n",
    "        if exp[x] == 'S' and exp[x+1]==' ' and exp[x+2] == 'P':\n",
    "            res += 'S '\n",
    "            res += pos[count] + ' ' + nltk_pos[count][1]\n",
    "            count += 1\n",
    "            x += 4\n",
    "        else:\n",
    "            res += exp[x]  \n",
    "    return res\n",
    "\n",
    "\n",
    "def direction(exp):\n",
    "    cont = False\n",
    "    for x in exp:\n",
    "        if x == '{':\n",
    "            cont = True\n",
    "        elif x == '}':\n",
    "            cont = False\n",
    "            continue\n",
    "        if cont == True:\n",
    "            continue\n",
    "        if x == '/':\n",
    "            return '/'\n",
    "        elif x == '\\\\':\n",
    "            return '\\\\'\n",
    "    return False\n",
    "\n",
    "\n",
    "def is_type_raising(tree):\n",
    "    tree_string = str(tree)\n",
    "    \n",
    "    # check type raising\n",
    "    exp = tree_string.split('_')[1]\n",
    "    pattern_1 = r'(.*?)\\\\(.*?){(.*?)/(.*?)}'\n",
    "    pattern_2 = r'(.*?)/(.*?){(.*?)\\\\(.*?)}'\n",
    "    \n",
    "    match = False\n",
    "    if re.search(pattern_1, exp):\n",
    "        match = True\n",
    "    elif re.search(pattern_2, exp):\n",
    "        match = True\n",
    "        \n",
    "    sub = []\n",
    "    for subtree in tree:\n",
    "        sub.append(subtree)\n",
    "    if len(sub) == 1 and match:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def map_wnpos_to_pennpos(pos):\n",
    "    if(pos == 'n'):\n",
    "        return 'NN'\n",
    "    elif(pos == 'a'):\n",
    "        return 'JJ'\n",
    "    elif(pos == 'v'):\n",
    "        return 'VB'\n",
    "    elif(pos == 'r'):\n",
    "        return 'RB'\n",
    "    \n",
    "def find_word_in_swn(swn_score, word):\n",
    "    for (wordd, synset, pos_score, neg_score) in swn_score:\n",
    "        if(wordd == word):\n",
    "            return (wordd, synset, pos_score, neg_score)\n",
    "        \n",
    "def polarity_with_score(pos, neg):\n",
    "    if(pos > neg):\n",
    "        return ('P', round(pos * 10))\n",
    "    elif(pos == neg):\n",
    "        return ('Ne', round(neg*10))\n",
    "    else:\n",
    "        return ('N', 0)\n",
    "    \n",
    "def adverb_type(word):\n",
    "    if (word in intensifier_adverb):\n",
    "        return 'I'\n",
    "    elif(word in negate_adverb):\n",
    "        return 'Ne'\n",
    "    else:\n",
    "        return 'N'\n",
    "    \n",
    "def pos_majority_voting(corenlp, nltk, babelfy):\n",
    "    pos = {}\n",
    "    \n",
    "    if(corenlp in pos):\n",
    "        pos[corenlp] += 1\n",
    "    else:\n",
    "        pos[corenlp] = 0\n",
    "        \n",
    "    \n",
    "    if(nltk in pos):\n",
    "        pos[nltk] += 1\n",
    "    else:\n",
    "        pos[nltk] = 0\n",
    "    \n",
    "    \n",
    "    if(babelfy in pos):\n",
    "        pos[babelfy] += 1\n",
    "    else:\n",
    "        pos[babelfy] = 0\n",
    "        \n",
    "        \n",
    "    #find biggeest counter in pos \n",
    "    return sorted(pos.items(), key=lambda x: x[1], reverse=True)[0][0];\n",
    "\n",
    "    \n",
    "def lambda_calculus(tree, swn_score):\n",
    "    tree_string = str(tree)\n",
    "    \n",
    "    if tree_string[2] == 'L':\n",
    "        corenlp_pos = tree_string.split('_')[3]\n",
    "        nltk_pos = tree_string.split('_')[4]\n",
    "        word = tree_string.split('_')[6]\n",
    "        #                        #\n",
    "        # masukin rulenya disini #\n",
    "        #                        #\n",
    "        r_word = ['PRP', 'FW', 'NN', 'LS']\n",
    "        word_swn_score = find_word_in_swn(swn_score, word)\n",
    "        pos_score = 0\n",
    "        neg_score = 0\n",
    "        \n",
    "        babelfy_pos = None;\n",
    "        \n",
    "        if(word_swn_score):\n",
    "            word, synset, pos_score, neg_score = word_swn_score\n",
    "            splitted_syns = synset.split('.');\n",
    "            babelfy_pos = map_wnpos_to_pennpos(splitted_syns[1]);\n",
    "            \n",
    "        \n",
    "        polarity, score = polarity_with_score(pos_score, neg_score)\n",
    "        \n",
    "        pos = pos_majority_voting(corenlp_pos, nltk_pos, babelfy_pos)\n",
    "            \n",
    "        if pos == 'CC':\n",
    "            return read_expr(r'CC')\n",
    "        elif pos in r_word:\n",
    "            return read_expr(word + '_' + pos + '_' + polarity + '_' + str(score))\n",
    "        elif 'JJ' in pos:\n",
    "            return read_expr(word + '_' + 'JJ' + '_' + polarity + '_' + str(score))\n",
    "        elif 'VB' in pos:\n",
    "            if '{S[dcl]\\\\NP}/{S[adj]\\\\NP}' in tree_string.split('_')[1]:\n",
    "                return read_expr(r'\\x.x')\n",
    "            else:\n",
    "                return read_expr(r'\\X.' + word + '_' + 'VB' + '_' + polarity + '_' + str(score) +'(X)')\n",
    "        elif 'RB' in pos:\n",
    "            #Adverb has three types. I: Intensifier, Ne: Negation, N: no affection toward sentiment\n",
    "            return read_expr(word + '_' + 'RB' + '_' + adverb_type(word))\n",
    "        elif 'IN' in pos:\n",
    "            return read_expr('of')\n",
    "        else:\n",
    "            print(word, tree_string, '--')\n",
    "            return read_expr(r'\\x.x')\n",
    "    \n",
    "    # ini cuma masukin ke array \n",
    "    chunk = []\n",
    "    chunk3 = []\n",
    "    sub = []\n",
    "    \n",
    "    for subtree in tree:\n",
    "        if type(subtree) == nltk.tree.Tree:\n",
    "            sub.append(subtree)\n",
    "            \n",
    "            # chunk temp array\n",
    "            subtree_str_array = str(subtree).split('_')\n",
    "            if subtree_str_array[0][2] == 'L':\n",
    "                if subtree_str_array[3] == 'NN':\n",
    "                    chunk.append(subtree_str_array[6])\n",
    "                    if len(chunk3) == 0:\n",
    "                        chunk3.append(subtree_str_array[6])\n",
    "                    \n",
    "            if len(chunk3) == 1:\n",
    "                for sub in subtree:\n",
    "                    # chunk temp array\n",
    "                    subtree_str_array3 = str(sub).split('_')\n",
    "                    if subtree_str_array3[0][2] == 'L':\n",
    "                        if subtree_str_array3[3] == 'NN':\n",
    "                            chunk3.append(subtree_str_array3[6])\n",
    "    \n",
    "    \n",
    "    # chunk noun phrase\n",
    "    if len(chunk) == 2:\n",
    "        chunk_str = '+'.join(chunk)\n",
    "        return read_expr(r'(' + chunk_str + '_NN_N_0)')\n",
    "    \n",
    "    if len(chunk3) == 3:\n",
    "        chunk_str = '+'.join(chunk3)\n",
    "        return read_expr(r'(' + chunk_str + '_NN_N_0)')\n",
    "    \n",
    "    # error anak 1\n",
    "    if len(sub) == 1:            \n",
    "        return lambda_calculus(sub[0], swn_score)\n",
    "                        \n",
    "    # urutan operasi lambda calculusnya    \n",
    "    first = sub[0]\n",
    "    second = sub[1]\n",
    "    \n",
    "    if is_type_raising(first):\n",
    "        if direction(str(first).split('_')[1]) == '/':\n",
    "            x = read_expr(r'\\F x.F(x, ' + str(lambda_calculus(first, swn_score)) + ')')\n",
    "            y = lambda_calculus(second, swn_score)\n",
    "            return ApplicationExpression(x, y).simplify()\n",
    "    \n",
    "    if is_type_raising(second):\n",
    "        if direction(str(second).split('_')[1]) == '/':\n",
    "            x = lambda_calculus(first, swn_score)\n",
    "            y = read_expr(r'\\F x.F(x, ' + str(lambda_calculus(second, swn_score)) + ')')\n",
    "            return ApplicationExpression(x, y).simplify()\n",
    "    \n",
    "        \n",
    "    length_1 = len(str(sub[0]).split('_')[1].replace('\\\\', '/').split('/'))\n",
    "    length_2 = len(str(sub[1]).split('_')[1].replace('\\\\', '/').split('/'))\n",
    "    if length_2 > length_1:\n",
    "        first = sub[1]\n",
    "        second = sub[0]\n",
    "    # rekursi\n",
    "    return deduction(lambda_calculus(first, swn_score), lambda_calculus(second, swn_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deduction(a, b):\n",
    "    str_a = str(a)\n",
    "    str_b = str(b)\n",
    "    print('old a ' + str_a)\n",
    "    print('old b ' + str_b)\n",
    "    \n",
    "    if str_a == 'of':\n",
    "        a = read_expr(r'\\x.x')\n",
    "        b = read_expr(r'\\x.x')\n",
    "    \n",
    "    #change identity function for adjective\n",
    "    is_adj_exist_in_a = re.search(r'JJ.*', str_a)\n",
    "    is_adj_exist_in_b = re.search(r'JJ.*', str_b)\n",
    "    \n",
    "    is_adverb_exist_in_a = re.search(r'RB.*', str_a)\n",
    "    is_adverb_exist_in_b = re.search(r'RB.*', str_b)\n",
    "    \n",
    "    \n",
    "    is_noun_exist_in_b = re.search(r'\\w*?\\+?\\w*?\\+?\\w*_NN_\\w*', str_b)\n",
    "    is_verb_exist_in_b = re.search(r'VB.*', str_b)\n",
    "    \n",
    "    \n",
    "    if is_adverb_exist_in_a and is_noun_exist_in_b:\n",
    "        a = read_expr(r'\\x.x')\n",
    "        b = read_expr(r'\\x.x')\n",
    "\n",
    "    elif( is_adj_exist_in_a and is_noun_exist_in_b):\n",
    "#         adjective_score = str_a.split('_')[3]\n",
    "#         sentiment_polarity = str_a.split('_')[2]\n",
    "        pattern = '(\\w*_JJ_\\w_\\d)'\n",
    "        adjective_score = re.findall(pattern, str_a)[0].split('_')[3]\n",
    "        sentiment_polarity = re.findall(pattern, str_a)[0].split('_')[2]\n",
    "        print(adjective_score, sentiment_polarity)\n",
    "        #get noun\n",
    "        noun_str_b = str_b[ is_noun_exist_in_b.start() : is_noun_exist_in_b.end() ]\n",
    "        #change sentiment and polarity\n",
    "        def mapFunction(data):\n",
    "            idx, x = data\n",
    "            if(idx == 2):\n",
    "                return sentiment_polarity\n",
    "            elif(idx == 3):\n",
    "                return adjective_score\n",
    "            else:\n",
    "                return x\n",
    "            \n",
    "        noun_update_str_b = '_'.join( list(map(mapFunction, enumerate(noun_str_b.split('_')))) )\n",
    "        #change str_b for noun filtered with x\n",
    "        list_str_b = list(str_b)\n",
    "        list_str_b[is_noun_exist_in_b.start() : is_noun_exist_in_b.end()] = 'x'\n",
    "        str_b = \"\".join(list_str_b)\n",
    "        print(str_b, noun_update_str_b, is_noun_exist_in_b)\n",
    "        str_b = str(ApplicationExpression(read_expr(r\"\\x.\" + str_b), read_expr(noun_update_str_b)).simplify())\n",
    "        \n",
    "        a = read_expr(r'\\x.x')\n",
    "        b = read_expr(str_b)\n",
    "    \n",
    "    #change identity function for adverb.\n",
    "    elif( is_adverb_exist_in_a and is_adj_exist_in_b or is_adverb_exist_in_a and is_verb_exist_in_b ):\n",
    "        #adverb modify adjective\n",
    "        #example very_RB_I excellent_JJ_P_10 = excellent_JJ_P_20\n",
    "        #I Intensifier must *2\n",
    "        #N Negate must *-1\n",
    "        adverb_type = str_a.split('_')[2]\n",
    "        if(adverb_type == 'N'):\n",
    "            a = read_expr(r'\\x.x')\n",
    "        elif(adverb_type == 'Ne'):\n",
    "            #for negation adverb just change polairty\n",
    "            def mapFunction(data):\n",
    "                idx, x = data\n",
    "                if(idx == 2):\n",
    "                    if(x == 'P'):\n",
    "                        return 'Ne'\n",
    "                    elif(x == 'Ne'):\n",
    "                        return 'P'\n",
    "                    else:\n",
    "                        return x\n",
    "                else:\n",
    "                    return x\n",
    "\n",
    "            str_b = '_'.join( list(map(mapFunction, enumerate(str_b.split('_')))) )\n",
    "            a = read_expr(r'\\x.x')\n",
    "            b = read_expr(str_b)\n",
    "            \n",
    "        elif(adverb_type == 'I'):\n",
    "            #for intensifier adverb. scale adjective value \n",
    "            def mapFunction(data):\n",
    "                idx, x = data\n",
    "                if(idx == 3):\n",
    "                    return str(int(x) * 2);\n",
    "                else:\n",
    "                    return x;\n",
    "\n",
    "            str_b = '_'.join( list(map(mapFunction, enumerate(str_b.split('_')))) )\n",
    "            a = read_expr(r'\\x.x')\n",
    "            b = read_expr(str_b)\n",
    "        \n",
    "    elif( is_adverb_exist_in_a and is_adverb_exist_in_b):\n",
    "        #adverb modify other adverb\n",
    "        #kondisi yang jarang bertemu\n",
    "        pass\n",
    "    \n",
    "    if(str_a == 'CC'):\n",
    "        print('m')\n",
    "        pass\n",
    "    if(str_b == 'CC'):\n",
    "        a = read_expr(str_b)\n",
    "        b = read_expr(str_a)\n",
    "    if (re.search(r'CC\\(', str_a)) and (re.search(r',', str_a)) :\n",
    "        a = read_expr(r'\\x.x')\n",
    "        b = read_expr(replacer(str_a, str_b))\n",
    "        \n",
    "    r_word = ['PRP', 'FW', 'NN', 'LS', 'JJ']\n",
    "    \n",
    "    if '(' not in str_a and '(' not in str_b:\n",
    "            x = False\n",
    "            y = False\n",
    "            for r in r_word:\n",
    "                if r in str_a:\n",
    "                    x = True\n",
    "                if r in str_b:\n",
    "                    y = True\n",
    "                    \n",
    "            if x and y:\n",
    "                a = read_expr(r'\\x.x')\n",
    "                b = read_expr('seq(' + str_a + ',' + str_b + ')')\n",
    "                \n",
    "    str_a = str(a)\n",
    "    str_b = str(b)\n",
    "    print('new a ' + str_a)\n",
    "    print('new b ' + str_b)\n",
    "    print('hasil ' + str(ApplicationExpression(a, b).simplify()))\n",
    "    return ApplicationExpression(a, b).simplify()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replacer(a, b):\n",
    "    print(a, b)\n",
    "    pattern = '(.*?_JJ_.*?_)'\n",
    "    end_pos = len(re.findall(pattern, a)[0]) + 1\n",
    "    start_pos = 0\n",
    "    for x in range(0, end_pos):\n",
    "        if a[x] == '(' or a[x] == ',':\n",
    "            start_pos = x + 1\n",
    "    temp = ''\n",
    "    for x in range(start_pos, end_pos):\n",
    "        temp += a[x]\n",
    "    res = str(a).replace(str(temp), str(b))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glue_process(sent):\n",
    "    url = \"http://localhost:5000/ccgParsing\"\n",
    "    data = {\"sent\": sent}\n",
    "    r = requests.post(url, data=data)\n",
    "\n",
    "    res = r.json()\n",
    "    \n",
    "    from_res = res['tree']\n",
    "    \n",
    "    text = nltk.word_tokenize(sent)\n",
    "    nltk_pos = nltk.pos_tag(text)\n",
    "    \n",
    "    pos_tagged = insert_pos_tag(from_res, pos_tag(data['sent']), nltk_pos)\n",
    "\n",
    "    hasil = parser(pos_tagged)\n",
    "\n",
    "    tree = Tree.fromstring(hasil)\n",
    "    swn_score = get_score(sent)\n",
    "    \n",
    "#     print(tree)\n",
    "    return lambda_calculus(tree, swn_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glue_process('the breakfast that the restaurant served daily was excellent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "glue_process('i bought my canon g3 about a month ago and i have to say i am very satisfied')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_process(\"The bowl of squid eyeball stew is hot and delicious\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = '(\\w*?\\+?\\w*_JJ_\\w_\\d)'\n",
    "re.findall(pattern, 'CC(have_VB_P_2(say_VB_Ne_0(seq(satisfied_JJ_P_2,i_FW_Ne_0)),i_FW_Ne_0))')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
