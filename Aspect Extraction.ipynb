{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Farza\n",
      "[nltk_data]     Nurifan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import re\n",
    "import pandas as pd\n",
    "import itertools, nltk, string \n",
    "nltk.download('wordnet')\n",
    "#from transforms import flatten_deeptree\n",
    "\n",
    "rx_dict = {\n",
    "    'title': re.compile(r'\\[t\\](?P<title>.*)'),\n",
    "    'review': re.compile(r'(?P<aspect>.*)##(?P<review>.*)')\n",
    "}\n",
    "\n",
    "def parse_data(file, data, reviews=[], aspects=[]):\n",
    "    line = file.readline();\n",
    "    if(line):\n",
    "        match_title = rx_dict['title'].search(line);\n",
    "        if match_title:\n",
    "            data['title'].append(match_title.group('title'))\n",
    "            data['domain'].append('canon g3')\n",
    "            if(len(reviews) > 0 or len(aspects) > 0):\n",
    "                data['review'].append(\"\".join(reviews))\n",
    "                data['aspect'].append(\", \".join(aspects))\n",
    "                reviews = []\n",
    "        \n",
    "        match_review = rx_dict['review'].search(line)\n",
    "        if match_review:\n",
    "            review_text = match_review.group('review');\n",
    "            aspect_text = match_review.group('aspect');\n",
    "            \n",
    "            if(review_text):\n",
    "                reviews.append(review_text);\n",
    "            \n",
    "            if(aspect_text):\n",
    "                aspects.append(aspect_text);\n",
    "                \n",
    "        parse_data(file, data, reviews, aspects)\n",
    "    else:\n",
    "        if(len(reviews) > 0 or len(aspects) > 0):\n",
    "            data['review'].append(\"\".join(reviews))\n",
    "            data['aspect'].append(\", \".join(aspects))\n",
    "    \n",
    "data = {\n",
    "    'title': [],\n",
    "    'review': [],\n",
    "    'aspect': [],\n",
    "    'domain': []\n",
    "}\n",
    "    \n",
    "def read_file():\n",
    "    with open(os.path.join(os.path.abspath('dataset/bing_liu/') , 'Canon_G3.txt'), 'r') as file:\n",
    "        parse_data(file, data);\n",
    "        \n",
    "positive_lexicon = []\n",
    "negative_lexicon = []\n",
    "\n",
    "def read_lexicon():\n",
    "    global positive_lexicon;\n",
    "    global negative_lexicon;\n",
    "    \n",
    "    with open(os.path.join(os.path.abspath('opinion-lexicon-English/') , 'positive-words.txt'), 'r') as file:\n",
    "        line = file.readline();\n",
    "        while \";\" in line:\n",
    "            line = file.readline();\n",
    "         \n",
    "        positive_lexicon = file.readlines()\n",
    "    \n",
    "    with open(os.path.join(os.path.abspath('opinion-lexicon-English/') , 'negative-words.txt'), 'r', encoding = \"ISO-8859-1\") as file:\n",
    "        line = file.readline();\n",
    "        while \";\" in line:\n",
    "            line = file.readline();\n",
    "        \n",
    "        negative_lexicon = file.readlines()\n",
    "        \n",
    "    positive_lexicon = list(map(lambda word: word.rstrip(\"\\n\\r\"), positive_lexicon))\n",
    "    negative_lexicon = list(map(lambda word: word.rstrip(\"\\n\\r\"), negative_lexicon))\n",
    "    \n",
    "        \n",
    "read_file()\n",
    "read_lexicon()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print dataset with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xdata = pd.DataFrame(data)\n",
    "len(xdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CORENLP_HOME\"] = r'C:\\stanford-corenlp-full-2018-10-05'\n",
    "\n",
    "import corenlp \n",
    "client = corenlp.CoreNLPClient()\n",
    "\n",
    "def chunk_check(text, word):\n",
    "    try:\n",
    "        pattern = '{tag:/NN.*/} <compound {word:'+ word +';tag:/NN.*/}'\n",
    "        matches = client.semgrex(text, pattern)\n",
    "        res = matches['sentences']\n",
    "        if len(res) == 1:\n",
    "            if res[0]['length'] == 0:\n",
    "                return word\n",
    "        return res[0]['0']['text'] + ' ' + word\n",
    "    except:\n",
    "        return word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Stanford CoreNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i recently purchased the canon!powershot!g3 and am extremely satisfied with the purchase .'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pycorenlp import StanfordCoreNLP\n",
    "import json\n",
    "\n",
    "nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "dependency_parser = nlp.annotate\n",
    "\n",
    "def entity_check(sentence):\n",
    "    res = sentence\n",
    "    result = dependency_parser(sentence, properties={\"outputFormat\": \"json\", \"annotators\": \"openie\"})\n",
    "    if len(result['sentences'][0]['openie']) != 0:\n",
    "        s = result['sentences'][0]['openie'][0]['subject']\n",
    "        o = result['sentences'][0]['openie'][0]['object']\n",
    "        res = res.replace(s, '!'.join(s.split(' ')))\n",
    "        res = res.replace(o, '!'.join(o.split(' ')))\n",
    "    return res\n",
    "\n",
    "entity_check('i recently purchased the canon powershot g3 and am extremely satisfied with the purchase .')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from nltk.parse.corenlp import CoreNLPDependencyParser\n",
    "\n",
    "parser = CoreNLPDependencyParser()\n",
    "parse = next(parser.raw_parse(\"my name is khan\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('khan', 'JJ'), 'nsubj', ('name', 'NN')),\n",
       " (('name', 'NN'), 'nmod:poss', ('my', 'PRP$')),\n",
       " (('khan', 'JJ'), 'cop', ('is', 'VBZ'))]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(parse.triples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my\tPRP$\t2\tnmod:poss\n",
      "name\tNN\t4\tnsubj\n",
      "is\tVBZ\t4\tcop\n",
      "khan\tJJ\t0\tROOT\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(parse.to_conll(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i recently purchased the canon powershot g3 and am extremely satisfied with the purchase . the camera is very easy to use , in fact on a recent trip this past week i was asked to take a picture of a vacationing elderly group . after i took their picture with their camera , they offered to take a picture of us . i just told them , press halfway , wait for the box to turn green and press the rest of the way . they fired away and the picture turned out quite nicely . ( as all of my pictures have thusfar ) . a few of my work constituants owned the g2 and highly recommended the canon for picture quality . i 'm easily enlarging pictures to 8 1/2 x 11 with no visable loss in picture quality and not even using the best possible setting as yet ( super fine ) . ensure you get a larger flash , 128 or 256 , some are selling with the larger flash , 32mb will do in a pinch but you 'll quickly want a larger flash card as with any of the 4mp cameras . bottom line , well made camera , easy to use , very flexible and powerful features to include the ability to use external flash and lense / filters choices . i 'd highly recommend this camera for anyone who is looking for excellent quality pictures and a combination of ease of use and the flexibility to get advanced with many options to adjust if you like . great job canon ! \""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xdata.iloc[0]['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_candidate_chunks(text, grammar=r'''NP: {<NN.*><JJ>?<IN>?<PRP.*>?<NN.*>} \n",
    "                                            ...AP: {<JJ.*><.*>?<VB.*>+}'''):\n",
    "    # exclude candidates that are stop words or entirely punctuation\n",
    "    punct = set(string.punctuation)\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    # tokenize, POS-tag, and chunk using regular expressions\n",
    "    chunker = nltk.chunk.regexp.RegexpParser(grammar)\n",
    "    tagged_sents=[]\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        tagged_sents.append(nltk.pos_tag(nltk.word_tokenize(sent)))\n",
    "    \n",
    "    #print(tagged_sents)\n",
    "    all_chunks = list(itertools.chain.from_iterable(nltk.chunk.tree2conlltags(chunker.parse(tagged_sent)) for tagged_sent in tagged_sents))\n",
    "    # join constituent chunk words into a single chunked phrase\n",
    "    candidates = [' '.join(word for word, pos, chunk in group).lower()\n",
    "                  for key, group in itertools.groupby(all_chunks, lambda chunk: chunk[2] != 'O') if key]\n",
    "\n",
    "    return [cand for cand in candidates\n",
    "            if cand not in stop_words and not all(char in punct for char in cand)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "# from IPython.display import clear_outputfrom \n",
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocessing(semua_kalimat):\n",
    "    i=0\n",
    "    kalimat_semua = []\n",
    "    panjang = len(semua_kalimat) - 1\n",
    "    for sentence in semua_kalimat:\n",
    "        kalimat = []\n",
    "        for word in nltk.word_tokenize(sentence):\n",
    "            kata = wordnet_lemmatizer.lemmatize(word)\n",
    "            if re.match(r'^[0-9]+$', kata) != None:\n",
    "                kata = 'Num'\n",
    "            kalimat.append(kata)\n",
    "        kalimat_semua.append(' '.join(kalimat))\n",
    "#         print(sentence)        \n",
    "#         clear_output(wait=True)\n",
    "#         print((i/panjang)*100,\"%\")\n",
    "        i+=1\n",
    "    return kalimat_semua\n",
    "\n",
    "# input file\n",
    "df = pd.read_csv(\"dataset.csv\")\n",
    "# preprocess\n",
    "semkal = preprocessing(df['review'])\n",
    "labels = df[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33503\n"
     ]
    }
   ],
   "source": [
    "# settings tf-idf\n",
    "tfidf = TfidfVectorizer(sublinear_tf=False, analyzer='word', ngram_range=(1,2))\n",
    "\n",
    "# tf-idf\n",
    "features = tfidf.fit_transform(semkal).toarray()\n",
    "features_name = tfidf.vocabulary_\n",
    "print(len(features_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7364864864864865\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.81      0.78       670\n",
      "           1       0.72      0.64      0.68       514\n",
      "\n",
      "   micro avg       0.74      0.74      0.74      1184\n",
      "   macro avg       0.73      0.73      0.73      1184\n",
      "weighted avg       0.74      0.74      0.73      1184\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.30, random_state=42)\n",
    "\n",
    "''' The important part '''\n",
    "# ''' SVM classifier ''' \n",
    "# model\n",
    "model_svm = LinearSVC()\n",
    "# training\n",
    "model_svm.fit(X_train, y_train)\n",
    "# predict / testing\n",
    "pred = model_svm.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(precision_score(y_test, pred, average='micro'))\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(model_svm.predict([features[0]])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i recently purchased the canon powershot g3 and am extremely satisfied with the purchase .'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semkal[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3944"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rather heavy for point and shoot but a great camera for semi pros . '"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'][596]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double Propagation\n",
    "\n",
    "### Rule 1.1 if a word A, whose POS is NN, is depended by an opinion word O through Dep, where Dep is one of the dependency relations amod, prep, nsubj, csubj, xsubj, dobj, and iobj, then A is an aspect.\n",
    "\n",
    "### Rule 1.2 if an opinion word O and a word A, whose POS is NN, depend on a third word H through dependency relations Depi and Depj respectively, where Depi and Dep j are one of the relations amod, prep, nsubj, csubj, xsubj, dobj, and iobj, then A is an aspect.\n",
    "\n",
    "### Rule 2.1 if a word O, whose POS is JJ (adjective), directly depends on an aspect A through dependency relation Dep, where Dep is one of the dependency rela- tions amod, prep, nsubj, csubj, xsubj, dobj, and iobj, then O is an opinion word.\n",
    "\n",
    "### Rule 2.2 if a word O, whose POS is JJ, and an aspect A, directly depend on a third word H through relations Depi and Depj respectively, where Depi and Depj are one of the relations amod, prep, nsubj, csubj, xsubj, dobj, and iobj, then O is an opinion word.\n",
    "\n",
    "### Rule 3.1 if a word Aj, whose POS is NN, directly depends on an aspect Ai through conj, then A j is an aspect\n",
    "\n",
    "### Rule 3.2 if a word Aj, whose POS is NN, and an aspect Ai, directly depend on a third word H through the dependency relations De pi and De p j , where De pi and Depj are one of the relations in amod, prep, nsubj, csubj, xsubj, dobj, and conj, then A j is an aspect\n",
    "\n",
    "\n",
    "### Rule 4.1 if a word Oj, whose POS is JJ, directly depends on an opinion word Oi through conj, then O j is an opinion word. \n",
    "\n",
    "### Rule 4.2 if a word Oj, whose POS is JJ, and an opinion word Oi, directly depend on a third word H through the dependance relations De pi and De p j , where Dep1 and Dep2 are one of the relations in amod, prep, nsubj, csubj, xsubj, dobj, and conj, then O j is an opinion word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding new dependencies for DP 'nmod' and'advmod'\n",
    "dep_DP = ['amod', 'prep', 'nsubj', 'csubj', 'xsubj', 'dobj', 'iobj', 'advmod']\n",
    "conj_DP = ['conj']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output = nlp.annotate('Pusheen and Smitha walked along the beach.', properties={\n",
    "  'annotators': 'tokenize,ssplit,pos,depparse,parse',\n",
    "  'outputFormat': 'json'\n",
    "  })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import Tree\n",
    "t = Tree.fromstring(output['sentences'][0]['parse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunking(text):\n",
    "    chunking_noun = extract_candidate_chunks(text, r'NP: {<NN.*|JJ.*><.*>?<NN>}') \n",
    "    chunking_adj = extract_candidate_chunks(text, r'AP: {<JJ.*|RB.*><TO>?<VB.*>}') \n",
    "    #print(text)\n",
    "  #  nlpParser.parser_annotators = 'tokenize,ssplit,pos,lemma,depparse'\n",
    "    \n",
    "    output = nlp.annotate(text, properties={\n",
    "      'annotators': 'tokenize,ssplit,pos,depparse,parse',\n",
    "      'outputFormat': 'json'\n",
    "      })\n",
    "    \n",
    "    pp = Tree.fromstring(output['sentences'][0]['parse'])\n",
    "   \n",
    "    new_chunking = []\n",
    "    for c in chunking_noun:\n",
    "        for i in pp.subtrees(filter=lambda x: x.label() == 'NP'):\n",
    "            lls = i.leaves()\n",
    "            s = len(lls)\n",
    "            match = 0\n",
    "            for xx in lls:\n",
    "                if xx in c:\n",
    "                    match += 1\n",
    "\n",
    "            if(match >= (s/2)):\n",
    "                new_chunking.append(c)\n",
    "        \n",
    "                break\n",
    "\n",
    "    for c in chunking_adj:\n",
    "        #chunk_a.append(c)\n",
    "        for i in pp.subtrees(filter=lambda x: x.label() == 'ADJP'):\n",
    "            lls = i.leaves()\n",
    "            s = len(lls)\n",
    "            match = 0\n",
    "            for xx in lls:\n",
    "                if xx in c:\n",
    "                    match += 1\n",
    "\n",
    "            if(match >= (s/2)):\n",
    "                new_chunking.append(c)\n",
    "        \n",
    "                break\n",
    "        #new_chunking.append(c)\n",
    "\n",
    "    for chunk in new_chunking:\n",
    "        text = text.replace(chunk, '-'.join(chunk.split(' ')))\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "candidate_aspect = []\n",
    "new_opinion = []\n",
    "op_set = positive_lexicon + negative_lexicon\n",
    "\n",
    "def double_propagation(O: op_set, reviews, using_chunking=True, using_objective_detection=False, save_to_file=False):\n",
    "    o_expanded = O\n",
    "    f = []\n",
    "    is_stop = False\n",
    "    flag_cycle = 0\n",
    "    \n",
    "    t_a_p = []\n",
    "    a_p = []\n",
    "    r_p = []\n",
    "    \n",
    "    while (not is_stop):\n",
    "        f_i = []\n",
    "        o_i = []\n",
    "    \n",
    "        index = 0\n",
    "        for r in reviews:\n",
    "            temp = []\n",
    "            \n",
    "            if using_objective_detection:\n",
    "                if model_svm.predict([features[index]])[0] == 0:\n",
    "                    index += 1\n",
    "                    continue\n",
    "            \n",
    "            sentences = nltk.sent_tokenize(r)\n",
    "\n",
    "            for sent in sentences:\n",
    "                if using_chunking:\n",
    "                    r = chunking(sent)\n",
    "                else:\n",
    "                    r = sent\n",
    "                    \n",
    "                parse = next(parser.raw_parse(r))\n",
    "\n",
    "                #Rule 1.1\n",
    "                for (w1, dep, w2) in list(parse.triples()):\n",
    "                    if(dep in dep_DP):\n",
    "                        # Rule 1.1\n",
    "                        if(w1[0] in o_expanded):\n",
    "                            if w2[1] == 'NN' and w2[0] not in f:\n",
    "                                f_i.append(w2[0])\n",
    "                                temp.append(w2[0])\n",
    "            #                         candidate_aspect.append(chunk_check(r, w2[0]))\n",
    "                        elif(w2[0] in o_expanded):          \n",
    "                            if w1[1] == 'NN' and w1[0] not in f:\n",
    "                                f_i.append(w1[0])\n",
    "                                temp.append(w1[0])\n",
    "\n",
    "            #                         candidate_aspect.append(chunk_check(r, w1[0]))\n",
    "\n",
    "\n",
    "                # Rule 1.2\n",
    "                for (w1, dep, w2) in parse.triples():\n",
    "                    if(dep in dep_DP):\n",
    "                        H = ''\n",
    "                        O = ''\n",
    "                        if w1[0] in o_expanded:\n",
    "                            H = w2[0]\n",
    "                            O = w1\n",
    "                        elif w2[0] in o_expanded:\n",
    "                            H = w1[0]\n",
    "                            O = w2\n",
    "\n",
    "                        if H:\n",
    "                            for (w1, dep, w2) in list(parse.triples()):\n",
    "                                if w1[0] == H and w2[0] != O[0]:\n",
    "                                    if w2[1] == 'NN' and w2[0] not in f:\n",
    "                                        f_i.append(w2[0])\n",
    "                                        temp.append(w2[0])\n",
    "\n",
    "            #                                 candidate_aspect.append(chunk_check(r, w2[0]))\n",
    "                                elif w2[0] == H  and w1[0] != O[0]:\n",
    "                                    if w1[1] == 'NN' and w1[0] not in f:\n",
    "                                        f_i.append(w1[0])\n",
    "                                        temp.append(w1[0])\n",
    "\n",
    "            #                                 candidate_aspect.append(chunk_check(r, w1[0]))\n",
    "\n",
    "\n",
    "                # Rule 4.1\n",
    "                for (w1, dep, w2) in list(parse.triples()):\n",
    "                    if(dep in conj_DP):\n",
    "                        if w1[0] in o_expanded:\n",
    "                            if w2[1] == 'JJ' and w2[0] not in o_expanded:\n",
    "                                o_i.append(w2[0])\n",
    "\n",
    "                        elif w2[0] in o_expanded:\n",
    "                            if w1[1] == 'JJ' and w1[0] not in o_expanded:\n",
    "                                o_i.append(w1[0])\n",
    "\n",
    "\n",
    "                # Rule 4.2\n",
    "                for (w1, dep, w2) in parse.triples():\n",
    "                    if(dep in dep_DP or dep in conj_DP):\n",
    "                        H = ''\n",
    "                        O = ''\n",
    "                        if w1[0] in o_expanded:\n",
    "                            H = w2[0]\n",
    "                            O = w1\n",
    "                        elif w2[0] in o_expanded:\n",
    "                            H = w1[0]\n",
    "                            O = w2\n",
    "\n",
    "                        if H:\n",
    "                            for (w1, dep, w2) in list(parse.triples()):\n",
    "                                if w1[0] == H and w2[0] != O[0]:\n",
    "                                    if w2[1] == 'JJ' and w2[0] not in o_expanded:                  \n",
    "                                        o_i.append(w2[0])\n",
    "                                elif w2[0] == H  and w1[0] != O[0]:\n",
    "                                    if w1[1] == 'JJ' and w1[0] not in o_expanded:\n",
    "                                        o_i.append(w1[0])\n",
    "\n",
    "\n",
    "\n",
    "            if flag_cycle == 0:\n",
    "                r_p.append(r)\n",
    "                a_array = []\n",
    "                tes = []\n",
    "                try:\n",
    "                    tes = df['aspect'][index].split(', ')\n",
    "                except:\n",
    "                    tes = []\n",
    "                for x in tes:\n",
    "                    a_array.append(x.split('[')[0])\n",
    "            \n",
    "                a_p.append('|'.join(a_array))\n",
    "                t_a_p.append('|'.join(temp))\n",
    "            else:\n",
    "                if len(temp) != 0:\n",
    "                    t_a_p[index] += '|' + '|'.join(temp)\n",
    "            index += 1\n",
    "            \n",
    "\n",
    "        #calculate target and opinion expanded\n",
    "        f = f + f_i \n",
    "        o_expanded = o_expanded + o_i\n",
    "\n",
    "        \n",
    "        #reread review, and run rule 3.1, 3.2, 2.1, and 2.2\n",
    "        index = 0\n",
    "        f_ii = []\n",
    "        o_ii = []\n",
    "        for r in reviews:\n",
    "            temp = []\n",
    "            \n",
    "            if using_objective_detection:\n",
    "                if model_svm.predict([features[index]])[0] == 0:\n",
    "                    index += 1\n",
    "                    continue\n",
    "\n",
    "            sentences = nltk.sent_tokenize(r)\n",
    "            for sent in sentences:\n",
    "                if using_chunking:\n",
    "                    r = chunking(sent)\n",
    "                else:\n",
    "                    r = sent\n",
    "\n",
    "                parse = next(parser.raw_parse(r))\n",
    "                #Rule 3.1\n",
    "                for (w1, dep, w2) in list(parse.triples()):\n",
    "                    if(dep in conj_DP):\n",
    "                        if(w1[0] in f_i): \n",
    "                            if w2[1] == 'NN' and w2[0] not in f:\n",
    "                                f_ii.append(w2[0])\n",
    "                                temp.append(w2[0])\n",
    "            #                         candidate_aspect.append(chunk_check(r, w2[0]))\n",
    "                        elif(w2[0] in f_i):          \n",
    "                            if w1[1] == 'NN' and w1[0] not in f:\n",
    "                                f_ii.append(w1[0])\n",
    "                                temp.append(w1[0])\n",
    "            #                         candidate_aspect.append(chunk_check(r, w1[0]))\n",
    "\n",
    "\n",
    "                # Rule 3.2\n",
    "                for (w1, dep, w2) in parse.triples():\n",
    "                    if(dep in dep_DP or dep in conj_DP):\n",
    "                        H = ''\n",
    "                        O = ''\n",
    "                        if w1[0] in f_i:\n",
    "                            H = w2[0]\n",
    "                            O = w1\n",
    "                        elif w2[0] in f_i:\n",
    "                            H = w1[0]\n",
    "                            O = w2\n",
    "\n",
    "                        if H:\n",
    "                            for (w1, dep, w2) in list(parse.triples()):\n",
    "                                if w1[0] == H and w2[0] != O[0]:\n",
    "                                    if w2[1] == 'NN' and w2[0] not in f:\n",
    "                                        f_ii.append(w2[0])  \n",
    "                                        temp.append(w2[0])\n",
    "            #                                 candidate_aspect.append(chunk_check(r, w2[0]))\n",
    "                                elif w2[0] == H  and w1[0] != O[0]:\n",
    "                                    if w1[1] == 'NN' and w1[0] not in f:\n",
    "                                        f_ii.append(w1[0])\n",
    "                                        temp.append(w1[0])\n",
    "            #                                 candidate_aspect.append(chunk_check(r, w1[0]))\n",
    "\n",
    "\n",
    "                # Rule 2.1\n",
    "                for (w1, dep, w2) in list(parse.triples()):\n",
    "                    if(dep in dep_DP):\n",
    "                        if w1[0] in f_i:\n",
    "                            if w2[1] == 'JJ' and w2[0] not in o_expanded:\n",
    "                                o_ii.append(w2[0])\n",
    "\n",
    "                        elif w2[0] in f_i:\n",
    "                            if w1[1] == 'JJ' and w1[0] not in o_expanded:\n",
    "                                o_ii.append(w1[0])\n",
    "\n",
    "\n",
    "                # Rule 2.2\n",
    "                for (w1, dep, w2) in parse.triples():\n",
    "                    if(dep in dep_DP):\n",
    "                        H = ''\n",
    "                        O = ''\n",
    "                        if w1[0] in f_i:\n",
    "                            H = w2[0]\n",
    "                            O = w1\n",
    "                        elif w2[0] in f_i:\n",
    "                            H = w1[0]\n",
    "                            O = w2\n",
    "\n",
    "                        if H:\n",
    "                            for (w1, dep, w2) in list(parse.triples()):\n",
    "                                if w1[0] == H and w2[0] != O[0]:\n",
    "                                    if w2[1] == 'JJ' and w2[0] not in o_expanded:                  \n",
    "                                        o_ii.append(w2[0])\n",
    "                                elif w2[0] == H  and w1[0] != O[0]:\n",
    "                                    if w1[1] == 'JJ' and w1[0] not in o_expanded:\n",
    "                                        o_ii.append(w1[0])\n",
    "            if len(temp) != 0:\n",
    "                t_a_p[index] += '|' + '|'.join(temp)\n",
    "\n",
    "            index += 1\n",
    "            \n",
    "        f_i = f_i + f_ii\n",
    "        o_i = o_i + o_ii\n",
    "        f = f + f_ii\n",
    "        o_expanded = o_expanded + o_ii     \n",
    "        \n",
    "        flag_cycle = 1\n",
    "        \n",
    "        if(len(f_i) == 0 and len(o_i) == 0):\n",
    "            if save_to_file == True:\n",
    "                out = pd.DataFrame(r_p)\n",
    "                out['aspect'] = a_p\n",
    "                out['prediction'] = t_a_p\n",
    "                out.to_csv('hasil_dp.csv')\n",
    "            is_stop = True\n",
    "        \n",
    "    return f, o_expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_frequency(aspects):\n",
    "    aspect_frequency = {}\n",
    "    \n",
    "    for aspect in aspects:\n",
    "        if(aspect in aspect_frequency):\n",
    "            aspect_frequency[aspect] += 1\n",
    "        else:\n",
    "            aspect_frequency[aspect] = 1\n",
    "            \n",
    "    return aspect_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pruning_based_on_clause(aspect_frequency, reviews):\n",
    "    pruning = []\n",
    "    for review in reviews:\n",
    "        parse = next(parser.raw_parse(review))\n",
    "        for (w1, dep, w2) in list(parse.triples()):\n",
    "            if(w1[0] in aspect_frequency and w2[0] in aspect_frequency):\n",
    "                if(dep not in conj_DP):\n",
    "                    if(aspect_frequency[w1[0]] > aspect_frequency[w2[0]]):\n",
    "                        pruning.append(w2[0])\n",
    "                    elif(aspect_frequency[w1[0]] < aspect_frequency[w2[0]]):\n",
    "                        pruning.append(w1[0])\n",
    "                \n",
    "    return pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pruning_based_other_products_and_dealers(aspect_frequency, reviews, window=3):\n",
    "    pruning = []\n",
    "    ProductINDI = [\"compare to\", \"compare with\", \"better than\", \"worse than\"]\n",
    "    DealerINDI  = [\"shop with\", \"buy from\"]\n",
    "    for review in reviews:\n",
    "        if any(indication in review for indication in ProductINDI):\n",
    "            tokens = nltk.word_tokenize(review)\n",
    "            index = 0\n",
    "            while index < len(tokens) - 1:\n",
    "                if tokens[index] + \" \" + tokens[index + 1] in ProductINDI:\n",
    "                    index += 2\n",
    "                    for x in range(window):\n",
    "                        next_window = index + x + 1;\n",
    "                        if next_window < len(tokens) and tokens[next_window] in aspect_frequency:\n",
    "                            pruning.append(tokens[next_window])\n",
    "                else :\n",
    "                    index += 1\n",
    "                    \n",
    "        if any(indication in review for indication in DealerINDI):\n",
    "            tokens = nltk.word_tokenize(review)\n",
    "            index = 0\n",
    "            while index < len(tokens) - 1:\n",
    "                if tokens[index] + \" \" + tokens[index + 1] in DealerINDI:\n",
    "                    index += 2\n",
    "                    for x in range(window):\n",
    "                        next_window = index + x + 1;\n",
    "                        if next_window < len(tokens) and tokens[next_window] in aspect_frequency:\n",
    "                            pruning.append(tokens[next_window])\n",
    "                else :\n",
    "                    index += 1 \n",
    "                        \n",
    "    return pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CORENLP_HOME\"] = r'C:\\stanford-corenlp-full-2018-10-05'\n",
    "\n",
    "import corenlp \n",
    "client = corenlp.CoreNLPClient()\n",
    "    \n",
    "def identify_target_phrase_global_pruning(review):\n",
    "    try:\n",
    "        pattern = '{tag:/NN.*/} < {tag:/NN.*/} < {tag:/NN.*/;word:quality }'\n",
    "        matches = client.semgrex(review, pattern)\n",
    "        res = matches['sentences']\n",
    "        if len(res) == 1:\n",
    "            if res[0]['length'] == 0:\n",
    "                pass\n",
    "        return res[0]['0']['text']\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_aspect, opinion_expand = double_propagation(op_set, df['review'][0:597], False, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap = calculate_frequency(dp_aspect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['g3', 'powershot', 'trip', 'trip', 'picture', 'picture', 'card', 'use', 'use', 'job', 'worth', 'cent', 'flagship', 'powershot', 'flagship', 'series', 'powershot', 'megapixel', 'control', 'control', 'kind', 'type', 'cf', 'use', 'picture', 'awe', 'buy', 'g3', 'set', 'use', 'flaw', 'anyone', 'anyone', 'photo', 'programming', 'screen', '4mp', 'right', 'market', '4x', 'screen', 'move', 'focus', 'manual', 'focus', 'plastic', 'line', 'scoying', 'scoying', 'screen', '14x', 'software', 'computer', 'cap', 'worth', 'bargain', 'auto', 'right', 'photo', 'difference', 'range', 'powershot', 'auto', 'buy', 'battery', 'type', 'battery', 'type', 'g3', 'mb', 'cf', 'lever', 'lever', 'colorimetry', 'battery', 'company', 'g3', 'auto', 'shutter', 'slr', 'shutter', 'length', 'program', 'photo', 'manual', 'selection', 'shutter', 'access', 'love', 'cf', 'use', 'auto', 'shoot', 'night', 'drawback', 'market', 'picture', 'color', 'cover', 'tigt', 'work', 'wish', 'work', 'issue', 'seller', 'love', '4x', 'price', 'lack', 'life', 'set', 'photography', 'need', 'use', 'shoot', 'light', 'equipment', 'equipment', 'sensor', 'reputation', 'lag', 'light', 'flash', 'lag', 'problem', 'film', 'lock', 'shutter', 'lag', 'time', 'field', 'f8', 'lock', 'problem', 'slr', 'flash', 'use', 'nikon', 'use', 'canon', 'cord', 'metz', 'module', 'cord', 'set', 'set', 'manual', 'set', 'shutter', 'set', 'set', 'shutter', 'head', 'panel', 'duration', 'metz', 'exposure', 'range', 'metz', 'metz', 'cant', 'exposure', 'nikon', 'film', 'head', 'mode', 'mode', 'software', 'adobe', 'photoshop', 'hardware', 'hardware', 'adobe', 'software', 'software', 'use', 'system', 'hobbiest', 'hobbiest', 'ability', 'battery', 'drawback', 'functionality', 'month', 'review', 'dream', 'shutter', 'option', 'panel', 'set', 'set', 'powershot', 'auto-correction', 'image', 'color', 'response', 'view', 'range', 'nikon', 'love', 'c5050', 'cp5000', 'g3', 'power', 'use', 'screen', 'c5050', 'c5050', 'body', 'screen', 'g3', '4mp', '4x', 'screen', 'stamp', 'g3', 'powershot', 'canon', 'g3', 'thing', 'film', 'g3', 'period', 'g3', 'delay', 'canon', 'g3', 'film', 'buy', 'film', 'capture', 'love', 'g3', 'g3', '4mp', 'review', 'cap', 'display', 'display', 'lens', 'cap', 'damage', 'cap', 'damage', 'plastic', 'body', 'choice', 'auto', 'worth', 'note', 'battery', 'charge', 'buy', 'buy', 'everything', 'plus', 'camcorder', 'flaw', 'buy', 'buy', 'megapixel/4x', 'zoom', 'trade', 'choice', 'barrel', 'jpeg', 'power', 'move', 'olympus', 'printer', 'pic', 'film', 'couple', 'photo', 'use', 'battery', 'experience', 'use', 'use', 'review', 'printer', 'slr', 'film', 'need', 'lot', 'thing', 'choice', 'duration', '4mp', 'plastic', 'semi-pro', 'slr', 'photography', 'program', 'auto', 'screen', 'expense', 'unit', 'unit', 'bag', 'view', 'photography', 'quibble', 'functionality', 'image', 'power', 'power', 'note', 'price', 'right', 'auto', 'digicam', '4mp', '4mp', 'distortion', 'distortion', 'use', 'camea', 'screen', 'distortion', 'distortion', 'issue', 'effect', 'litium', 'viking', '4mp', 'max', 'picture', 'buy', 'g2', 'picture', 'protector', 'shooting', 'coating', 'filter', 'adapter', 'adapter', 'work', 'barrel', 'part', 'corner', 'viewfinder', 'viewfinder', 'viewfinder', 'playback', 'processing', 'charge', 'credit', 'flaw', 'viewfinder', 'corner', 'use', 'view', 'flaw', 'flaw', 'use', 'view', 'use', 'view', 'frame', 'picture', 'megapixel', 'shoe', 'coolpix', 'g3', 'image', 'megapixel', 'lens', 'picture', 'light', 'screen', 'position', 'combination', 'attachment', 'attachment', 'buy', 'unit', 'difference', 'unit', 'powerhouse', 'battery', 'system', 'system', 'use', 'shutter', 'use', 'use', 'right', 'plastic', 'disappointment', 'mb', 'ibook', 't', 'buy', 's330', 'line', 'buy', 'unit', 'buy', 'use', 'use', 'color', 'use', 'paper', 'buy', 'telephoto', 'coolpix', 'lack', 'lack', 'image', 'battery', 'need', 'enthusiast', 'market', 'g3', 'coolpix', 'performance', 'picture', 'battery', 'megapixel', 'canon', 'megapixel', 'g3', 'g3', 'today', 'point', 'point', 'microdrive', 'type', 'maximum', 'battery', 'life', 'wrist', 'neck']\n"
     ]
    }
   ],
   "source": [
    "pruning_clause = pruning_based_on_clause(ap,df['review'][0:597] );\n",
    "print(pruning_clause)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['use', 'anything', 'camera']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prun_dealer_product = pruning_based_other_products_and_dealers(ap, df['review'][0:597])\n",
    "prun_dealer_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision_recall(aspect, target):\n",
    "    tp = 0\n",
    "    \"\"\"      \n",
    "    \n",
    "    for a in aspect: \n",
    "        if a in target:\n",
    "             tp += 1\n",
    "   \n",
    "    \"\"\"\n",
    "    for t in target: \n",
    "        for a in aspect:\n",
    "            zz = t.split(' ')\n",
    "            vv =0\n",
    "            for z in zz:\n",
    "                if z in a:\n",
    "                    vv+=1\n",
    "            if len(zz) == vv:\n",
    "                tp+=1\n",
    "                break\n",
    "    \n",
    "    P = (tp * 1.0) / (len(aspect) * 1.0)\n",
    "    R = (tp * 1.0) / (len(target) * 1.0)\n",
    "#     print(P, len(aspect))\n",
    "#     print(R, len(target))\n",
    "    \n",
    "    f1 = 2.0 * P * R / (P+R)\n",
    "    \n",
    "    return P, R, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "target = []\n",
    "\n",
    "for t in df['aspect'][0:597]:\n",
    "    if t is not np.nan:\n",
    "        for s in t.split(', '):\n",
    "            for x in s.split(','):\n",
    "                jj = re.sub(r'\\[[+|-]\\d\\]', '',x)\n",
    "                jjs = re.sub(r'\\[\\w\\]', '',jj)\n",
    "                if(jjs):\n",
    "                    target.append(jjs);\n",
    "dp_aspect =  list(map(lambda aspect: ' '.join(aspect.split('-')), dp_aspect))               \n",
    "candidate_aspect = list(map(lambda aspect: ' '.join(aspect.split('-')), candidate_aspect))\n",
    "\n",
    "\n",
    "# candidate_aspect = list(map(lambda aspect: aspect), candidate_aspect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate_precision_recall(list(dict.fromkeys(candidate_aspect)), list(dict.fromkeys(target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.25333333333333335, 0.1792452830188679, 0.20994475138121546)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_precision_recall(list(dict.fromkeys(new_dp_aspect)), list(dict.fromkeys(target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pruning_clause + prun_dealer_product\n",
    "\n",
    "for freq in a:\n",
    "    try:\n",
    "        if ap[freq] > 0:\n",
    "            ap[freq] -= 1\n",
    "    except: continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in ap:\n",
    "    if ap[x] == 1:\n",
    "        ap[x] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dp_aspect = []\n",
    "for a in ap:\n",
    "    if ap[a] > 0:\n",
    "        new_dp_aspect.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_aspect_frequency = sorted(ap.items(), key=lambda kv: kv[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_freq_more_1 = []\n",
    "for x in sorted_aspect_frequency:\n",
    "    if(x[1] > 1):\n",
    "        a_freq_more_1.append(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.232, 0.27358490566037735, 0.2510822510822511)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_precision_recall(a_freq_more_1, list(dict.fromkeys(target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['camera', 'quality', 'optical zoom', 'lens', 'box', 'canon', 'canon g3', 'picture quality', 'way', 'reputation', 'anything', 'g2', 'fiance', 'excellent grip', 'time', 'notice', 'fact', 'great feel', 'problem', 'sync', 'mbyte cf', 'maintains this rep', 'lot', 'shutter button', 'view finder', 'outstanding image quality', 'len', 'easier choice']\n"
     ]
    }
   ],
   "source": [
    "print(new_dp_aspect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "opo = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 261 219 97 596 0.07983193277310924 0.16379310344827586 0.4697986577181208 596\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "hasil = pd.read_csv(\"hasil_dp.csv\", na_values=\" kakaka\")\n",
    "target = hasil['prediction']\n",
    "aspek = hasil['aspect']\n",
    "\n",
    "true = 0\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "false = 0\n",
    "count = 0\n",
    "for x in range(0,596):\n",
    "    tipe_aspek = type(aspek[x]) is str\n",
    "    tipe_target = type(target[x]) is str\n",
    "    count+=1\n",
    "    \n",
    "    flag = False\n",
    "    if tipe_aspek == tipe_target and tipe_aspek == True:\n",
    "        aspek_split = aspek[x].split('|')\n",
    "        for t in target[x].split('|'):\n",
    "            if t != '':\n",
    "                if t in aspek_split:\n",
    "                    tp += 1\n",
    "                    flag = True\n",
    "                    break\n",
    "                    \n",
    "    if flag == True:\n",
    "        continue\n",
    "#     if target[x] == aspek[x]:\n",
    "#         true += 1\n",
    "    elif tipe_aspek == tipe_target and tipe_aspek == False:\n",
    "        tn += 1\n",
    "    elif tipe_aspek == False:\n",
    "        fn += 1\n",
    "    else:\n",
    "        fp += 1\n",
    "        \n",
    "print(tp, tn, fp, fn, tp+tn+fp+fn, tp/(tp+fp), tp/(tp+fn), (tp+tn)/(tp+tn+fp+fn), count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
