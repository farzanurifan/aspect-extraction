{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "import math\n",
    "from math import exp, expm1, log, log10\n",
    "import numpy as np\n",
    "import turtle\n",
    "import pandas as pd\n",
    "from nltk.wsd import lesk\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "from pywsd.lesk import adapted_lesk\n",
    "from nltk.corpus import stopwords\n",
    "stopWords = set(stopwords.words('english'))\n",
    "import sys, os\n",
    "\n",
    "\n",
    "nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "dependency_parser = nlp.annotate\n",
    "positive_lexicon = []\n",
    "negative_lexicon = []\n",
    "\n",
    "def read_lexicon():\n",
    "    global positive_lexicon;\n",
    "    global negative_lexicon;\n",
    "    \n",
    "    with open(os.path.join(os.path.abspath('opinion-lexicon-English/') , 'positive-words.txt'), 'r') as file:\n",
    "        line = file.readline();\n",
    "        while \";\" in line:\n",
    "            line = file.readline();\n",
    "         \n",
    "        positive_lexicon = file.readlines()\n",
    "    \n",
    "    with open(os.path.join(os.path.abspath('opinion-lexicon-English/') , 'negative-words.txt'), 'r', encoding = \"ISO-8859-1\") as file:\n",
    "        line = file.readline();\n",
    "        while \";\" in line:\n",
    "            line = file.readline();\n",
    "        \n",
    "        negative_lexicon = file.readlines()\n",
    "        \n",
    "    positive_lexicon = list(map(lambda word: word.rstrip(\"\\n\\r\"), positive_lexicon))\n",
    "    negative_lexicon = list(map(lambda word: word.rstrip(\"\\n\\r\"), negative_lexicon))\n",
    "    \n",
    "        \n",
    "read_lexicon()\n",
    "op_set = positive_lexicon + negative_lexicon\n",
    "\n",
    "negation = [\n",
    "    \"afraid\",\n",
    "    \"can't\",\n",
    "    \"cannot\",\n",
    "    \"deny\",\n",
    "    \"mean\",\n",
    "    \"negate\",\n",
    "    \"negation\",\n",
    "    \"negative\",\n",
    "    \"neither\",\n",
    "    \"never\",\n",
    "    \"no\",\n",
    "    \"non\",\n",
    "    \"none\",\n",
    "    \"nor\",\n",
    "    \"not\",\n",
    "    \"nothing\",\n",
    "    \"refusal\",\n",
    "    \"refuse\",\n",
    "    \"reject\",\n",
    "    \"rejection\"\n",
    "]\n",
    "\n",
    "def analyse_file(key, lines):    \n",
    "    radii = get_TDOC(lines, key)    \n",
    "    return radii\n",
    "\n",
    "def get_TDOC(lines, key):\n",
    "    freq = {'Init': 0}              #Number of times context term occurs with key\n",
    "    freq.clear()\n",
    "    prohib = [''] #stopWords\n",
    "    for line in lines:\n",
    "        words = line.split(\" \")\n",
    "        if key in words:\n",
    "            for context in words:\n",
    "                flag=0\n",
    "                for i in prohib:\n",
    "                    if i == context:\n",
    "                        flag=1\n",
    "                        break\n",
    "                if flag==0 and context!=key:# and context in op_set:\n",
    "                    freq.setdefault(context, 0)\n",
    "                    freq[context] = freq.get(context) + 1\n",
    "                                           \n",
    "    N = 0                           #Total Number of terms in Document\n",
    "    for line in lines:\n",
    "        words = line.split(\" \")\n",
    "        N += len(words)\n",
    "\n",
    "    Nci = {'Init': 0}               #Total terms that occur with context term\n",
    "    Nci.clear()\n",
    "    for context in freq.keys():\n",
    "        for line in lines:\n",
    "            words = line.split(\" \")\n",
    "            if context in words:\n",
    "                Nci.setdefault(context, 0)\n",
    "                Nci[context] += len(words)\n",
    "\n",
    "    radii = {'Init': 0}             #Get Radius of context term with TDOC formula\n",
    "    radii.clear()\n",
    "    \n",
    "    df = pd.DataFrame(columns=['c', 'm', 'N', 'Nc', 'f', 'N/Nc', 'log(N/Nc)', 'fxlog(N/Nc)', '/4'])\n",
    "    max_value = 0\n",
    "    for term in freq.keys():\n",
    "        radii[term] = (freq[term]*(log(N/Nci[term])))\n",
    "        \n",
    "        if radii[term] > max_value:\n",
    "            max_value = radii[term]\n",
    "        \n",
    "    for term in freq.keys():\n",
    "        radii[term] = radii[term]/max_value\n",
    "        \n",
    "        df = df.append({'c': term,\n",
    "                'm': key,\n",
    "                'N': N,\n",
    "                'Nc': Nci[term],\n",
    "                'f': freq[term],\n",
    "                'N/Nc': \"{0:.2f}\".format(N/Nci[term]),\n",
    "                'log(N/Nc)': \"{0:.2f}\".format(log(N/Nci[term])),\n",
    "                'fxlog(N/Nc)': \"{0:.2f}\".format(freq[term]*(log(N/Nci[term]))),\n",
    "                'normalisasi': \"{0:.2f}\".format((freq[term]*(log(N/Nci[term])))/max_value)\n",
    "               }, ignore_index=True)\n",
    "    \n",
    "#     df.to_excel(\"tdoc2.xlsx\")\n",
    "    return radii                    #Returns entire set of context terms related to key\n",
    "\n",
    "def pos_tag(sentence):\n",
    "    result = dependency_parser(sentence, properties={\"outputFormat\": \"json\", \"annotators\": \"pos\"})['sentences'][0]['tokens']\n",
    "    res = []\n",
    "    for pos in result:\n",
    "        res.append((pos['word'], pos['pos']))\n",
    "    return res\n",
    "\n",
    "def get_theta(key, sentences):\n",
    "    scores = []\n",
    "    for sentence in sentences:\n",
    "        flag = True\n",
    "        \n",
    "        pp_tagged = pos_tag(sentence)\n",
    "        tagged = ('','')\n",
    "        for p in pp_tagged:\n",
    "            if p[0] == key:\n",
    "                tagged = p\n",
    "        if tagged == ('', ''):\n",
    "            flag = False\n",
    "        \n",
    "        ambiguous = tagged[0]\n",
    "        tag = tagged[1]\n",
    "        pos = ''\n",
    "\n",
    "        if 'NN' in tag:\n",
    "            pos = 'n'\n",
    "        elif 'NNS' in tag:\n",
    "            pos = 'nns'\n",
    "        elif 'VB' in tag:\n",
    "            pos = 'v'\n",
    "        elif 'VBG' in tag:\n",
    "            pos = 'v'\n",
    "        elif 'JJ' in tag:\n",
    "            pos = 'a'\n",
    "        elif 'RB' in tag:\n",
    "            pos = 'r'\n",
    "        else:\n",
    "            flag = False\n",
    "\n",
    "        if flag:\n",
    "            answer = adapted_lesk(sentence, ambiguous, pos)\n",
    "            if answer:\n",
    "                score = swn.senti_synset(answer.name())\n",
    "                endscore = 0\n",
    "                \n",
    "                if score.pos_score() > score.neg_score():\n",
    "                    endscore = score.pos_score()\n",
    "                else:\n",
    "                    endscore = score.neg_score() * (-1)\n",
    "                \n",
    "#                 words = sentence.split(' ')\n",
    "#                 word_around = []\n",
    "#                 for x in range(0, len(words)):\n",
    "#                     try:\n",
    "#                         if (words[x+1] == key) or (words[x+2] == key) or (words[x+3]== key):\n",
    "#                             word_around.append(words[x])\n",
    "#                         elif (words[x-1] == key) or (words[x-2] == key) or (words[x-3]== key):\n",
    "#                             word_around.append(words[x])\n",
    "#                     except:\n",
    "#                         pass\n",
    "                    \n",
    "                for neg in negation:\n",
    "                    if neg in sentence:\n",
    "                        endscore *= (-1)\n",
    "                        break\n",
    "                        \n",
    "                scores.append(endscore)\n",
    "            else:\n",
    "                scores.append(0)\n",
    "        else:\n",
    "            scores.append(0)\n",
    "            \n",
    "    final_score = np.average(scores)\n",
    "    return np.pi * final_score\n",
    "\n",
    "def prior_sentiment(radii, key, all_sentences):\n",
    "    theta = {'Init': 0}\n",
    "    theta.clear()\n",
    "    for word in radii.keys():\n",
    "        sentences = []\n",
    "        for sentence in all_sentences:\n",
    "            words = sentence.split(' ')\n",
    "            if (word in words) and (key in words):\n",
    "                sentences.append(sentence)\n",
    "                \n",
    "        filter = get_theta(word, sentences)            #if function returns 0 word does not exist in lexicon\n",
    "        theta[word] = filter\n",
    "        \n",
    "    return theta\n",
    "\n",
    "def senti(key, lines):\n",
    "    radii = analyse_file(key, lines)\n",
    "    theta = prior_sentiment(radii, key, lines)\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import re\n",
    "import pandas as pd\n",
    "import itertools, nltk, string \n",
    "\n",
    "def read_file(file):\n",
    "    f = open(file, 'r')\n",
    "\n",
    "    pattern_title = '\\[t\\]'\n",
    "    pattern_sentence = '(?<=##).+'\n",
    "    pattern_aspect = '.+(?=##)'\n",
    "\n",
    "    review = []\n",
    "    for a in f:\n",
    "        if re.search('##', a):\n",
    "            sentence = re.findall(pattern_sentence, a)[0]\n",
    "            aspect = re.findall(pattern_aspect, a)\n",
    "            if len(aspect) > 0:\n",
    "                aspect = aspect[0]\n",
    "            else:\n",
    "                aspect = ''\n",
    "            review.append((sentence, aspect))\n",
    "\n",
    "    df = pd.DataFrame(columns=['review','target'])\n",
    "    for r in review:\n",
    "        df = df.append({'review': r[0], 'target': r[1]}, ignore_index=True)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_program(review, target, theta, filename, tp):\n",
    "    predicts = []\n",
    "    labels = []\n",
    "    index = 0\n",
    "    \n",
    "    senti = {}\n",
    "    \n",
    "    for sentence in review:\n",
    "        aspects = target[index].split(',')\n",
    "        label = 0\n",
    "        score = 0  \n",
    "        if aspects[0] != '':\n",
    "            for aspect in aspects:\n",
    "                feature = aspect.split('[')[0]\n",
    "                tanda = ''\n",
    "                try:\n",
    "                    tanda = aspect.split('[')[1][0]\n",
    "                    angka = aspect.split('[')[1][1]\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                try:\n",
    "                    if tanda == '+':\n",
    "                        label += int(angka)\n",
    "                    else:\n",
    "                        label -= int(angka)\n",
    "                except:\n",
    "                    pass\n",
    "                          \n",
    "\n",
    "        try:\n",
    "            aspek_pasangan = tp[index].split('|')\n",
    "            if aspek_pasangan[0] != '':\n",
    "                for ap in aspek_pasangan:\n",
    "                    feature = ap.split('!')[0]\n",
    "                    for word in sentence.split(' '):\n",
    "                        if word in op_set:\n",
    "                            try:\n",
    "                                score += theta[feature][word]\n",
    "                            except:\n",
    "                                score = score\n",
    "        except:\n",
    "            k = 1\n",
    "                \n",
    "        if label != 0:\n",
    "            if label > 0:\n",
    "                labels.append(1)\n",
    "            elif label < 0:\n",
    "                labels.append(2)\n",
    "\n",
    "            if score >= 0:\n",
    "                predicts.append(1)\n",
    "            elif score < 0:\n",
    "                predicts.append(2)\n",
    "                        \n",
    "            \n",
    "        index += 1\n",
    "        \n",
    "    data = {'label': labels, 'prediction': predicts}\n",
    "    out = pd.DataFrame(data)\n",
    "    out.to_csv(filename)\n",
    "    \n",
    "    return labels, predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(sentences):\n",
    "    res = []\n",
    "    for sentence in sentences:\n",
    "        try:\n",
    "            res.append(sentence.replace(\"n't\", \"not\").replace(\"'m\", \"am\"))\n",
    "        except:\n",
    "            pass\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = [\n",
    "            'Apex AD2600 Progressive-scan DVD player',\n",
    "            'Canon G3',\n",
    "            'Creative Labs Nomad Jukebox Zend Xtra 40GB',\n",
    "            'Nikon coolpix 4300',\n",
    "            'Nokia 6610'\n",
    "]\n",
    "\n",
    "def run_all():\n",
    "    out_file = pd.DataFrame(columns=['name', 'TP', 'TN', 'FP', 'FN'])\n",
    "    for file in filename:\n",
    "        print(file)\n",
    "        TP = 0\n",
    "        TN = 0\n",
    "        FP = 0\n",
    "        FN = 0\n",
    "        \n",
    "        df = read_file('dataset/bing_liu/' + file + '.txt')\n",
    "        pred_f = pd.read_csv('hasil_clause/' + file + '.csv')\n",
    "        \n",
    "        all_theta = {}\n",
    "        all_target = []\n",
    "        for target in pred_f['prediction']:\n",
    "            try:\n",
    "                aspects = target.split('|')\n",
    "                if aspects[0] != '':\n",
    "                    for aspect in aspects:\n",
    "                        feature = aspect.split('!')[0]\n",
    "                        if feature not in all_target:\n",
    "                            all_theta[feature] = senti(feature, df['review'])\n",
    "                            all_target.append(feature)\n",
    "            except:\n",
    "                pass\n",
    "                        \n",
    "        label, pred = main_program(preprocessing(df['review']), df['target'], all_theta, 'sentiwordnet.csv', pred_f['prediction'])\n",
    "        for x in range(0, len(label)):\n",
    "            l = label[x]\n",
    "            p = pred[x]\n",
    "            if l == 1 and p == 1:\n",
    "                TP += 1\n",
    "            elif l == 2 and p == 2:\n",
    "                TN += 1\n",
    "            elif l == 1 and p == 2:\n",
    "                FP += 1\n",
    "            elif l == 2 and p == 1:\n",
    "                FN += 1\n",
    "        \n",
    "        out_file = out_file.append({'name': file, 'TP': TP, 'TN': TN, 'FP': FP, 'FN': FN}, ignore_index=True)\n",
    "        out_file.to_csv(\"result-senticircle-v2.csv\")\n",
    "        out_file.to_excel(\"result-senticircle-v2.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
