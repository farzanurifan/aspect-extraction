{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warming up PyWSD (takes ~10 secs)... took 4.607001543045044 secs.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "import math\n",
    "from math import exp, expm1, log, log10\n",
    "import numpy as np\n",
    "import turtle\n",
    "import pandas as pd\n",
    "from nltk.wsd import lesk\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "from pywsd.lesk import simple_lesk\n",
    "from nltk.corpus import stopwords\n",
    "stopWords = set(stopwords.words('english'))\n",
    "import sys, os\n",
    "\n",
    "\n",
    "nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "dependency_parser = nlp.annotate\n",
    "positive_lexicon = []\n",
    "negative_lexicon = []\n",
    "\n",
    "def read_lexicon():\n",
    "    global positive_lexicon;\n",
    "    global negative_lexicon;\n",
    "    \n",
    "    with open(os.path.join(os.path.abspath('opinion-lexicon-English/') , 'positive-words.txt'), 'r') as file:\n",
    "        line = file.readline();\n",
    "        while \";\" in line:\n",
    "            line = file.readline();\n",
    "         \n",
    "        positive_lexicon = file.readlines()\n",
    "    \n",
    "    with open(os.path.join(os.path.abspath('opinion-lexicon-English/') , 'negative-words.txt'), 'r', encoding = \"ISO-8859-1\") as file:\n",
    "        line = file.readline();\n",
    "        while \";\" in line:\n",
    "            line = file.readline();\n",
    "        \n",
    "        negative_lexicon = file.readlines()\n",
    "        \n",
    "    positive_lexicon = list(map(lambda word: word.rstrip(\"\\n\\r\"), positive_lexicon))\n",
    "    negative_lexicon = list(map(lambda word: word.rstrip(\"\\n\\r\"), negative_lexicon))\n",
    "    \n",
    "        \n",
    "read_lexicon()\n",
    "op_set = positive_lexicon + negative_lexicon\n",
    "\n",
    "negation = [\n",
    "    \"afraid\",\n",
    "    \"can't\",\n",
    "    \"cannot\",\n",
    "    \"deny\",\n",
    "    \"mean\",\n",
    "    \"negate\",\n",
    "    \"negation\",\n",
    "    \"negative\",\n",
    "    \"neither\",\n",
    "    \"never\",\n",
    "    \"no\",\n",
    "    \"non\",\n",
    "    \"none\",\n",
    "    \"nor\",\n",
    "    \"not\",\n",
    "    \"nothing\",\n",
    "    \"refusal\",\n",
    "    \"refuse\",\n",
    "    \"reject\",\n",
    "    \"rejection\"\n",
    "]\n",
    "\n",
    "def analyse_file(key, lines):    \n",
    "    radii = get_TDOC(lines, key)    \n",
    "    return radii\n",
    "\n",
    "def get_TDOC(lines, key):\n",
    "    freq = {'Init': 0}              #Number of times context term occurs with key\n",
    "    freq.clear()\n",
    "    prohib = [''] #stopWords\n",
    "    for line in lines:\n",
    "        words = line.split(\" \")\n",
    "        if key in words:\n",
    "            for context in words:\n",
    "                flag=0\n",
    "                for i in prohib:\n",
    "                    if i == context:\n",
    "                        flag=1\n",
    "                        break\n",
    "                if flag==0 and context!=key:# and context in op_set:\n",
    "                    freq.setdefault(context, 0)\n",
    "                    freq[context] = freq.get(context) + 1\n",
    "                                           \n",
    "    N = 0                           #Total Number of terms in Document\n",
    "    for line in lines:\n",
    "        words = line.split(\" \")\n",
    "        N += len(words)\n",
    "\n",
    "    Nci = {'Init': 0}               #Total terms that occur with context term\n",
    "    Nci.clear()\n",
    "    for context in freq.keys():\n",
    "        for line in lines:\n",
    "            words = line.split(\" \")\n",
    "            if context in words:\n",
    "                Nci.setdefault(context, 0)\n",
    "                Nci[context] += len(words)\n",
    "\n",
    "    radii = {'Init': 0}             #Get Radius of context term with TDOC formula\n",
    "    radii.clear()\n",
    "    \n",
    "    df = pd.DataFrame(columns=['c', 'm', 'N', 'Nc', 'f', 'N/Nc', 'log(N/Nc)', 'fxlog(N/Nc)', '/4'])\n",
    "    max_value = 0\n",
    "    for term in freq.keys():\n",
    "        radii[term] = (freq[term]*(log(N/Nci[term])))\n",
    "        \n",
    "        if radii[term] > max_value:\n",
    "            max_value = radii[term]\n",
    "        \n",
    "    for term in freq.keys():\n",
    "        radii[term] = radii[term]/max_value\n",
    "        \n",
    "        df = df.append({'c': term,\n",
    "                'm': key,\n",
    "                'N': N,\n",
    "                'Nc': Nci[term],\n",
    "                'f': freq[term],\n",
    "                'N/Nc': \"{0:.2f}\".format(N/Nci[term]),\n",
    "                'log(N/Nc)': \"{0:.2f}\".format(log(N/Nci[term])),\n",
    "                'fxlog(N/Nc)': \"{0:.2f}\".format(freq[term]*(log(N/Nci[term]))),\n",
    "                'normalisasi': \"{0:.2f}\".format((freq[term]*(log(N/Nci[term])))/max_value)\n",
    "               }, ignore_index=True)\n",
    "    \n",
    "#     df.to_excel(\"tdoc2.xlsx\")\n",
    "    return radii                    #Returns entire set of context terms related to key\n",
    "\n",
    "def pos_tag(sentence):\n",
    "    result = dependency_parser(sentence, properties={\"outputFormat\": \"json\", \"annotators\": \"pos\"})['sentences'][0]['tokens']\n",
    "    res = []\n",
    "    for pos in result:\n",
    "        res.append((pos['word'], pos['pos']))\n",
    "    return res\n",
    "\n",
    "def get_theta(key, sentences):\n",
    "    scores = []\n",
    "    for sentence in sentences:\n",
    "        flag = True\n",
    "        \n",
    "        pp_tagged = pos_tag(sentence)\n",
    "        tagged = ('','')\n",
    "        for p in pp_tagged:\n",
    "            if p[0] == key:\n",
    "                tagged = p\n",
    "        if tagged == ('', ''):\n",
    "            flag = False\n",
    "        \n",
    "        ambiguous = tagged[0]\n",
    "        tag = tagged[1]\n",
    "        pos = ''\n",
    "\n",
    "        if 'NN' in tag:\n",
    "            pos = 'n'\n",
    "        elif 'NNS' in tag:\n",
    "            pos = 'nns'\n",
    "        elif 'VB' in tag:\n",
    "            pos = 'v'\n",
    "        elif 'VBG' in tag:\n",
    "            pos = 'v'\n",
    "        elif 'JJ' in tag:\n",
    "            pos = 'a'\n",
    "        elif 'RB' in tag:\n",
    "            pos = 'r'\n",
    "        else:\n",
    "            flag = False\n",
    "\n",
    "        if flag:\n",
    "            answer = simple_lesk(sentence, ambiguous, pos)\n",
    "            if answer:\n",
    "                score = swn.senti_synset(answer.name())\n",
    "                endscore = 0\n",
    "                \n",
    "                if score.pos_score() > score.neg_score():\n",
    "                    endscore = score.pos_score()\n",
    "                else:\n",
    "                    endscore = score.neg_score() * (-1)\n",
    "                \n",
    "                words = sentence.split(' ')\n",
    "                word_around = []\n",
    "                for x in range(0, len(words)):\n",
    "                    try:\n",
    "                        if (words[x+1] == key) or (words[x+2] == key) or (words[x+3]== key):\n",
    "                            word_around.append(words[x])\n",
    "                        elif (words[x-1] == key) or (words[x-2] == key) or (words[x-3]== key):\n",
    "                            word_around.append(words[x])\n",
    "                    except:\n",
    "                        pass\n",
    "                    \n",
    "                for neg in negation:\n",
    "                    if neg in word_around:\n",
    "                        endscore *= (-1)\n",
    "                        break\n",
    "                        \n",
    "                scores.append(endscore)\n",
    "            else:\n",
    "                scores.append(0)\n",
    "        else:\n",
    "            scores.append(0)\n",
    "            \n",
    "    final_score = np.average(scores)\n",
    "    return np.pi * final_score\n",
    "\n",
    "def prior_sentiment(radii, key, all_sentences):\n",
    "    theta = {'Init': 0}\n",
    "    theta.clear()\n",
    "    for word in radii.keys():\n",
    "        sentences = []\n",
    "        for sentence in all_sentences:\n",
    "            words = sentence.split(' ')\n",
    "            if (word in words) and (key in words):\n",
    "                sentences.append(sentence)\n",
    "                \n",
    "        filter = get_theta(word, sentences)            #if function returns 0 word does not exist in lexicon\n",
    "        theta[word] = filter\n",
    "        \n",
    "    return theta\n",
    "\n",
    "def senti(key, lines):\n",
    "    radii = analyse_file(key, lines)\n",
    "    theta = prior_sentiment(radii, key, lines)\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import re\n",
    "import pandas as pd\n",
    "import itertools, nltk, string \n",
    "\n",
    "def read_file(file):\n",
    "    f = open(file, 'r')\n",
    "\n",
    "    pattern_title = '\\[t\\]'\n",
    "    pattern_sentence = '(?<=##).+'\n",
    "    pattern_aspect = '.+(?=##)'\n",
    "\n",
    "    review = []\n",
    "    for a in f:\n",
    "        if re.search('##', a):\n",
    "            sentence = re.findall(pattern_sentence, a)[0]\n",
    "            aspect = re.findall(pattern_aspect, a)\n",
    "            if len(aspect) > 0:\n",
    "                aspect = aspect[0]\n",
    "            else:\n",
    "                aspect = ''\n",
    "            review.append((sentence, aspect))\n",
    "\n",
    "    df = pd.DataFrame(columns=['review','target'])\n",
    "    for r in review:\n",
    "        df = df.append({'review': r[0], 'target': r[1]}, ignore_index=True)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_program(review, target, theta, filename):\n",
    "    predicts = []\n",
    "    labels = []\n",
    "    index = 0\n",
    "    \n",
    "    senti = {}\n",
    "    \n",
    "    for sentence in review:\n",
    "        aspects = target[index].split(',')\n",
    "        label = 0\n",
    "        score = 0  \n",
    "        if aspects[0] != '':\n",
    "            for aspect in aspects:\n",
    "                feature = aspect.split('[')[0]\n",
    "                tanda = aspect.split('[')[1][0]\n",
    "                angka = aspect.split('[')[1][1]\n",
    "                \n",
    "            \n",
    "                if tanda == '+':\n",
    "                    label += int(angka)\n",
    "                else:\n",
    "                    label -= int(angka)\n",
    "\n",
    "                                  \n",
    "                for word in sentence.split(' '):\n",
    "                    #if word in op_set:\n",
    "                    try:\n",
    "                        score += theta[feature][word]\n",
    "                    except:\n",
    "                        score = score\n",
    "                            \n",
    "            if label > 0:\n",
    "                labels.append(1)\n",
    "            elif label <= 0:\n",
    "                labels.append(2)\n",
    "#             else:            \n",
    "#                 labels.append(0)\n",
    "\n",
    "            if score >= 0:\n",
    "                predicts.append(1)\n",
    "            elif score < 0:\n",
    "                predicts.append(2)\n",
    "#             else:\n",
    "#                 predicts.append(0)\n",
    "                        \n",
    "\n",
    "#         else:\n",
    "#             labels.append(0)\n",
    "#             predicts.append(0)\n",
    "            \n",
    "        index += 1\n",
    "        \n",
    "    data = {'label': labels, 'prediction': predicts}\n",
    "    out = pd.DataFrame(data)\n",
    "    out.to_csv(filename)\n",
    "    \n",
    "    return labels, predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_theta = {}\n",
    "# all_target = []\n",
    "# for target in df['target']:\n",
    "#     aspects = target.split(', ')\n",
    "#     if aspects[0] != '':\n",
    "#         for aspect in aspects:\n",
    "#             feature = aspect.split('[')[0]\n",
    "#             if feature not in all_target:\n",
    "#                 all_theta[feature] = senti(feature, df['review'])\n",
    "#                 all_target.append(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_file('dataset/bing_liu/Nikon coolpix 4300.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "label, pred = main_program(df['review'], df['target'], all_theta, 'sentiwordnet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>113</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted    1   2\n",
       "Actual            \n",
       "1          113  16\n",
       "2           25   6"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_actu = pd.Series(label, name='Actual')\n",
    "y_pred = pd.Series(pred, name='Predicted')\n",
    "df_confusion = pd.crosstab(y_actu, y_pred)\n",
    "df_confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "population: 0\n",
      "P: 0\n",
      "N: 0\n",
      "PositiveTest: 0\n",
      "NegativeTest: 0\n",
      "TP: 0\n",
      "TN: 0\n",
      "FP: 0\n",
      "FN: 0\n",
      "TPR: nan\n",
      "TNR: nan\n",
      "PPV: nan\n",
      "NPV: nan\n",
      "FPR: nan\n",
      "FDR: nan\n",
      "FNR: nan\n",
      "ACC: nan\n",
      "F1_score: nan\n",
      "MCC: nan\n",
      "informedness: nan\n",
      "markedness: nan\n",
      "prevalence: nan\n",
      "LRP: nan\n",
      "LRN: nan\n",
      "DOR: nan\n",
      "FOR: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas_ml\\confusion_matrix\\bcm.py:191: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return(np.float64(self.TP) / self.P)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas_ml\\confusion_matrix\\bcm.py:213: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return(np.float64(self.TN) / self.N)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas_ml\\confusion_matrix\\bcm.py:236: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return(np.float64(self.TP) / self.PositiveTest)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas_ml\\confusion_matrix\\bcm.py:259: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return(np.float64(self.TN) / self.NegativeTest)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas_ml\\confusion_matrix\\bcm.py:181: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return(np.float64(self.FP) / self.N)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas_ml\\confusion_matrix\\bcm.py:267: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return(np.float64(self.FP) / self.PositiveTest)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas_ml\\confusion_matrix\\bcm.py:276: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return(np.float64(self.FN) / self.P)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas_ml\\confusion_matrix\\bcm.py:284: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return(np.float64(self.TP + self.TN) / self.population)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas_ml\\confusion_matrix\\bcm.py:293: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return(2 * np.float64(self.TP) / (2 * self.TP + self.FP + self.FN))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas_ml\\confusion_matrix\\bcm.py:302: RuntimeWarning: invalid value encountered in true_divide\n",
      "  * (self.TN + self.FP) * (self.TN + self.FN)))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas_ml\\confusion_matrix\\bcm.py:323: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return(np.float64(self.P) / self.population)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas_ml\\confusion_matrix\\bcm.py:251: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return(np.float64(self.FN) / self.NegativeTest)\n"
     ]
    }
   ],
   "source": [
    "from pandas_ml import ConfusionMatrix\n",
    "cm = ConfusionMatrix(label, pred)\n",
    "cm.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(senti('love', ['i love you', 'you love me']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
