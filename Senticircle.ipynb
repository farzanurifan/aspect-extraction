{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warming up PyWSD (takes ~10 secs)... took 4.472042560577393 secs.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "import math\n",
    "from math import exp, expm1, log, log10\n",
    "import numpy as np\n",
    "import turtle\n",
    "import pandas as pd\n",
    "from nltk.wsd import lesk\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "from pywsd.lesk import simple_lesk\n",
    "from nltk.corpus import stopwords\n",
    "stopWords = set(stopwords.words('english'))\n",
    "import sys, os\n",
    "\n",
    "\n",
    "nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "dependency_parser = nlp.annotate\n",
    "positive_lexicon = []\n",
    "negative_lexicon = []\n",
    "\n",
    "def read_lexicon():\n",
    "    global positive_lexicon;\n",
    "    global negative_lexicon;\n",
    "    \n",
    "    with open(os.path.join(os.path.abspath('opinion-lexicon-English/') , 'positive-words.txt'), 'r') as file:\n",
    "        line = file.readline();\n",
    "        while \";\" in line:\n",
    "            line = file.readline();\n",
    "         \n",
    "        positive_lexicon = file.readlines()\n",
    "    \n",
    "    with open(os.path.join(os.path.abspath('opinion-lexicon-English/') , 'negative-words.txt'), 'r', encoding = \"ISO-8859-1\") as file:\n",
    "        line = file.readline();\n",
    "        while \";\" in line:\n",
    "            line = file.readline();\n",
    "        \n",
    "        negative_lexicon = file.readlines()\n",
    "        \n",
    "    positive_lexicon = list(map(lambda word: word.rstrip(\"\\n\\r\"), positive_lexicon))\n",
    "    negative_lexicon = list(map(lambda word: word.rstrip(\"\\n\\r\"), negative_lexicon))\n",
    "    \n",
    "        \n",
    "read_lexicon()\n",
    "op_set = positive_lexicon + negative_lexicon\n",
    "\n",
    "negation = [\n",
    "    \"afraid\",\n",
    "    \"can't\",\n",
    "    \"cannot\",\n",
    "    \"deny\",\n",
    "    \"mean\",\n",
    "    \"negate\",\n",
    "    \"negation\",\n",
    "    \"negative\",\n",
    "    \"neither\",\n",
    "    \"never\",\n",
    "    \"no\",\n",
    "    \"non\",\n",
    "    \"none\",\n",
    "    \"nor\",\n",
    "    \"not\",\n",
    "    \"nothing\",\n",
    "    \"refusal\",\n",
    "    \"refuse\",\n",
    "    \"reject\",\n",
    "    \"rejection\"\n",
    "]\n",
    "\n",
    "def analyse_file(key, lines):    \n",
    "    radii = get_TDOC(lines, key)    \n",
    "    return radii\n",
    "\n",
    "def get_TDOC(lines, key):\n",
    "    freq = {'Init': 0}              #Number of times context term occurs with key\n",
    "    freq.clear()\n",
    "    prohib = [''] #stopWords\n",
    "    for line in lines:\n",
    "        words = line.split(\" \")\n",
    "        if key in words:\n",
    "            for context in words:\n",
    "                flag=0\n",
    "                for i in prohib:\n",
    "                    if i == context:\n",
    "                        flag=1\n",
    "                        break\n",
    "                if flag==0 and context!=key and context in op_set:\n",
    "                    freq.setdefault(context, 0)\n",
    "                    freq[context] = freq.get(context) + 1\n",
    "                                           \n",
    "    N = 0                           #Total Number of terms in Document\n",
    "    for line in lines:\n",
    "        words = line.split(\" \")\n",
    "        N += len(words)\n",
    "\n",
    "    Nci = {'Init': 0}               #Total terms that occur with context term\n",
    "    Nci.clear()\n",
    "    for context in freq.keys():\n",
    "        for line in lines:\n",
    "            words = line.split(\" \")\n",
    "            if context in words:\n",
    "                Nci.setdefault(context, 0)\n",
    "                Nci[context] += len(words)\n",
    "\n",
    "    radii = {'Init': 0}             #Get Radius of context term with TDOC formula\n",
    "    radii.clear()\n",
    "    \n",
    "    df = pd.DataFrame(columns=['c', 'm', 'N', 'Nc', 'f', 'N/Nc', 'log(N/Nc)', 'fxlog(N/Nc)', '/4'])\n",
    "    max_value = 0\n",
    "    for term in freq.keys():\n",
    "        radii[term] = (freq[term]*(log(N/Nci[term])))\n",
    "        \n",
    "        if radii[term] > max_value:\n",
    "            max_value = radii[term]\n",
    "        \n",
    "    for term in freq.keys():\n",
    "        radii[term] = radii[term]/max_value\n",
    "        \n",
    "        df = df.append({'c': term,\n",
    "                'm': key,\n",
    "                'N': N,\n",
    "                'Nc': Nci[term],\n",
    "                'f': freq[term],\n",
    "                'N/Nc': \"{0:.2f}\".format(N/Nci[term]),\n",
    "                'log(N/Nc)': \"{0:.2f}\".format(log(N/Nci[term])),\n",
    "                'fxlog(N/Nc)': \"{0:.2f}\".format(freq[term]*(log(N/Nci[term]))),\n",
    "                'normalisasi': \"{0:.2f}\".format((freq[term]*(log(N/Nci[term])))/max_value)\n",
    "               }, ignore_index=True)\n",
    "    \n",
    "#     df.to_excel(\"tdoc2.xlsx\")\n",
    "    return radii                    #Returns entire set of context terms related to key\n",
    "\n",
    "def pos_tag(sentence):\n",
    "    result = dependency_parser(sentence, properties={\"outputFormat\": \"json\", \"annotators\": \"pos\"})['sentences'][0]['tokens']\n",
    "    res = []\n",
    "    for pos in result:\n",
    "        res.append((pos['word'], pos['pos']))\n",
    "    return res\n",
    "\n",
    "def get_theta(key, sentences):\n",
    "    scores = []\n",
    "    for sentence in sentences:\n",
    "        flag = True\n",
    "        \n",
    "        pp_tagged = pos_tag(sentence)\n",
    "        tagged = ('','')\n",
    "        for p in pp_tagged:\n",
    "            if p[0] == key:\n",
    "                tagged = p\n",
    "        if tagged == ('', ''):\n",
    "            flag = False\n",
    "        \n",
    "        ambiguous = tagged[0]\n",
    "        tag = tagged[1]\n",
    "        pos = ''\n",
    "\n",
    "        if 'NN' in tag:\n",
    "            pos = 'n'\n",
    "        elif 'NNS' in tag:\n",
    "            pos = 'nns'\n",
    "        elif 'VB' in tag:\n",
    "            pos = 'v'\n",
    "        elif 'VBG' in tag:\n",
    "            pos = 'v'\n",
    "        elif 'JJ' in tag:\n",
    "            pos = 'a'\n",
    "        elif 'RB' in tag:\n",
    "            pos = 'r'\n",
    "        else:\n",
    "            flag = False\n",
    "\n",
    "        if flag:\n",
    "            answer = simple_lesk(sentence, ambiguous, pos)\n",
    "            if answer:\n",
    "                score = swn.senti_synset(answer.name())\n",
    "                endscore = 0\n",
    "                \n",
    "                if score.pos_score() > score.neg_score():\n",
    "                    endscore = score.pos_score()\n",
    "                else:\n",
    "                    endscore = score.neg_score() * (-1)\n",
    "                \n",
    "                words = sentence.split(' ')\n",
    "                word_around = []\n",
    "                for x in range(0, len(words)):\n",
    "                    try:\n",
    "                        if (words[x+1] == key) or (words[x+2] == key) or (words[x+3]== key):\n",
    "                            word_around.append(words[x])\n",
    "                        elif (words[x-1] == key) or (words[x-2] == key) or (words[x-3]== key):\n",
    "                            word_around.append(words[x])\n",
    "                    except:\n",
    "                        pass\n",
    "                    \n",
    "                for neg in negation:\n",
    "                    if neg in word_around:\n",
    "                        endscore *= (-1)\n",
    "                        break\n",
    "                        \n",
    "                scores.append(endscore)\n",
    "            else:\n",
    "                scores.append(0)\n",
    "        else:\n",
    "            scores.append(0)\n",
    "            \n",
    "    final_score = np.average(scores)\n",
    "    return np.pi * final_score\n",
    "\n",
    "def prior_sentiment(radii, key, all_sentences):\n",
    "    theta = {'Init': 0}\n",
    "    theta.clear()\n",
    "    for word in radii.keys():\n",
    "        sentences = []\n",
    "        for sentence in all_sentences:\n",
    "            words = sentence.split(' ')\n",
    "            if (word in words) and (key in words):\n",
    "                sentences.append(sentence)\n",
    "                \n",
    "        filter = get_theta(word, sentences)            #if function returns 0 word does not exist in lexicon\n",
    "        theta[word] = filter\n",
    "        \n",
    "    return theta\n",
    "\n",
    "def senti(key, lines):\n",
    "    radii = analyse_file(key, lines)\n",
    "    theta = prior_sentiment(radii, key, lines)\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import re\n",
    "import pandas as pd\n",
    "import itertools, nltk, string \n",
    "\n",
    "def read_file(file):\n",
    "    f = open(file, 'r')\n",
    "\n",
    "    pattern_title = '\\[t\\]'\n",
    "    pattern_sentence = '(?<=##).+'\n",
    "    pattern_aspect = '.+(?=##)'\n",
    "\n",
    "    review = []\n",
    "    for a in f:\n",
    "        if re.search('##', a):\n",
    "            sentence = re.findall(pattern_sentence, a)[0]\n",
    "            aspect = re.findall(pattern_aspect, a)\n",
    "            if len(aspect) > 0:\n",
    "                aspect = aspect[0]\n",
    "            else:\n",
    "                aspect = ''\n",
    "            review.append((sentence, aspect))\n",
    "\n",
    "    df = pd.DataFrame(columns=['review','target'])\n",
    "    for r in review:\n",
    "        df = df.append({'review': r[0], 'target': r[1]}, ignore_index=True)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_program(review, target, theta, filename):\n",
    "    predicts = []\n",
    "    labels = []\n",
    "    index = 0\n",
    "    \n",
    "    senti = {}\n",
    "    \n",
    "    for sentence in review:\n",
    "        aspects = target[index].split(',')\n",
    "        label = 0\n",
    "        score = 0  \n",
    "        if aspects[0] != '':\n",
    "            for aspect in aspects:\n",
    "                feature = aspect.split('[')[0]\n",
    "                tanda = aspect.split('[')[1][0]\n",
    "                angka = aspect.split('[')[1][1]\n",
    "                \n",
    "            \n",
    "                if tanda == '+':\n",
    "                    label += int(angka)\n",
    "                else:\n",
    "                    label -= int(angka)\n",
    "\n",
    "                                  \n",
    "                for word in sentence.split(' '):\n",
    "                    if word in op_set:\n",
    "                        try:\n",
    "                            score += theta[feature][word]\n",
    "                        except:\n",
    "                            score = score\n",
    "                            \n",
    "            if label >= 0:\n",
    "                labels.append(1)\n",
    "            elif label < 0:\n",
    "                labels.append(2)\n",
    "#             else:            \n",
    "#                 labels.append(0)\n",
    "\n",
    "            if score >= 0:\n",
    "                predicts.append(1)\n",
    "            elif score < 0:\n",
    "                predicts.append(2)\n",
    "#             else:\n",
    "#                 predicts.append(0)\n",
    "                        \n",
    "\n",
    "        else:\n",
    "            labels.append(0)\n",
    "            predicts.append(0)\n",
    "            \n",
    "        index += 1\n",
    "        \n",
    "    data = {'label': labels, 'prediction': predicts}\n",
    "    out = pd.DataFrame(data)\n",
    "    out.to_csv(filename)\n",
    "    \n",
    "    return labels, predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_theta = {}\n",
    "# all_target = []\n",
    "# for target in df['target']:\n",
    "#     aspects = target.split(', ')\n",
    "#     if aspects[0] != '':\n",
    "#         for aspect in aspects:\n",
    "#             feature = aspect.split('[')[0]\n",
    "#             if feature not in all_target:\n",
    "#                 all_theta[feature] = senti(feature, df['review'])\n",
    "#                 all_target.append(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_file('dataset/bing_liu/Nikon coolpix 4300.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "label, pred = main_program(df['review'], df['target'], all_theta, 'sentiwordnet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>186</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>116</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted    0    1   2\n",
       "Actual                 \n",
       "0          186    0   0\n",
       "1            0  116  14\n",
       "2            0   25   5"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_actu = pd.Series(label, name='Actual')\n",
    "y_pred = pd.Series(pred, name='Predicted')\n",
    "df_confusion = pd.crosstab(y_actu, y_pred)\n",
    "df_confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "\n",
      "Predicted    0    1   2  __all__\n",
      "Actual                          \n",
      "0          186    0   0      186\n",
      "1            0  116  14      130\n",
      "2            0   25   5       30\n",
      "__all__    186  141  19      346\n",
      "\n",
      "\n",
      "Overall Statistics:\n",
      "\n",
      "Accuracy: 0.8872832369942196\n",
      "95% CI: (0.8491447202686204, 0.9186086931747108)\n",
      "No Information Rate: ToDo\n",
      "P-Value [Acc > NIR]: 9.938995662017918e-45\n",
      "Kappa: 0.7962247055270312\n",
      "Mcnemar's Test P-Value: ToDo\n",
      "\n",
      "\n",
      "Class Statistics:\n",
      "\n",
      "Classes                                       0          1          2\n",
      "Population                                  346        346        346\n",
      "P: Condition positive                       186        130         30\n",
      "N: Condition negative                       160        216        316\n",
      "Test outcome positive                       186        141         19\n",
      "Test outcome negative                       160        205        327\n",
      "TP: True Positive                           186        116          5\n",
      "TN: True Negative                           160        191        302\n",
      "FP: False Positive                            0         25         14\n",
      "FN: False Negative                            0         14         25\n",
      "TPR: (Sensitivity, hit rate, recall)          1   0.892308   0.166667\n",
      "TNR=SPC: (Specificity)                        1   0.884259   0.955696\n",
      "PPV: Pos Pred Value (Precision)               1   0.822695   0.263158\n",
      "NPV: Neg Pred Value                           1   0.931707   0.923547\n",
      "FPR: False-out                                0   0.115741  0.0443038\n",
      "FDR: False Discovery Rate                     0   0.177305   0.736842\n",
      "FNR: Miss Rate                                0   0.107692   0.833333\n",
      "ACC: Accuracy                                 1   0.887283   0.887283\n",
      "F1 score                                      1   0.856089   0.204082\n",
      "MCC: Matthews correlation coefficient         1   0.765404   0.151148\n",
      "Informedness                                  1   0.776567   0.122363\n",
      "Markedness                                    1   0.754402   0.186705\n",
      "Prevalence                             0.537572   0.375723  0.0867052\n",
      "LR+: Positive likelihood ratio              inf    7.70954     3.7619\n",
      "LR-: Negative likelihood ratio                0   0.121788   0.871965\n",
      "DOR: Diagnostic odds ratio                  inf    63.3029    4.31429\n",
      "FOR: False omission rate                      0  0.0682927  0.0764526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas_ml\\confusion_matrix\\stats.py:60: FutureWarning: supplying multiple axes to axis is deprecated and will be removed in a future version.\n",
      "  num = df[df > 1].dropna(axis=[0, 1], thresh=1).applymap(lambda n: choose(n, 2)).sum().sum() - np.float64(nis2 * njs2) / n2\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas_ml\\confusion_matrix\\bcm.py:330: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  return(np.float64(self.TPR) / self.FPR)\n"
     ]
    }
   ],
   "source": [
    "from pandas_ml import ConfusionMatrix\n",
    "cm = ConfusionMatrix(label, pred)\n",
    "cm.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(senti('love', ['i love you', 'you love me']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
