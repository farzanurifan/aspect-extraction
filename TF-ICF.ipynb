{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import math\n",
    "\n",
    "def generate_tficf(reviews, n_cluster = 4):\n",
    "    tf_word_freq = {}\n",
    "    for n in range(n_cluster):\n",
    "        tf_word_freq[n] = {}\n",
    "   \n",
    "    tf = []\n",
    "    for index, review in enumerate(reviews):\n",
    "        for sentence in nltk.sent_tokenize(review):\n",
    "            for word in nltk.word_tokenize(sentence):\n",
    "                if word in tf_word_freq[index]:\n",
    "                    tf_word_freq[index][word] += 1\n",
    "                else:\n",
    "                    tf_word_freq[index][word] = 1\n",
    "\n",
    "                if word not in tf:\n",
    "                    tf.append(word)\n",
    "                    \n",
    "                    \n",
    "    tf_icf = []\n",
    "    for term in tf:\n",
    "        tf_in_c = {}\n",
    "        c_i = 0\n",
    "\n",
    "\n",
    "        for n in range(n_cluster):\n",
    "            tf_in_c[n] = 0\n",
    "            \n",
    "            if term in tf_word_freq[n]:\n",
    "                tf_in_c[n] = tf_word_freq[n][term]\n",
    "                c_i += 1\n",
    "\n",
    "\n",
    "        icf = 1 + math.log(n_cluster/c_i)\n",
    "        t = ()\n",
    "        t += (term,)\n",
    "        for n in range(n_cluster):\n",
    "            t += (tf_in_c[n]*icf, )\n",
    "        tf_icf.append(t)\n",
    "    \n",
    "    return tf_icf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "nltk_words = set(nltk.corpus.stopwords.words('english'))\n",
    "stop_words = []\n",
    "for word in nltk_words:\n",
    "    stop_words.append(word.translate(str.maketrans('', '', string.punctuation)))\n",
    "\n",
    "def preprocess(sentence):\n",
    "    res = sentence.lower()\n",
    "    res = res.translate(str.maketrans('', '', string.punctuation))\n",
    "    tokenized_words = nltk.word_tokenize(res)\n",
    "    res = [word for word in tokenized_words if word not in stop_words]\n",
    "    res = [lemmatizer.lemmatize(r) for r in res]\n",
    "    res = [re.sub(r\"[^A-Za-z]+\", '', r) for r in res]\n",
    "    res = [r for r in res if len(r) > 3]\n",
    "    return ' '.join(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_file(kategori):\n",
    "    f = open('wiki/source/' + kategori + '-sentence.txt', 'r', encoding='utf-8')\n",
    "    all_sentences = []\n",
    "    for line in f:\n",
    "        all_sentences.append(preprocess(str(line).replace('\\n','')))\n",
    "    return ' '.join(all_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_daftar(name, daftar):\n",
    "    pickle_out = open(\"Term/daftar_\" + name + \".pickle\",\"wb\")\n",
    "    pickle.dump(daftar, pickle_out)\n",
    "    pickle_out.close()\n",
    "\n",
    "def load_daftar(name):\n",
    "    pickle_in = open(\"Term/daftar_\" + name + \".pickle\", \"rb\")\n",
    "    return pickle.load(pickle_in)\n",
    "\n",
    "def gabung_file(name):\n",
    "    daftar = load_daftar(name)\n",
    "    res = []\n",
    "    for judul in daftar:\n",
    "        res.append(open_file(judul))\n",
    "    return ' '.join(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keyword(index, top_n, data_tf):\n",
    "    sorted_word = sorted(data_tf, key=lambda value: value[index], reverse=True)\n",
    "    keyword = sorted_word[0:top_n]\n",
    "    max_k = keyword[0][index]\n",
    "    min_k = keyword[top_n - 1][index]\n",
    "    keyword = list(map(lambda value: (value[0], (value[1] - min_k) / (max_k - min_k)), keyword))\n",
    "    return keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def run(name, index, top_n, data_tf):\n",
    "    all_keyword = []\n",
    "    for a, b in get_keyword(index, top_n, data_tf):\n",
    "        all_keyword.append(a)\n",
    "    print(all_keyword)\n",
    "    pickle_out = open(\"Term/\" + name + \".pickle\",\"wb\")\n",
    "    pickle.dump(all_keyword, pickle_out)\n",
    "    pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [('ambience', 1), ('food', 2), ('service', 3), ('price', 4)]\n",
    "\n",
    "save_daftar('ambience', ['Theme restaurant', 'Atmosphere (architecture and spatial design)', 'Ambience (sound recording)'])\n",
    "save_daftar('food', ['Food', 'Drink', 'Meal'])\n",
    "save_daftar('service', ['Customer service', \"Waiting staff\"])\n",
    "save_daftar('price', [\"Price\", \"Pricing\"])\n",
    "    \n",
    "ambience = gabung_file('ambience')\n",
    "food = gabung_file('food')\n",
    "service = gabung_file('service')\n",
    "price = gabung_file('price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['restaurant', 'atmosphere', 'theme', 'space', 'architecture', 'sound', 'location', 'located', 'light', 'building']\n",
      "['food', 'drink', 'diet', 'fruit', 'health', 'meal', 'juice', 'breakfast', 'vegetable', 'cooking']\n",
      "['customer', 'service', 'server', 'waiting', 'restaurant', 'staff', 'feedback', 'food', 'table', 'serving']\n",
      "['price', 'pricing', 'product', 'premium', 'service', 'buyer', 'market', 'cost', 'consumer', 'value']\n"
     ]
    }
   ],
   "source": [
    "data_tf = generate_tficf([ambience, food, service, price], 4)\n",
    "for name, index in data:\n",
    "    run(name, index, 10, data_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['restaurant', 'atmosphere', 'theme', 'space', 'architecture', 'sound', 'location', 'located', 'light', 'building']\n",
      "['food', 'drink', 'diet', 'fruit', 'health', 'meal', 'juice', 'breakfast', 'vegetable', 'cooking']\n",
      "['customer', 'service', 'server', 'waiting', 'restaurant', 'staff', 'feedback', 'food', 'table', 'serving']\n",
      "['price', 'pricing', 'product', 'premium', 'service', 'buyer', 'market', 'cost', 'consumer', 'value']\n"
     ]
    }
   ],
   "source": [
    "data_tf = generate_tficf([ambience, food, service, price], 4)\n",
    "for name, index in data:\n",
    "    run(name, index, 10, data_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
