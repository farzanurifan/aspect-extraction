{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warming up PyWSD (takes ~10 secs)... took 5.22561240196228 secs.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "import math\n",
    "from math import exp, expm1, log, log10\n",
    "import numpy as np\n",
    "import turtle\n",
    "import pandas as pd\n",
    "from nltk.wsd import lesk\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "from pywsd.lesk import simple_lesk\n",
    "from nltk.corpus import stopwords\n",
    "stopWords = set(stopwords.words('english'))\n",
    "import sys, os\n",
    "\n",
    "\n",
    "nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "dependency_parser = nlp.annotate\n",
    "# positive_lexicon = []\n",
    "# negative_lexicon = []\n",
    "\n",
    "# def read_lexicon():\n",
    "#     global positive_lexicon;\n",
    "#     global negative_lexicon;\n",
    "    \n",
    "#     with open(os.path.join(os.path.abspath('../opinion-lexicon-English/') , 'positive-words.txt'), 'r') as file:\n",
    "#         line = file.readline();\n",
    "#         while \";\" in line:\n",
    "#             line = file.readline();\n",
    "         \n",
    "#         positive_lexicon = file.readlines()\n",
    "    \n",
    "#     with open(os.path.join(os.path.abspath('../opinion-lexicon-English/') , 'negative-words.txt'), 'r', encoding = \"ISO-8859-1\") as file:\n",
    "#         line = file.readline();\n",
    "#         while \";\" in line:\n",
    "#             line = file.readline();\n",
    "        \n",
    "#         negative_lexicon = file.readlines()\n",
    "        \n",
    "#     positive_lexicon = list(map(lambda word: word.rstrip(\"\\n\\r\"), positive_lexicon))\n",
    "#     negative_lexicon = list(map(lambda word: word.rstrip(\"\\n\\r\"), negative_lexicon))\n",
    "    \n",
    "        \n",
    "# read_lexicon()\n",
    "# op_set = positive_lexicon + negative_lexicon\n",
    "\n",
    "negation = [\n",
    "    \"afraid\",\n",
    "    \"can't\",\n",
    "    \"cannot\",\n",
    "    \"deny\",\n",
    "    \"mean\",\n",
    "    \"negate\",\n",
    "    \"negation\",\n",
    "    \"negative\",\n",
    "    \"neither\",\n",
    "    \"never\",\n",
    "    \"no\",\n",
    "    \"non\",\n",
    "    \"none\",\n",
    "    \"nor\",\n",
    "    \"not\",\n",
    "    \"nothing\",\n",
    "    \"refusal\",\n",
    "    \"refuse\",\n",
    "    \"reject\",\n",
    "    \"rejection\"\n",
    "]\n",
    "\n",
    "def analyse_file(key, lines):    \n",
    "    radii = get_TDOC(lines, key)    \n",
    "    return radii\n",
    "\n",
    "def get_TDOC(lines, key):\n",
    "    freq = {'Init': 0}              #Number of times context term occurs with key\n",
    "    freq.clear()\n",
    "    prohib = ['']#stopWords\n",
    "    for line in lines:\n",
    "        words = line.split(\" \")\n",
    "        if key in words:\n",
    "            for context in words:\n",
    "                flag=0\n",
    "                for i in prohib:\n",
    "                    if i == context:\n",
    "                        flag=1\n",
    "                        break\n",
    "                if flag==0 and context!=key:# and context in op_set:\n",
    "                    freq.setdefault(context, 0)\n",
    "                    freq[context] = freq.get(context) + 1\n",
    "                                           \n",
    "    N = 0                           #Total Number of terms in Document\n",
    "    for line in lines:\n",
    "        words = line.split(\" \")\n",
    "        N += len(words)\n",
    "\n",
    "    Nci = {'Init': 0}               #Total terms that occur with context term\n",
    "    Nci.clear()\n",
    "    for context in freq.keys():\n",
    "        for line in lines:\n",
    "            words = line.split(\" \")\n",
    "            if context in words:\n",
    "                Nci.setdefault(context, 0)\n",
    "                Nci[context] += len(words)\n",
    "\n",
    "    radii = {'Init': 0}             #Get Radius of context term with TDOC formula\n",
    "    radii.clear()\n",
    "    \n",
    "    df = pd.DataFrame(columns=['c', 'm', 'N', 'Nc', 'f', 'N/Nc', 'log(N/Nc)', 'fxlog(N/Nc)', '/4'])\n",
    "    max_value = 0\n",
    "    for term in freq.keys():\n",
    "        radii[term] = (freq[term]*(log(N/Nci[term])))\n",
    "        \n",
    "        if radii[term] > max_value:\n",
    "            max_value = radii[term]\n",
    "        \n",
    "    for term in freq.keys():\n",
    "        radii[term] = radii[term]/max_value\n",
    "        \n",
    "        df = df.append({'c': term,\n",
    "                'm': key,\n",
    "                'N': N,\n",
    "                'Nc': Nci[term],\n",
    "                'f': freq[term],\n",
    "                'N/Nc': \"{0:.2f}\".format(N/Nci[term]),\n",
    "                'log(N/Nc)': \"{0:.2f}\".format(log(N/Nci[term])),\n",
    "                'fxlog(N/Nc)': \"{0:.2f}\".format(freq[term]*(log(N/Nci[term]))),\n",
    "                'normalisasi': \"{0:.2f}\".format((freq[term]*(log(N/Nci[term])))/max_value)\n",
    "               }, ignore_index=True)\n",
    "    \n",
    "#     df.to_excel(\"tdoc2.xlsx\")\n",
    "    return radii                    #Returns entire set of context terms related to key\n",
    "\n",
    "def pos_tag(sentence):\n",
    "    result = dependency_parser(sentence, properties={\"outputFormat\": \"json\", \"annotators\": \"pos\"})['sentences'][0]['tokens']\n",
    "    res = []\n",
    "    for pos in result:\n",
    "        res.append((pos['word'], pos['pos']))\n",
    "    return res\n",
    "\n",
    "def get_theta(key, sentences):\n",
    "    scores = []\n",
    "    for sentence in sentences:\n",
    "        flag = True\n",
    "        \n",
    "        pp_tagged = pos_tag(sentence)\n",
    "        tagged = ('','')\n",
    "        for p in pp_tagged:\n",
    "            if p[0] == key:\n",
    "                tagged = p\n",
    "        if tagged == ('', ''):\n",
    "            flag = False\n",
    "        \n",
    "        ambiguous = tagged[0]\n",
    "        tag = tagged[1]\n",
    "        pos = ''\n",
    "\n",
    "        if 'NN' in tag:\n",
    "            pos = 'n'\n",
    "        elif 'NNS' in tag:\n",
    "            pos = 'nns'\n",
    "        elif 'VB' in tag:\n",
    "            pos = 'v'\n",
    "        elif 'VBG' in tag:\n",
    "            pos = 'v'\n",
    "        elif 'JJ' in tag:\n",
    "            pos = 'a'\n",
    "        elif 'RB' in tag:\n",
    "            pos = 'r'\n",
    "        else:\n",
    "            flag = False\n",
    "\n",
    "        if flag:\n",
    "            answer = simple_lesk(sentence, ambiguous, pos)\n",
    "            if answer:\n",
    "                score = swn.senti_synset(answer.name())\n",
    "                endscore = 0\n",
    "                \n",
    "                if score.pos_score() > score.neg_score():\n",
    "                    endscore = score.pos_score()\n",
    "                else:\n",
    "                    endscore = score.neg_score() * (-1)\n",
    "                \n",
    "                words = sentence.split(' ')\n",
    "                word_around = []\n",
    "                for x in range(0, len(words)):\n",
    "                    try:\n",
    "                        if (words[x+1] == key) or (words[x+2] == key) or (words[x+3]== key):\n",
    "                            word_around.append(words[x])\n",
    "                        elif (words[x-1] == key) or (words[x-2] == key) or (words[x-3]== key):\n",
    "                            word_around.append(words[x])\n",
    "                    except:\n",
    "                        pass\n",
    "                    \n",
    "                for neg in negation:\n",
    "                    if neg in word_around:\n",
    "                        endscore *= (-1)\n",
    "                        break\n",
    "                        \n",
    "                scores.append(endscore)\n",
    "            else:\n",
    "                scores.append(0)\n",
    "        else:\n",
    "            scores.append(0)\n",
    "            \n",
    "    final_score = np.average(scores)\n",
    "    return np.pi * final_score\n",
    "\n",
    "def prior_sentiment(radii, key, all_sentences):\n",
    "    theta = {'Init': 0}\n",
    "    theta.clear()\n",
    "    for word in radii.keys():\n",
    "        sentences = []\n",
    "        for sentence in all_sentences:\n",
    "            words = sentence.split(' ')\n",
    "            if (word in words) and (key in words):\n",
    "                sentences.append(sentence)\n",
    "                \n",
    "        filter = get_theta(word, sentences)            #if function returns 0 word does not exist in lexicon\n",
    "        theta[word] = filter\n",
    "        \n",
    "    return theta\n",
    "\n",
    "def senti(key, lines):\n",
    "    radii = analyse_file(key, lines)\n",
    "    theta = prior_sentiment(radii, key, lines)\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv(\"nokia.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target(data):\n",
    "    index = 0\n",
    "    aspect = {}\n",
    "    for aspects in data:\n",
    "        index += 1\n",
    "        if aspects is not np.nan:\n",
    "            for aspect_pair in aspects.split('|'):\n",
    "                temp = aspect_pair.split('!')\n",
    "                aspek = temp[0]\n",
    "                try:\n",
    "                    senti = temp[1]\n",
    "                except:\n",
    "                    senti = ''\n",
    "                try:\n",
    "                    aspect[aspek]\n",
    "                    if senti == '+':\n",
    "                        aspect[aspek]['positive'] += 1\n",
    "                    elif senti == '-':\n",
    "                        aspect[aspek]['negative'] += 1\n",
    "                except:\n",
    "                    if senti == '+':\n",
    "                        aspect[aspek] = {'positive': 1, 'negative': 0}\n",
    "                    elif senti == '-':\n",
    "                        aspect[aspek] = {'positive': 0, 'negative': 1}\n",
    "                    else:\n",
    "                        aspect[aspek] = {'positive': 0, 'negative': 0}\n",
    "    return aspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aspect(data):\n",
    "    index = 0\n",
    "    aspect = {}\n",
    "    for aspects in data:\n",
    "        index += 1\n",
    "        if aspects is not np.nan:\n",
    "            for aspect_pair in aspects.split('|'):\n",
    "                if aspect_pair != '':\n",
    "                    temp = aspect_pair.split('!')\n",
    "                    aspek = temp[0]\n",
    "                    opini = temp[1]\n",
    "                    try:\n",
    "                        aspect[aspek] += '|' + opini\n",
    "                    except:\n",
    "                        aspect[aspek] = opini\n",
    "    return aspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "targets = target(data['aspect'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspects = aspect(data['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(aspects, targets, data):\n",
    "    aspect = {}\n",
    "    for aspek in aspects:\n",
    "        if aspek in targets:\n",
    "            theta = senti(aspek, data)\n",
    "            for opini in aspects[aspek].split('|'):\n",
    "                try:\n",
    "                    aspect[aspek]\n",
    "                    if theta[opini] > 0:\n",
    "                        aspect[aspek]['positive'] += 1\n",
    "                    else:#if theta[opini] < 0:\n",
    "                        aspect[aspek]['negative'] += 1\n",
    "                except:\n",
    "                    if theta[opini] > 0:\n",
    "                        aspect[aspek] = {'positive': 1, 'negative': 0}\n",
    "                    else:#if theta[opini] < 0:\n",
    "                        aspect[aspek] = {'positive': 0, 'negative': 1}\n",
    "    return aspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_wordnet(aspects, targets, data):\n",
    "    aspect = {}\n",
    "    for aspek in aspects:\n",
    "        if aspek in targets:\n",
    "            for opini in aspects[aspek].split('|'):\n",
    "                try:\n",
    "                    score = swn.senti_synset(opini + '.a.1')\n",
    "                    p_s = score.pos_score()\n",
    "                    n_s = score.neg_score()\n",
    "                except:\n",
    "                    p_s = 0\n",
    "                    n_s = 1\n",
    "                try:\n",
    "                    aspect[aspek]\n",
    "                    if p_s >= n_s:\n",
    "                        aspect[aspek]['positive'] += 1\n",
    "                    else:\n",
    "                        aspect[aspek]['negative'] += 1\n",
    "                except:\n",
    "                    if p_s >= n_s:\n",
    "                        aspect[aspek] = {'positive': 1, 'negative': 0}\n",
    "                    else:\n",
    "                        aspect[aspek] = {'positive': 0, 'negative': 1}\n",
    "    return aspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate(targets, predictions):\n",
    "    t = 0\n",
    "    f = 0\n",
    "    total_p = 0\n",
    "    total_t = 0\n",
    "    \n",
    "    for predict in predictions:\n",
    "        t_pos = targets[predict]['positive']\n",
    "        t_neg = targets[predict]['negative']\n",
    "\n",
    "        p_pos = predictions[predict]['positive']\n",
    "        p_neg = predictions[predict]['negative']\n",
    "        \n",
    "#         print(predict)\n",
    "#         print(targets[predict])\n",
    "#         print(predictions[predict])\n",
    "#         print()\n",
    "        \n",
    "        if p_pos >= t_pos:\n",
    "            t += t_pos\n",
    "            f += (p_pos - t_pos)\n",
    "        else:\n",
    "            t += p_pos\n",
    "        \n",
    "        if p_neg >= t_neg:\n",
    "            t += t_neg\n",
    "            f += (p_neg - t_neg)\n",
    "        else:\n",
    "            t += p_neg\n",
    "            \n",
    "        total_p += (p_pos + p_neg)\n",
    "        total_t += (t_pos + t_neg)\n",
    "#     print(total_t, total_p)\n",
    "    akurasi = t/total_p\n",
    "    return t, f, akurasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = predict(aspects, targets, data['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_w = predict_wordnet(aspects, targets, data['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(138, 194, 0.41566265060240964)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate(targets, prediction) #senticircle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(149, 183, 0.44879518072289154)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate(targets, prediction_w) #sentiwordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
