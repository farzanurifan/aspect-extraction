{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy.load('en')\n",
    "from spacy.lang.en import English\n",
    "parser = English()\n",
    "def tokenize(text):\n",
    "    lda_tokens = []\n",
    "    tokens = parser(text)\n",
    "    for token in tokens:\n",
    "        if token.orth_.isspace():\n",
    "            continue\n",
    "        elif token.like_url:\n",
    "            lda_tokens.append('URL')\n",
    "        elif token.orth_.startswith('@'):\n",
    "            lda_tokens.append('SCREEN_NAME')\n",
    "        else:\n",
    "            lda_tokens.append(token.lower_)\n",
    "    return lda_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Farza\n",
      "[nltk_data]     Nurifan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "def get_lemma(word):\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemma\n",
    "    \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "def get_lemma2(word):\n",
    "    return WordNetLemmatizer().lemmatize(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Farza\n",
      "[nltk_data]     Nurifan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text_for_lda(text):\n",
    "    tokens = tokenize(text)\n",
    "    tokens = [token for token in tokens if len(token) > 4]\n",
    "    tokens = [token for token in tokens if token not in en_stop]\n",
    "    tokens = [get_lemma(token) for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>reviewID</th>\n",
       "      <th>sentenceID</th>\n",
       "      <th>review</th>\n",
       "      <th>category</th>\n",
       "      <th>polarity</th>\n",
       "      <th>entity</th>\n",
       "      <th>preprocessed_sentence</th>\n",
       "      <th>type_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>RL#3</td>\n",
       "      <td>RL#3:1</td>\n",
       "      <td>I am not necessarily fanatical about this plac...</td>\n",
       "      <td>VALUE#PRICES</td>\n",
       "      <td>positive</td>\n",
       "      <td>VALUE</td>\n",
       "      <td>i am not necessarily fanatical about this plac...</td>\n",
       "      <td>compound_sentence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>TR#2</td>\n",
       "      <td>TR#2:2</td>\n",
       "      <td>The high prices you're going to pay is for the...</td>\n",
       "      <td>VALUE#PRICES</td>\n",
       "      <td>negative</td>\n",
       "      <td>VALUE</td>\n",
       "      <td>the high prices you 're going to pay is for th...</td>\n",
       "      <td>complex_sentence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>TR#2</td>\n",
       "      <td>TR#2:2</td>\n",
       "      <td>The high prices you're going to pay is for the...</td>\n",
       "      <td>VALUE#PRICES</td>\n",
       "      <td>negative</td>\n",
       "      <td>VALUE</td>\n",
       "      <td>the high prices you 're going to pay is for th...</td>\n",
       "      <td>complex_sentence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>TR#2</td>\n",
       "      <td>TR#2:2</td>\n",
       "      <td>The high prices you're going to pay is for the...</td>\n",
       "      <td>VALUE#PRICES</td>\n",
       "      <td>negative</td>\n",
       "      <td>VALUE</td>\n",
       "      <td>the high prices you 're going to pay is for th...</td>\n",
       "      <td>complex_sentence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>TR#2</td>\n",
       "      <td>TR#2:2</td>\n",
       "      <td>The high prices you're going to pay is for the...</td>\n",
       "      <td>VALUE#PRICES</td>\n",
       "      <td>negative</td>\n",
       "      <td>VALUE</td>\n",
       "      <td>the high prices you 're going to pay is for th...</td>\n",
       "      <td>complex_sentence</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id reviewID sentenceID                                             review  \\\n",
       "0   0     RL#3     RL#3:1  I am not necessarily fanatical about this plac...   \n",
       "1   2     TR#2     TR#2:2  The high prices you're going to pay is for the...   \n",
       "2   3     TR#2     TR#2:2  The high prices you're going to pay is for the...   \n",
       "3   4     TR#2     TR#2:2  The high prices you're going to pay is for the...   \n",
       "4   5     TR#2     TR#2:2  The high prices you're going to pay is for the...   \n",
       "\n",
       "       category  polarity entity  \\\n",
       "0  VALUE#PRICES  positive  VALUE   \n",
       "1  VALUE#PRICES  negative  VALUE   \n",
       "2  VALUE#PRICES  negative  VALUE   \n",
       "3  VALUE#PRICES  negative  VALUE   \n",
       "4  VALUE#PRICES  negative  VALUE   \n",
       "\n",
       "                               preprocessed_sentence      type_sentence  \n",
       "0  i am not necessarily fanatical about this plac...  compound_sentence  \n",
       "1  the high prices you 're going to pay is for th...   complex_sentence  \n",
       "2  the high prices you 're going to pay is for th...   complex_sentence  \n",
       "3  the high prices you 're going to pay is for th...   complex_sentence  \n",
       "4  the high prices you 're going to pay is for th...   complex_sentence  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('res_mul_all.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = []\n",
    "for sentence in df['review']:\n",
    "    tokens = prepare_text_for_lda(sentence)\n",
    "    text_data.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(text_data)\n",
    "corpus = [dictionary.doc2bow(text) for text in text_data]\n",
    "\n",
    "import pickle\n",
    "pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
    "dictionary.save('dictionary.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = 30, id2word=dictionary, random_state = 42)\n",
    "ldamodel.save('model5.gensim')\n",
    "topics = ldamodel.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, '0.026*\"bagel\" + 0.018*\"location\" + 0.018*\"place\" + 0.018*\"different\" + 0.018*\"especially\" + 0.018*\"service\" + 0.018*\"always\" + 0.009*\"means\" + 0.009*\"tomato\" + 0.009*\"simple\"')\n",
      "\n",
      "(5, '0.034*\"excellent\" + 0.034*\"great\" + 0.023*\"price\" + 0.012*\"shanty\" + 0.012*\"drink\" + 0.012*\"phenomenal\" + 0.012*\"glass\" + 0.012*\"staff\" + 0.012*\"music\" + 0.012*\"quality\"')\n",
      "\n",
      "(2, '0.037*\"place\" + 0.037*\"service\" + 0.037*\"price\" + 0.029*\"plentiful\" + 0.029*\"flavorful\" + 0.029*\"reasonably\" + 0.022*\"great\" + 0.015*\"offer\" + 0.015*\"would\" + 0.015*\"garden\"')\n",
      "\n",
      "(17, '0.023*\"delicious\" + 0.023*\"service\" + 0.016*\"setting\" + 0.016*\"special\" + 0.016*\"staff\" + 0.016*\"thing\" + 0.016*\"several\" + 0.016*\"excellent\" + 0.016*\"cheap\" + 0.016*\"serve\"')\n",
      "\n",
      "(12, '0.042*\"service\" + 0.017*\"notch\" + 0.017*\"waiter\" + 0.017*\"location\" + 0.017*\"great\" + 0.017*\"attentive\" + 0.009*\"better\" + 0.009*\"check\" + 0.009*\"letting\" + 0.009*\"husband\"')\n",
      "\n",
      "(15, '0.090*\"great\" + 0.049*\"place\" + 0.033*\"plentiful\" + 0.033*\"reasonably\" + 0.033*\"flavorful\" + 0.033*\"price\" + 0.025*\"service\" + 0.017*\"pasta\" + 0.017*\"authentic\" + 0.017*\"staff\"')\n",
      "\n",
      "(6, '0.028*\"friendly\" + 0.028*\"place\" + 0.028*\"pretty\" + 0.028*\"always\" + 0.019*\"taste\" + 0.019*\"portion\" + 0.019*\"decor\" + 0.019*\"people\" + 0.019*\"staff\" + 0.019*\"dish\"')\n",
      "\n",
      "(21, '0.069*\"service\" + 0.062*\"atmosphere\" + 0.038*\"relax\" + 0.038*\"casual\" + 0.024*\"great\" + 0.019*\"recommend\" + 0.014*\"sometimes\" + 0.014*\"experience\" + 0.014*\"friendly\" + 0.014*\"price\"')\n",
      "\n",
      "(8, '0.024*\"family\" + 0.016*\"tasty\" + 0.016*\"never\" + 0.016*\"small\" + 0.016*\"fresh\" + 0.016*\"going\" + 0.016*\"amaze\" + 0.016*\"price\" + 0.008*\"pistachio\" + 0.008*\"whole\"')\n",
      "\n",
      "(0, '0.038*\"place\" + 0.033*\"great\" + 0.030*\"price\" + 0.023*\"restaurant\" + 0.023*\"service\" + 0.015*\"first\" + 0.015*\"sushi\" + 0.008*\"judge\" + 0.008*\"suggest\" + 0.008*\"special\"')\n",
      "\n",
      "(26, '0.091*\"great\" + 0.033*\"price\" + 0.029*\"service\" + 0.021*\"reasonably\" + 0.021*\"flavorful\" + 0.021*\"plentiful\" + 0.021*\"place\" + 0.017*\"value\" + 0.017*\"dish\" + 0.014*\"average\"')\n",
      "\n",
      "(14, '0.035*\"staff\" + 0.032*\"pizza\" + 0.021*\"fantastic\" + 0.021*\"service\" + 0.019*\"always\" + 0.014*\"vinegar\" + 0.014*\"fresh\" + 0.014*\"going\" + 0.014*\"friendly\" + 0.014*\"pasta\"')\n",
      "\n",
      "(19, '0.039*\"service\" + 0.024*\"caviar\" + 0.016*\"delicious\" + 0.016*\"decor\" + 0.016*\"recommend\" + 0.016*\"excellent\" + 0.016*\"dumplings\" + 0.016*\"portion\" + 0.016*\"times\" + 0.016*\"better\"')\n",
      "\n",
      "(11, '0.022*\"roll\" + 0.022*\"sushi\" + 0.022*\"spicy\" + 0.022*\"choice\" + 0.022*\"service\" + 0.022*\"delicious\" + 0.022*\"price\" + 0.011*\"tempura\" + 0.011*\"evening\" + 0.011*\"creative\"')\n",
      "\n",
      "(9, '0.046*\"service\" + 0.031*\"great\" + 0.016*\"chicken\" + 0.016*\"wine\" + 0.016*\"selection\" + 0.016*\"pizza\" + 0.016*\"price\" + 0.016*\"order\" + 0.008*\"spicy\" + 0.008*\"excpetiona\"')\n",
      "\n",
      "(25, '0.071*\"price\" + 0.036*\"going\" + 0.021*\"pizza\" + 0.014*\"penne\" + 0.014*\"worst\" + 0.014*\"slice\" + 0.014*\"amaze\" + 0.014*\"pretty\" + 0.014*\"place\" + 0.014*\"little\"')\n",
      "\n",
      "(22, '0.029*\"place\" + 0.029*\"price\" + 0.029*\"great\" + 0.020*\"filet\" + 0.020*\"japanese\" + 0.020*\"moderate\" + 0.020*\"appetizer\" + 0.020*\"romantic\" + 0.020*\"absolutely\" + 0.020*\"appropriate\"')\n",
      "\n",
      "(3, '0.055*\"great\" + 0.037*\"delicious\" + 0.019*\"almost\" + 0.019*\"price\" + 0.019*\"tasting\" + 0.019*\"beautiful\" + 0.019*\"place\" + 0.019*\"service\" + 0.009*\"things\" + 0.009*\"thalia\"')\n",
      "\n",
      "(23, '0.020*\"sushi\" + 0.013*\"completely\" + 0.013*\"hollondaise\" + 0.013*\"roast\" + 0.013*\"definitely\" + 0.013*\"atmoshpere\" + 0.013*\"continually\" + 0.013*\"mortal\" + 0.013*\"worst\" + 0.013*\"waiter\"')\n",
      "\n",
      "(24, '0.045*\"relax\" + 0.045*\"atmosphere\" + 0.039*\"casual\" + 0.028*\"price\" + 0.023*\"delicious\" + 0.023*\"service\" + 0.017*\"lobster\" + 0.017*\"going\" + 0.014*\"pizza\" + 0.013*\"though\"')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for a in topics:\n",
    "    print(a)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_topics = []\n",
    "for topic in topics:\n",
    "    for pair in topic[1].split(' + '):\n",
    "        word = pair.split('*')[1].replace(\"\\\"\", '')\n",
    "        all_topics.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "       \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def aspect_topic(all_topics):\n",
    "    sf = pd.DataFrame(columns=['id','review','category','term'])\n",
    "    count = 0\n",
    "    index = 0\n",
    "    res = []\n",
    "    for sentence in df['review']:\n",
    "        lowercased = sentence.lower()\n",
    "        term = []\n",
    "        category = []\n",
    "        for cat in df['category'][index].split(','):\n",
    "            category.append(cat.split('#')[0])\n",
    "        id_name = df['id'][index]\n",
    "        for topic in all_topics:\n",
    "            if topic in lowercased:\n",
    "                term.append(topic)\n",
    "#         print(term)\n",
    "        if len(term) == 0:\n",
    "            print(sentence)\n",
    "            count += 1\n",
    "        sf = sf.append({'id': id_name, 'review': sentence.strip().lower().replace('  ', ' '), 'category': '|'.join(category), 'term': '|'.join(term)}, ignore_index=True)\n",
    "        index += 1\n",
    "    print(count)\n",
    "    sf.to_csv(\"lda.csv\")\n",
    "    sf.to_excel(\"lda.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Food is bad.\n",
      "Food is disgusting.\n",
      "The rice to fish ration was also good and they didn't try to overpack the rice.\n",
      "The food here is not good.\n",
      "I like the somosas, chai, and the chole, but the dhosas and dhal were kinda dissapointing.\n",
      "The lava cake dessert was terrible.\n",
      "My wife had the fried shrimp which are huge and loved it.\n",
      "The hostess is rude to the point of being offensive.\n",
      "The food was bland oily.\n",
      "I went there for lunch and the lunch was not as good as I expected from the reviews I read.\n",
      "Sauce was watery and the food didn't have much flavor.\n",
      "It may be a bit packed on weekends, but the vibe is good and it is the best French food you will find in the area.\n",
      "The food was bad.\n",
      "The tuna and wasabe potatoes are bad.\n",
      "It's simply the best meal in NYC.\n",
      "My chow fun and chow see was really bland and oily.\n",
      "Salads were bad.\n",
      "Ingredients are organic which is a real plus for me.\n",
      "The Yellowtail was particularly good as well.\n",
      "We even had a visit from the Manager who wanted to make sure we were enjoying ourselves.\n",
      "I'm still mad that i had to pay for lousy food.\n",
      "I like the ambience, it's very dark and original.\n",
      "Not one of our meals was edible - bland and/or made with weird rosemary or orange flavoring.\n",
      "Fish was overdone.\n",
      "My boyfriend had Prime Rib it was good .\n",
      "My wife and I ate here earlier this week and have not stopped ranting and raving about the food.\n",
      "The menu is very limited - i think we counted 4 or 5 entrees.\n",
      "Delivery is fast too.\n",
      "Not impressed with the food.\n",
      "Thius is a must for anyone who loves Shabu-Shabu.\n",
      "Taxan horrible!\n",
      "The quantity is also very good, you will come out satisfied.\n",
      "The food was bad.\n",
      "The food is bad, I can't lie.\n",
      "Most importantly, food is horrible.\n",
      "I am sad i did the food was horrible.\n",
      "and yes Dal Bukhara is so dam good and so are all the kababs.\n",
      "The seats are uncomfortable if you are sitting against the wall on wooden benches.\n",
      "Food is horrible.\n",
      "(The asparagus, truffle oil, parmesan bruschetta is a winner!)\n",
      "We did tip, I guess the model/waitress just wanted more and complained to the manager.\n",
      "If the weather is nice, try to snag an outside table.\n",
      "The food was good.\n",
      "Consequently, their burgers fell apart in their hands and made such a mess that they did'nt feel like finishing them.\n",
      "And the Tom Kha soup was pathetic.\n",
      "We went here for lunch a couple of weeks ago on a Saturday, and I was thoroughly impressed with the food.\n",
      "love the food.\n",
      "The food arrived 20 minutes after I called, cold and soggy.\n",
      "The menu has so many fish items and oysters.\n",
      "I expected quite a bit more from such an expensive menu.\n",
      "I really like both the scallops and the mahi mahi (on saffron risotto-yum!).\n",
      "Their calzones are horrific, bad, vomit-inducing, YUCK.\n",
      "Unique apppetizers.\n",
      "The cream cheeses are out of this world and I love that coffee!!\n",
      "the turkey burgers are scary!\n",
      "The brioche and lollies as party favors is a cute and sweet touch to a most memorable meal.\n",
      "I LOVE their Thai\n",
      "The food now is inconsistent.\n",
      "The food was exceptional.\n",
      "The lamb was tender so full of flavor, the dessert was divine!!\n",
      "Dessert is a joke...dont bother\n",
      "Too bad the food wasn't of the same heritage.\n",
      "Good food.\n",
      "Best Reuben sandwich ever!\n",
      "The best pad thai i've ever had.\n",
      "The food was actually aweful.\n",
      "Ask for Usha, the nicest bartender in manhattan.\n",
      "Delicate spices, onions, eggs and a kick-ass roti.\n",
      "Toons has recently been redone, so it's now a very attractive space.\n",
      "The food's as good as ever.\n",
      "In an area sadly lacking in decent Thai food, this is one of the best spots.\n",
      "Beef noodle soup is good as well.\n",
      "Best Taiwanese food in NY!\n",
      "Indoor was very cozy and cute.\n",
      "The only problem is that the manager is a complete incompetent.\n",
      "Food was good and the view of the new york city skiline was terrific even on a foggy rainy day like that of when I went.\n",
      "Personal pans are the perfect size for those hungry nights.\n",
      "Nice view of river and NYC.\n",
      "  Boucherie is our new favorite neighborhood spot.\n",
      "79\n"
     ]
    }
   ],
   "source": [
    "aspect_topic(list(dict.fromkeys(all_topics)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['atmosphere',\n",
       " 'casual',\n",
       " 'relax',\n",
       " 'absolutely',\n",
       " 'always',\n",
       " 'staff',\n",
       " 'price',\n",
       " 'appropriate',\n",
       " 'moderate',\n",
       " 'serve',\n",
       " 'flavorful',\n",
       " 'plentiful',\n",
       " 'reasonably',\n",
       " 'place',\n",
       " 'great',\n",
       " 'amaze',\n",
       " 'sushi',\n",
       " 'service',\n",
       " 'appetizer',\n",
       " 'would',\n",
       " 'pleasant',\n",
       " 'raise',\n",
       " 'french',\n",
       " 'attentive',\n",
       " 'taste',\n",
       " 'selection',\n",
       " 'salad',\n",
       " 'decor',\n",
       " 'restaurant',\n",
       " 'waiter',\n",
       " 'server',\n",
       " 'closing',\n",
       " 'special',\n",
       " 'consider',\n",
       " 'another',\n",
       " 'manager',\n",
       " 'italian',\n",
       " 'never',\n",
       " 'dish',\n",
       " 'outside',\n",
       " 'ambience',\n",
       " 'pizza',\n",
       " 'caviar',\n",
       " 'constitute',\n",
       " 'fresh',\n",
       " 'everything',\n",
       " 'quick',\n",
       " 'thing',\n",
       " 'terrible',\n",
       " 'different',\n",
       " 'despite',\n",
       " 'outstanding',\n",
       " 'really',\n",
       " 'worst',\n",
       " 'every',\n",
       " 'cheese',\n",
       " 'spicy',\n",
       " 'delicious',\n",
       " 'indian',\n",
       " 'could',\n",
       " 'ingredient',\n",
       " 'value',\n",
       " 'drink',\n",
       " 'quality',\n",
       " 'sweet',\n",
       " 'prompt',\n",
       " 'lasagna',\n",
       " 'friendly',\n",
       " 'worth',\n",
       " 'going',\n",
       " 'pasta',\n",
       " 'slice',\n",
       " 'portion',\n",
       " 'bagel',\n",
       " 'tasty',\n",
       " 'fancy',\n",
       " 'ask',\n",
       " 'beginning',\n",
       " 'shabu',\n",
       " 'vegetarian',\n",
       " 'pumkin',\n",
       " 'tortelini',\n",
       " 'following',\n",
       " 'particular',\n",
       " 'order',\n",
       " 'cramp',\n",
       " 'provide',\n",
       " 'sleek',\n",
       " 'take',\n",
       " 'bland',\n",
       " 'moreover',\n",
       " 'beautifully',\n",
       " 'meal',\n",
       " 'outrageously',\n",
       " 'saturday',\n",
       " 'highly',\n",
       " 'exotic',\n",
       " 'small',\n",
       " 'excellent',\n",
       " 'personal',\n",
       " 'garden',\n",
       " 'bistro',\n",
       " 'miraculously',\n",
       " 'review',\n",
       " 'cosette',\n",
       " 'though',\n",
       " 'topping',\n",
       " 'longer',\n",
       " 'panchetta',\n",
       " 'rival']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dict.fromkeys(all_topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
