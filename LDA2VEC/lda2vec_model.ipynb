{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import pickle\n",
    "import time\n",
    "import shelve\n",
    "\n",
    "import chainer\n",
    "from chainer import cuda\n",
    "from chainer import serializers\n",
    "import chainer.optimizers as O\n",
    "import numpy as np\n",
    "\n",
    "from lda2vec import utils\n",
    "from lda2vec import prepare_topics, print_top_words_per_topic, topic_coherence\n",
    "from lda2vec import LDA2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu_id = int(os.getenv('CUDA_GPU', 0))\n",
    "# cuda.get_device(gpu_id).use()\n",
    "# print(\"Using GPU:\" + str(gpu_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_dir = os.getenv('data_dir', '../data/')\n",
    "fn_vocab = 'vocab.pkl'\n",
    "fn_corpus = 'corpus.pkl'\n",
    "fn_flatnd = 'flattened.npy'\n",
    "fn_docids = 'doc_ids.npy'\n",
    "fn_vectors = 'vectors.npy'\n",
    "vocab = pickle.load(open(fn_vocab, 'rb'))\n",
    "corpus = pickle.load(open(fn_corpus, 'rb'))\n",
    "flattened = np.load(fn_flatnd)\n",
    "doc_ids = np.load(fn_docids)\n",
    "vectors = np.load(fn_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Parameters\n",
    "# Number of documents\n",
    "n_docs = doc_ids.max() + 1\n",
    "# Number of unique words in the vocabulary\n",
    "n_vocab = flattened.max() + 1\n",
    "# 'Strength' of the dircihlet prior; 200.0 seems to work well\n",
    "clambda = 200.0\n",
    "# Number of topics to fit\n",
    "n_topics = int(os.getenv('n_topics', 30))\n",
    "batchsize = 4096\n",
    "# Power for neg sampling\n",
    "power = float(os.getenv('power', 0.75))\n",
    "# Intialize with pretrained word vectors\n",
    "pretrained = bool(int(os.getenv('pretrained', True)))\n",
    "# Sampling temperature\n",
    "temperature = float(os.getenv('temperature', 1.0))\n",
    "# Number of dimensions in a single word vector\n",
    "n_units = int(os.getenv('n_units', 300))\n",
    "# Get the string representation for every compact key\n",
    "words = corpus.word_list(vocab)[:n_vocab]\n",
    "# How many tokens are in each document\n",
    "doc_idx, lengths = np.unique(doc_ids, return_counts=True)\n",
    "doc_lengths = np.zeros(doc_ids.max() + 1, dtype='int32')\n",
    "doc_lengths[doc_idx] = lengths\n",
    "# Count all token frequencies\n",
    "tok_idx, freq = np.unique(flattened, return_counts=True)\n",
    "term_frequency = np.zeros(n_vocab, dtype='int32')\n",
    "term_frequency[tok_idx] = freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key in sorted(locals().keys()):\n",
    "#     val = locals()[key]\n",
    "#     if len(str(val)) < 100 and '<' not in str(val):\n",
    "#         print(key, val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49, 300)\n",
      "[[0.37454012 0.95071431 0.73199394 ... 0.21582103 0.62289048 0.08534746]\n",
      " [0.05168172 0.53135463 0.54063512 ... 0.17231987 0.19228902 0.04086862]\n",
      " [0.16893506 0.27859034 0.17701048 ... 0.89633582 0.01300192 0.08550853]\n",
      " ...\n",
      " [0.89466762 0.0116266  0.96590299 ... 0.97460921 0.68886648 0.89503275]\n",
      " [0.69937887 0.73511582 0.29041092 ... 0.03161501 0.77339222 0.0520524 ]\n",
      " [0.03815361 0.4298177  0.06307517 ... 0.38296701 0.20364864 0.64399795]]\n"
     ]
    }
   ],
   "source": [
    "model = LDA2Vec(n_documents=n_docs, n_document_topics=n_topics,\n",
    "                n_units=n_units, n_vocab=n_vocab, counts=term_frequency,\n",
    "                n_samples=20, power=power, temperature=temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if os.path.exists('lda2vec.hdf5'):\n",
    "#     print(\"Reloading from saved\")\n",
    "#     serializers.load_hdf5(\"lda2vec.hdf5\", model)\n",
    "    \n",
    "if pretrained:\n",
    "    model.sampler.W.data[:, :] = vectors[:n_vocab, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.to_gpu()\n",
    "optimizer = O.Adam()\n",
    "optimizer.setup(model)\n",
    "clip = chainer.optimizer.GradientClipping(5.0)\n",
    "optimizer.add_hook(clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "epoch = 0\n",
    "fraction = batchsize * 1.0 / flattened.shape[0]\n",
    "progress = shelve.open('progress.shelve')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words in topic 0 wine <SKIP> recommend appropriate value casual price reasonably fish ambience\n",
      "Top words in topic 1 dish moderate friendly delicious price good menu flavorful fresh recommend\n",
      "Top words in topic 2 out_of_vocabulary place atmosphere nice fish never would special ever like\n",
      "Top words in topic 3 out_of_vocabulary atmosphere staff relaxed friendly high excellent moderate view reasonably\n",
      "Top words in topic 4 recommend place appropriate <SKIP> view menu excellent relaxed best ambience\n",
      "Top words in topic 5 value out_of_vocabulary price amazing sushi fish nice moderate food pizza\n",
      "Top words in topic 6 recommend menu pizza flavorful special friendly appropriate moderate sushi wine\n",
      "Top words in topic 7 sushi delicious amazing flavorful fresh menu never really absolutely excellent\n",
      "Top words in topic 8 friendly relaxed plentiful atmosphere place reasonably casual view ambience moderate\n",
      "Top words in topic 9 moderate friendly restaurant pizza place meal ever best out_of_vocabulary great\n",
      "Top words in topic 10 sushi menu pizza wine ambience meal value decor delicious food\n",
      "Top words in topic 11 decor amazing restaurant value pizza meal dish fish staff menu\n",
      "Top words in topic 12 atmosphere fish ambience absolutely relaxed meal always really staff decor\n",
      "Top words in topic 13 recommend menu flavorful ever sushi view dish delicious decor appropriate\n",
      "Top words in topic 14 value absolutely staff recommend fresh dish best good time price\n",
      "Top words in topic 15 value menu price staff appropriate <SKIP> decor high reasonably out_of_vocabulary\n",
      "Top words in topic 16 price time meal special value food high would service recommend\n",
      "Top words in topic 17 high excellent ambience pizza nice dish delicious great time meal\n",
      "Top words in topic 18 fresh moderate high never best friendly relaxed staff out_of_vocabulary wine\n",
      "Top words in topic 19 out_of_vocabulary reasonably price view wine would value flavorful really fish\n",
      "Top words in topic 20 wine view absolutely restaurant special reasonably staff moderate excellent out_of_vocabulary\n",
      "Top words in topic 21 casual plentiful food atmosphere service <SKIP> ambience meal special relaxed\n",
      "Top words in topic 22 ambience view nice casual atmosphere decor plentiful high fresh good\n",
      "Top words in topic 23 out_of_vocabulary menu moderate fresh delicious price meal out_of_vocabulary fish pizza\n",
      "Top words in topic 24 restaurant menu pizza dish service sushi fish high decor moderate\n",
      "Top words in topic 25 flavorful <SKIP> delicious wine sushi amazing absolutely ambience plentiful menu\n",
      "Top words in topic 26 <SKIP> wine fresh out_of_vocabulary going good view atmosphere staff ever\n",
      "Top words in topic 27 nice best special good friendly excellent relaxed flavorful great really\n",
      "Top words in topic 28 restaurant decor flavorful ambience view amazing dish excellent friendly out_of_vocabulary\n",
      "Top words in topic 29 good excellent fish amazing recommend nice ever value best moderate\n",
      "0\n",
      "after partial fitting: 583.9747\n",
      "J:00000 E:00000 L:5.840e+02 P:-6.896e+04 R:4.091e+03\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1):\n",
    "    data = prepare_topics(cuda.to_cpu(model.mixture.weights.W.data).copy(),\n",
    "                          cuda.to_cpu(model.mixture.factors.W.data).copy(),\n",
    "                          cuda.to_cpu(model.sampler.W.data).copy(),\n",
    "                          words)\n",
    "    top_words = print_top_words_per_topic(data)\n",
    "    if j % 100 == 0 and j > 100:\n",
    "        coherence = topic_coherence(top_words)\n",
    "        for j in range(n_topics):\n",
    "            print(j, coherence[(j, 'cv')])\n",
    "        kw = dict(top_words=top_words, coherence=coherence, epoch=epoch)\n",
    "        progress[str(epoch)] = pickle.dumps(kw)\n",
    "    data['doc_lengths'] = doc_lengths\n",
    "    data['term_frequency'] = term_frequency\n",
    "    np.savez('topics.pyldavis', **data)\n",
    "    print(epoch)\n",
    "    for d, f in utils.chunks(batchsize, doc_ids, flattened):\n",
    "        t0 = time.time()\n",
    "        model.cleargrads()\n",
    "        #optimizer.use_cleargrads(use=False)\n",
    "        l = model.fit_partial(d.copy(), f.copy())\n",
    "        print(\"after partial fitting:\", l)\n",
    "        prior = model.prior()\n",
    "        loss = prior * fraction\n",
    "        loss.backward()\n",
    "        optimizer.update()\n",
    "        msg = (\"J:{j:05d} E:{epoch:05d} L:{loss:1.3e} \"\n",
    "               \"P:{prior:1.3e} R:{rate:1.3e}\")\n",
    "        prior.to_cpu()\n",
    "        loss.to_cpu()\n",
    "        t1 = time.time()\n",
    "        dt = t1 - t0\n",
    "        rate = batchsize / dt\n",
    "        logs = dict(loss=float(l), epoch=epoch, j=j,\n",
    "                    prior=float(prior.data), rate=rate)\n",
    "        print(msg.format(**logs))\n",
    "        j += 1\n",
    "    serializers.save_hdf5(\"lda2vec.hdf5\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<SKIP>', 'absolutely', 'always', 'amazing', 'ambience', 'appropriate', 'atmosphere', 'best', 'casual', 'decor', 'delicious', 'dish', 'ever', 'excellent', 'fish', 'flavorful', 'food', 'fresh', 'friendly', 'going', 'good', 'great', 'high', 'like', 'meal', 'menu', 'moderate', 'never', 'nice', 'out_of_vocabulary', 'pizza', 'place', 'plentiful', 'price', 'really', 'reasonably', 'recommend', 'relaxed', 'restaurant', 'service', 'special', 'staff', 'sushi', 'time', 'value', 'view', 'wine', 'would']\n",
      "48\n"
     ]
    }
   ],
   "source": [
    "all_topics = []\n",
    "for row in top_words:\n",
    "    for word in row:\n",
    "        all_topics.append(word)\n",
    "print(sorted(list(dict.fromkeys(all_topics))))\n",
    "print(len(sorted(list(dict.fromkeys(all_topics)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>reviewID</th>\n",
       "      <th>sentenceID</th>\n",
       "      <th>review</th>\n",
       "      <th>category</th>\n",
       "      <th>polarity</th>\n",
       "      <th>entity</th>\n",
       "      <th>preprocessed_sentence</th>\n",
       "      <th>type_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>RL#3</td>\n",
       "      <td>RL#3:1</td>\n",
       "      <td>I am not necessarily fanatical about this plac...</td>\n",
       "      <td>VALUE#PRICES</td>\n",
       "      <td>positive</td>\n",
       "      <td>VALUE</td>\n",
       "      <td>i am not necessarily fanatical about this plac...</td>\n",
       "      <td>compound_sentence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>TR#2</td>\n",
       "      <td>TR#2:2</td>\n",
       "      <td>The high prices you're going to pay is for the...</td>\n",
       "      <td>VALUE#PRICES</td>\n",
       "      <td>negative</td>\n",
       "      <td>VALUE</td>\n",
       "      <td>the high prices you 're going to pay is for th...</td>\n",
       "      <td>complex_sentence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>TR#2</td>\n",
       "      <td>TR#2:2</td>\n",
       "      <td>The high prices you're going to pay is for the...</td>\n",
       "      <td>VALUE#PRICES</td>\n",
       "      <td>negative</td>\n",
       "      <td>VALUE</td>\n",
       "      <td>the high prices you 're going to pay is for th...</td>\n",
       "      <td>complex_sentence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>TR#2</td>\n",
       "      <td>TR#2:2</td>\n",
       "      <td>The high prices you're going to pay is for the...</td>\n",
       "      <td>VALUE#PRICES</td>\n",
       "      <td>negative</td>\n",
       "      <td>VALUE</td>\n",
       "      <td>the high prices you 're going to pay is for th...</td>\n",
       "      <td>complex_sentence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>TR#2</td>\n",
       "      <td>TR#2:2</td>\n",
       "      <td>The high prices you're going to pay is for the...</td>\n",
       "      <td>VALUE#PRICES</td>\n",
       "      <td>negative</td>\n",
       "      <td>VALUE</td>\n",
       "      <td>the high prices you 're going to pay is for th...</td>\n",
       "      <td>complex_sentence</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id reviewID sentenceID                                             review  \\\n",
       "0   0     RL#3     RL#3:1  I am not necessarily fanatical about this plac...   \n",
       "1   2     TR#2     TR#2:2  The high prices you're going to pay is for the...   \n",
       "2   3     TR#2     TR#2:2  The high prices you're going to pay is for the...   \n",
       "3   4     TR#2     TR#2:2  The high prices you're going to pay is for the...   \n",
       "4   5     TR#2     TR#2:2  The high prices you're going to pay is for the...   \n",
       "\n",
       "       category  polarity entity  \\\n",
       "0  VALUE#PRICES  positive  VALUE   \n",
       "1  VALUE#PRICES  negative  VALUE   \n",
       "2  VALUE#PRICES  negative  VALUE   \n",
       "3  VALUE#PRICES  negative  VALUE   \n",
       "4  VALUE#PRICES  negative  VALUE   \n",
       "\n",
       "                               preprocessed_sentence      type_sentence  \n",
       "0  i am not necessarily fanatical about this plac...  compound_sentence  \n",
       "1  the high prices you 're going to pay is for th...   complex_sentence  \n",
       "2  the high prices you 're going to pay is for th...   complex_sentence  \n",
       "3  the high prices you 're going to pay is for th...   complex_sentence  \n",
       "4  the high prices you 're going to pay is for th...   complex_sentence  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../res_mul_all.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def aspect_topic(tipe, all_topics):\n",
    "    sf = pd.DataFrame(columns=['id','review','category','term'])\n",
    "    count = 0\n",
    "    index = 0\n",
    "    res = []\n",
    "    for sentence in df['review']:\n",
    "        lowercased = sentence.lower()\n",
    "        term = []\n",
    "        category = []\n",
    "        for cat in df['category'][index].split(','):\n",
    "            splitted = cat.split('#')\n",
    "            if splitted[1] == 'PRICES':\n",
    "                category.append('VALUE')\n",
    "            else:\n",
    "                category.append(splitted[0])\n",
    "        id_name = df['id'][index]\n",
    "        for topic in all_topics:\n",
    "            tokens = lowercased.split(' ')\n",
    "            for token in tokens:\n",
    "                if token.startswith(topic):\n",
    "                    term.append(topic)\n",
    "#         print(term)\n",
    "        if len(term) == 0:\n",
    "            print(lowercased)\n",
    "            count += 1\n",
    "        sf = sf.append({'id': id_name, 'review': sentence.strip().lower().replace('  ', ' '), 'category': '|'.join(category), 'term': '|'.join(term)}, ignore_index=True)\n",
    "        index += 1\n",
    "    print(count)\n",
    "    sf.to_csv(\"lda2vec\"+ tipe +\".csv\")\n",
    "    sf.to_excel(\"lda2vec\"+ tipe +\".xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chow fun was dry; pork shu mai was more than usually greasy and had to share a table with loud and rude family. \n",
      "the lava cake dessert was terrible.\n",
      "once you step into cosette, you're miraculously in a small, off-the-beaten path parisian bistro.\n",
      "my wife had the fried shrimp which are huge and loved it.\n",
      "the hostess is rude to the point of being offensive.\n",
      "there was a small wait, but shorter than i expected.\n",
      "first went here to enjoy their garden terrace.\n",
      "took my mom for mother's day, and the maitre d' was pretty rude.\n",
      "tiny dessert was $8.00...just plain overpriced for what it is.\n",
      "the tuna and wasabe potatoes are bad.\n",
      "the bagel was small.\n",
      "salads were bad.\n",
      "ingredients are organic which is a real plus for me.\n",
      "we even had a visit from the manager who wanted to make sure we were enjoying ourselves.\n",
      "their tuna tartar appetizer is to die for.\n",
      "the dining room is quietly elegant with no music to shout over -- how refreshing!\n",
      "delivery is fast too.\n",
      "thius is a must for anyone who loves shabu-shabu.\n",
      "taxan horrible!\n",
      "whether it's the parmesean porcini souffle or the lamb glazed with balsamic vinegar, you will surely be transported to northern italy with one bite.\n",
      "i had their eggs benedict for brunch, which were the worst in my entire life, i tried removing the hollondaise sauce completely that was how failed it was.\n",
      "the seats are uncomfortable if you are sitting against the wall on wooden benches.\n",
      "truly the mark of an attentive waiter.\n",
      "(the asparagus, truffle oil, parmesan bruschetta is a winner!)\n",
      "after dinner the manager grabbed my boyfriend, asked him: where are you from...maybe you dont know how things work in america...and in the end stormed away almost teareyed yelling that tips are the only thing they survive on.\n",
      "we did tip, i guess the model/waitress just wanted more and complained to the manager.\n",
      "we ate out in the back patio, which is worth it as it's cool and the music is hear well there.\n",
      "and the tom kha soup was pathetic.\n",
      "the back garden sitting area is very pleasant, where you can see their personal herb garden.\n",
      "we had the lobster sandwich and it was fantastic.\n",
      "mizu is home to creative and unique rolls not to found anywhere else.\n",
      "i ordered the smoked salmon and roe appetizer and it was off flavor.\n",
      "the entree was bland and small, dessert was not inspired.\n",
      "their calzones are horrific, bad, vomit-inducing, yuck.\n",
      "the dosas are skimpy, unattractive and drip with grease, and personally i'd drink popcorn topping before i'd eat another one of these.\n",
      "the sandwiches are dry, tasteless and way overpriced.\n",
      "unique apppetizers.\n",
      "the cream cheeses are out of this world and i love that coffee!!\n",
      "the turkey burgers are scary!\n",
      "the rice was poor quality and was cooked so badly it was hard.\n",
      "the location is perfect.\n",
      "i love their thai\n",
      "if you're daring, try the balsamic vinegar over icecream, it's wonderful!\n",
      "terrible, terrible management - deserves to be shut-down.\n",
      "the lamb was tender so full of flavor, the dessert was divine!!\n",
      "the waiter was attentive.\n",
      "dessert is a joke...dont bother\n",
      "cozy romantic atomosphere with only around 15 tables at most.\n",
      "i can't wait for summer, when they serve outside on their gigantic patio.\n",
      "delicate spices, onions, eggs and a kick-ass roti.\n",
      "toons has recently been redone, so it's now a very attractive space.\n",
      "we recently decided to try this location, and to our delight, they have outdoor seating, perfect since i had my yorkie with me.\n",
      "indoor was very cozy and cute.\n",
      "i fell in love with the egg noodles in the beef broth with shrimp dumplings and slices of bbq roast pork.\n",
      "yakitori (bbq meats) is tasty too.\n",
      "the only problem is that the manager is a complete incompetent.\n",
      "personal pans are the perfect size for those hungry nights.\n",
      "there is a downside if you're ordering in -- the delivery guys have major attitude.\n",
      " perfect location for those traveling in/out of the city by auto or bus\n",
      " the 8th ave location was very convenient and while busy, wasn't packed\n",
      "the location in the heart of manhattan adjacent to the port authority makes this an easy spot to grab a bite to eat\n",
      "location is convienient to businesses, hotels and theaters\n",
      "  boucherie is our new favorite neighborhood spot.\n",
      "63\n"
     ]
    }
   ],
   "source": [
    "aspect_topic(\"-nice\",list(dict.fromkeys(all_topics)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
