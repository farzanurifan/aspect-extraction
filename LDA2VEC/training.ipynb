{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import pickle\n",
    "import time\n",
    "import shelve\n",
    "\n",
    "import chainer\n",
    "from chainer import cuda\n",
    "from chainer import serializers\n",
    "import chainer.optimizers as O\n",
    "import numpy as np\n",
    "\n",
    "from lda2vec_old import utils\n",
    "from lda2vec_old import prepare_topics, print_top_words_per_topic, topic_coherence\n",
    "from lda2vec_model import LDA2Vec\n",
    "\n",
    "gpu_id = int(os.getenv('CUDA_GPU', 0))\n",
    "cuda.get_device(gpu_id).use()\n",
    "print \"Using GPU \" + str(gpu_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading pretrained topic vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "factors = np.load(\"final/factors.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper parameters and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_docs = doc_ids.max() + 1\n",
    "# Number of unique words in the vocabulary\n",
    "n_vocab = flattened.max() + 1\n",
    "# 'Strength' of the dircihlet prior; 200.0 seems to work well\n",
    "clambda = 200.0\n",
    "# Number of topics to fit\n",
    "n_topics = int(os.getenv('n_topics', 20))\n",
    "batchsize = 4096\n",
    "# Power for neg sampling\n",
    "power = float(os.getenv('power', 0.75))\n",
    "# Intialize with pretrained word vectors\n",
    "pretrained = bool(int(os.getenv('pretrained', True)))\n",
    "# Sampling temperature\n",
    "temperature = float(os.getenv('temperature', 1.0))\n",
    "# Number of dimensions in a single word vector\n",
    "n_units = int(os.getenv('n_units', 300))\n",
    "# Get the string representation for every compact key\n",
    "words = corpus.word_list(vocab)[:n_vocab]\n",
    "# How many tokens are in each document\n",
    "doc_idx, lengths = np.unique(doc_ids, return_counts=True)\n",
    "doc_lengths = np.zeros(doc_ids.max() + 1, dtype='int32')\n",
    "doc_lengths[doc_idx] = lengths\n",
    "# Count all token frequencies\n",
    "tok_idx, freq = np.unique(flattened, return_counts=True)\n",
    "term_frequency = np.zeros(n_vocab, dtype='int32')\n",
    "term_frequency[tok_idx] = freq\n",
    "\n",
    "for key in sorted(locals().keys()):\n",
    "    val = locals()[key]\n",
    "    if len(str(val)) < 100 and '<' not in str(val):\n",
    "        print key, val\n",
    "\n",
    "model = LDA2Vec(n_documents=n_docs, n_document_topics=n_topics,\n",
    "                n_units=n_units, n_vocab=n_vocab, counts=term_frequency,\n",
    "                n_samples=15, power=power, temperature=temperature)\n",
    "# if os.path.exists('lda2vec.hdf5'):\n",
    "#     print \"Reloading from saved\"\n",
    "#     serializers.load_hdf5(\"lda2vec.hdf5\", model)\n",
    "if pretrained:\n",
    "    model.sampler.W.data[:, :] = vectors[:n_vocab, :]\n",
    "model.mixture.factors.W.data = factors.astype(np.float32)\n",
    "model.to_gpu()\n",
    "optimizer = O.Adam()\n",
    "optimizer.setup(model)\n",
    "clip = chainer.optimizer.GradientClipping(5.0)\n",
    "optimizer.add_hook(clip)\n",
    "\n",
    "j = 0\n",
    "epoch = 0\n",
    "fraction = batchsize * 1.0 / flattened.shape[0]\n",
    "loss_ep=[]\n",
    "#progress = shelve.open('progress.shelve')\n",
    "for epoch in range(200):\n",
    "    data = prepare_topics(cuda.to_cpu(model.mixture.weights.W.data).copy(),\n",
    "                          cuda.to_cpu(model.mixture.factors.W.data).copy(),\n",
    "                          cuda.to_cpu(model.sampler.W.data).copy(),\n",
    "                          words)\n",
    "    top_words = print_top_words_per_topic(data)\n",
    "    if j % 100 == 0 and j > 100:\n",
    "        coherence = topic_coherence(top_words)\n",
    "        for j in range(n_topics):\n",
    "            print j, coherence[(j, 'cv')]\n",
    "        #kw = dict(top_words=top_words, coherence=coherence, epoch=epoch)\n",
    "        #progress[str(epoch)] = pickle.dumps(kw)\n",
    "    data['doc_lengths'] = doc_lengths\n",
    "    data['term_frequency'] = term_frequency\n",
    "    #np.savez('topics.pyldavis', **data)\n",
    "    for d, f in utils.chunks(batchsize, doc_ids, flattened):\n",
    "        t0 = time.time()\n",
    "        optimizer.zero_grads()\n",
    "        l = model.fit_partial(d.copy(), f.copy(),update_only_docs=True)\n",
    "        prior = model.prior()\n",
    "        loss = prior * fraction\n",
    "        loss.backward()\n",
    "        optimizer.update()\n",
    "        msg = (\"J:{j:05d} E:{epoch:05d} L:{loss:1.3e} \"\n",
    "               \"P:{prior:1.3e} R:{rate:1.3e}\")\n",
    "        prior.to_cpu()\n",
    "        loss.to_cpu()\n",
    "        t1 = time.time()\n",
    "        dt = t1 - t0\n",
    "        rate = batchsize / dt\n",
    "        j += 1\n",
    "    logs = dict(loss=float(l), epoch=epoch, j=j,\n",
    "                    prior=float(prior.data), rate=rate)\n",
    "    print msg.format(**logs)\n",
    "    print \"\\n =================== \\n\"\n",
    "    loss_ep.append(float(l))\n",
    "    #serializers.save_hdf5(\"final/lda2vec_final.hdf5\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is the topic distribution of the test document with out of vocabulary words :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Container object of 20 artists>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEACAYAAABI5zaHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nuEtSgwx3SWqQ4S5JDTLcJalB/w+c/xKbFP6AhwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f643a561650>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(np.arange(20), data['doc_topic_dists'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:dsci6007]",
   "language": "python",
   "name": "conda-env-dsci6007-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
