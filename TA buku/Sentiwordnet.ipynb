{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "from pywsd.lesk import adapted_lesk\n",
    "from pywsd.lesk import simple_lesk\n",
    "from pywsd.lesk import cosine_lesk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import requests\n",
    "import sys, os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "dependency_parser = nlp.annotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breakdown = swn.senti_synset('high.n.01')\n",
    "print(breakdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.synsets('love')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_lexicon = []\n",
    "negative_lexicon = []\n",
    "\n",
    "def read_lexicon():\n",
    "    global positive_lexicon;\n",
    "    global negative_lexicon;\n",
    "    \n",
    "    with open(os.path.join(os.path.abspath('../opinion-lexicon-English/') , 'positive-words.txt'), 'r') as file:\n",
    "        line = file.readline();\n",
    "        while \";\" in line:\n",
    "            line = file.readline();\n",
    "         \n",
    "        positive_lexicon = file.readlines()\n",
    "    \n",
    "    with open(os.path.join(os.path.abspath('../opinion-lexicon-English/') , 'negative-words.txt'), 'r', encoding = \"ISO-8859-1\") as file:\n",
    "        line = file.readline();\n",
    "        while \";\" in line:\n",
    "            line = file.readline();\n",
    "        \n",
    "        negative_lexicon = file.readlines()\n",
    "        \n",
    "    positive_lexicon = list(map(lambda word: word.rstrip(\"\\n\\r\"), positive_lexicon))\n",
    "    negative_lexicon = list(map(lambda word: word.rstrip(\"\\n\\r\"), negative_lexicon))\n",
    "    \n",
    "        \n",
    "read_lexicon()\n",
    "op_set = positive_lexicon + negative_lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tag(sentence):\n",
    "    result = dependency_parser(sentence, properties={\"outputFormat\": \"json\", \"annotators\": \"pos\"})['sentences'][0]['tokens']\n",
    "    res = []\n",
    "    for pos in result:\n",
    "        res.append((pos['word'], pos['pos']))\n",
    "    return res\n",
    "    \n",
    "# def pos_tag(sentence):\n",
    "#     '''url = \"http://localhost:9000\"\n",
    "#     request_params = {\"annotators\": \"pos\"}\n",
    "#     r = requests.post(url, data=sentence, params=request_params, timeout=120)\n",
    "#     try:\n",
    "#         results = r.json()['sentences'][0]['tokens']\n",
    "#         res = []\n",
    "#         for pos in results:\n",
    "#             res.append((pos['word'], pos['pos']))\n",
    "#         return res\n",
    "#     except Exception as e:\n",
    "#         print(e)\n",
    "#         return []\n",
    "#     '''\n",
    "#     return nltk.pos_tag(nltk.word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negation = [\n",
    "    \"afraid\",\n",
    "    \"can't\",\n",
    "    \"cannot\",\n",
    "    \"deny\",\n",
    "    \"mean\",\n",
    "    \"negate\",\n",
    "    \"negation\",\n",
    "    \"negative\",\n",
    "    \"neither\",\n",
    "    \"never\",\n",
    "    \"no\",\n",
    "    \"non\",\n",
    "    \"none\",\n",
    "    \"nor\",\n",
    "    \"not\",\n",
    "    \"nothing\",\n",
    "    \"refusal\",\n",
    "    \"refuse\",\n",
    "    \"reject\",\n",
    "    \"rejection\"\n",
    "]\n",
    "\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "import string\n",
    "\n",
    "def predict_lexicon(opinions, sent):\n",
    "    endscore = 0\n",
    "    for opinion in opinions:\n",
    "        if opinion in positive_lexicon:\n",
    "            endscore += 1\n",
    "        elif opinion in negative_lexicon:\n",
    "            endscore -= 1\n",
    "\n",
    "        words = sent.split(' ')\n",
    "        word_around = []\n",
    "        for x in range(0, len(words)):\n",
    "            if words[x] in string.punctuation:\n",
    "                continue\n",
    "            try:\n",
    "                if (words[x+1] == opinion) or (words[x+2] == opinion) or (words[x+3] == opinion) or (words[x+4] == opinion):\n",
    "                    word_around.append(words[x])\n",
    "                elif (words[x-1] == opinion):\n",
    "                    word_around.append(words[x])\n",
    "            except:\n",
    "                pass\n",
    "        for neg in negation:\n",
    "            if neg in word_around:\n",
    "                endscore *= (-1)\n",
    "                break\n",
    "\n",
    "    if endscore > 0:\n",
    "        polarity = 'positive'\n",
    "    else:\n",
    "        polarity = 'negative'\n",
    "           \n",
    "    return polarity\n",
    "\n",
    "def predict_sentiwordnet(opinions, sent):\n",
    "    endscore = 0\n",
    "    for opinion in opinions:\n",
    "#         if opinion in positive_lexicon:\n",
    "#             endscore += 1\n",
    "#         elif opinion in negative_lexicon:\n",
    "#             endscore -= 1\n",
    "#         break\n",
    "        try:\n",
    "            score = swn.senti_synset(opinion + '.a.1')\n",
    "            if score.pos_score() > score.neg_score():\n",
    "                endscore += score.pos_score()\n",
    "            else:\n",
    "                endscore += score.neg_score() * (-1)\n",
    "\n",
    "            words = sent.split(' ')\n",
    "            word_around = []\n",
    "            for x in range(0, len(words)):\n",
    "                if words[x] in string.punctuation:\n",
    "                    continue\n",
    "                try:\n",
    "                    if (words[x+1] == opinion) or (words[x+2] == opinion) or (words[x+3] == opinion) or (words[x+4] == opinion):\n",
    "                        word_around.append(words[x])\n",
    "                    elif (words[x-1] == opinion):\n",
    "                        word_around.append(words[x])\n",
    "                except:\n",
    "                    pass\n",
    "            for neg in negation:\n",
    "                if neg in word_around:\n",
    "                    endscore *= (-1)\n",
    "                    break\n",
    "        except:\n",
    "            if opinion in op_set:\n",
    "                print(opinion)\n",
    "            pass\n",
    "    if endscore > 0:\n",
    "        polarity = 'positive'\n",
    "    else:\n",
    "        polarity = 'negative'\n",
    "           \n",
    "    return polarity, endscore\n",
    "\n",
    "def predict_sentiwordnet_lesk(opinions, sent):\n",
    "    endscore = 0\n",
    "    for opinion in opinions:\n",
    "#         if opinion in positive_lexicon:\n",
    "#             endscore += 1\n",
    "#         elif opinion in negative_lexicon:\n",
    "#             endscore -= 1\n",
    "#         break\n",
    "        try:\n",
    "            tagged_sentence = pos_tag(sent)\n",
    "            op_with_tag = ('','')\n",
    "            for word, tag in tagged_sentence:\n",
    "                if opinion == word:\n",
    "                    op_with_tag = (word, tag)\n",
    "                    break\n",
    "                    \n",
    "            pos = ''\n",
    "            if 'NN' in op_with_tag[1]:\n",
    "                pos = 'n'\n",
    "            elif 'JJ' in op_with_tag[1]:\n",
    "                pos = 'a'\n",
    "            elif 'VB' in op_with_tag[1]:\n",
    "                pos = 'v'\n",
    "            elif 'RB' in op_with_tag[1]:\n",
    "                pos = 'r'\n",
    "            else:\n",
    "                pos = 'a'\n",
    "            \n",
    "            score = swn.senti_synset(simple_lesk(sent, opinion, pos).name())\n",
    "            if score.pos_score() > score.neg_score():\n",
    "                endscore += score.pos_score()\n",
    "            else:\n",
    "                endscore += score.neg_score() * (-1)\n",
    "                \n",
    "            words = sent.split(' ')\n",
    "            word_around = []\n",
    "            for x in range(0, len(words)):\n",
    "                if words[x] in string.punctuation:\n",
    "                    continue\n",
    "                try:\n",
    "                    if (words[x+1] == opinion) or (words[x+2] == opinion) or (words[x+3] == opinion) or (words[x+4] == opinion):\n",
    "                        word_around.append(words[x])\n",
    "                    elif (words[x-1] == opinion):\n",
    "                        word_around.append(words[x])\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            for neg in negation:\n",
    "                if neg in word_around:\n",
    "                    endscore *= (-1)\n",
    "                    break\n",
    "           \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass\n",
    "            \n",
    "    if endscore > 0:\n",
    "        polarity = 'positive'\n",
    "    else:\n",
    "        polarity = 'negative'\n",
    "           \n",
    "    return polarity, endscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate(category, tipe):\n",
    "    df = pd.read_csv('Results/Sentiwordnet/hasil_'+ tipe + category +'.csv')\n",
    "    tp, tn, fp, fn = 0, 0, 0, 0\n",
    "    p = 'positive'\n",
    "    n = 'negative'\n",
    "    for index in range(0, len(df)):\n",
    "        label = df['label'][index]\n",
    "        predict = df['predict'][index]\n",
    "        \n",
    "        if label == p and predict == p:\n",
    "            tp += 1\n",
    "        elif label == n and predict == n:\n",
    "            tn += 1\n",
    "        elif label == n and predict == p:\n",
    "            fp += 1\n",
    "        elif label == p and predict == n:\n",
    "            fn += 1\n",
    "    \n",
    "    precision = round(tp / (tp + fp), 2)\n",
    "    recall = round(tp / (tp + fn), 2)\n",
    "    f1 = round(2 * ((precision * recall) / (precision + recall)), 2)\n",
    "    accuracy = round((tp+tn)/(tp+tn+fp+fn), 2)\n",
    "    print(tp, '\\t', tn, '\\t', fp, '\\t', fn, '\\t', '\\t', precision, '\\t', recall, '\\t', f1, '\\t', '\\t', accuracy)\n",
    "    return tp, tn, fp, fn, precision, recall, f1, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(sentence):\n",
    "    return sentence.lower()\n",
    "\n",
    "def calculate_f1(p, r):\n",
    "    return round(2*((p*r)/(p+r)),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def print_hasil(tipe):\n",
    "    print('tp', '\\t', 'tn', '\\t', 'fp', '\\t', 'fn', '\\t', '\\t', 'prec', '\\t', 'rec', '\\t', 'f1', '\\t', '\\t', 'accuracy')\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1 = []\n",
    "    cat = ['AMBIENCE', 'FOOD', 'SERVICE', 'PRICES']\n",
    "    for c in cat:\n",
    "        a, b, c, d, e, f, g, h = calculate(c, tipe)\n",
    "        precision.append(e)\n",
    "        recall.append(f)\n",
    "        f1.append(g)\n",
    "#         sf = sf.append({'TP':a, 'TN':b, 'FP':c, 'FN':d, 'precision':e, 'recall':f, 'f1':g, 'accuracy': h}, ignore_index=True)\n",
    "#         sf.to_excel(\"Results/Calculation/\"+ tipe +\".xlsx\")\n",
    "    print('', '\\t', '', '\\t', '', '\\t', '', '\\t', '\\t', round(np.mean(precision),2), '\\t', round(np.mean(recall),2), '\\t', calculate_f1(round(np.mean(precision),2), round(np.mean(recall),2)), '\\t', '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def run_lex(category):\n",
    "    df = pd.read_csv('Results/'+ category +'.csv')\n",
    "    sf = pd.DataFrame(columns = ['id','review', 'opinion', 'label', 'predict'])\n",
    "    for index in range(0, len(df)):\n",
    "        opinion = []\n",
    "        if type(df['opinion'][index]) != float:\n",
    "            opinion = df['opinion'][index].split('|')\n",
    "            \n",
    "        prediction = predict_lexicon(opinion, preprocessing(df['review'][index]))\n",
    "        sf = sf.append({'id': df['id'][index], \n",
    "            'review': df['review'][index],\n",
    "            'opinion': df['opinion'][index],\n",
    "            'label': df['label'][index],\n",
    "            'predict': prediction\n",
    "           }, ignore_index=True)\n",
    "    \n",
    "    sf.to_csv(\"Results/Sentiwordnet/hasil_lex_\"+ category +\".csv\")\n",
    "    sf.to_excel(\"Results/Sentiwordnet/hasil_lex_\"+ category +\".xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def run(category):\n",
    "    df = pd.read_csv('Results/'+ category +'.csv')\n",
    "    sf = pd.DataFrame(columns = ['id','review', 'opinion', 'label', 'predict','score'])\n",
    "    for index in range(0, len(df)):\n",
    "        opinion = []\n",
    "        if type(df['opinion'][index]) != float:\n",
    "            opinion = df['opinion'][index].split('|')\n",
    "            \n",
    "        prediction, score = predict_sentiwordnet(opinion, preprocessing(df['review'][index]))\n",
    "        sf = sf.append({'id': df['id'][index], \n",
    "            'review': df['review'][index],\n",
    "            'opinion': df['opinion'][index],\n",
    "            'label': df['label'][index],\n",
    "            'predict': prediction,\n",
    "            'score': score\n",
    "           }, ignore_index=True)\n",
    "    \n",
    "    sf.to_csv(\"Results/Sentiwordnet/hasil_\"+ category +\".csv\")\n",
    "    sf.to_excel(\"Results/Sentiwordnet/hasil_\"+ category +\".xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def run_lesk(category):\n",
    "    df = pd.read_csv('Results/'+ category +'.csv')\n",
    "    sf = pd.DataFrame(columns = ['id','review', 'opinion', 'label', 'predict','score'])\n",
    "    for index in range(0, len(df)):\n",
    "        opinion = []\n",
    "        if type(df['opinion'][index]) != float:\n",
    "            opinion = df['opinion'][index].split('|')\n",
    "            \n",
    "        prediction, score = predict_sentiwordnet_lesk(opinion, preprocessing(df['review'][index]))\n",
    "        sf = sf.append({'id': df['id'][index], \n",
    "            'review': df['review'][index],\n",
    "            'opinion': df['opinion'][index],\n",
    "            'label': df['label'][index],\n",
    "            'predict': prediction,\n",
    "            'score': score\n",
    "           }, ignore_index=True)\n",
    "    \n",
    "    sf.to_csv(\"Results/Sentiwordnet/hasil_lesk_\"+ category +\".csv\")\n",
    "    sf.to_excel(\"Results/Sentiwordnet/hasil_lesk_\"+ category +\".xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def predict(category):\n",
    "    df = pd.read_csv(category + '.csv')\n",
    "    sf = pd.DataFrame(columns = ['id','review', 'opinion', 'label', 'predict'])\n",
    "    for index in range(0, len(df)):\n",
    "        opinion = []\n",
    "        if type(df['opinion'][index]) != float:\n",
    "            opinion = df['opinion'][index].split('|')\n",
    "            \n",
    "        prediction = predict_lexicon(opinion, preprocessing(df['review'][index]))\n",
    "        sf = sf.append({'id': df['id'][index], \n",
    "            'review': df['review'][index],\n",
    "            'opinion': df['opinion'][index],\n",
    "            'label': df['label'][index],\n",
    "            'predict': prediction\n",
    "           }, ignore_index=True)\n",
    "    \n",
    "    sf.to_csv(\"output_\"+ category +\".csv\")\n",
    "    sf.to_excel(\"output_\"+ category +\".xlsx\")\n",
    "\n",
    "predict('FOOD')\n",
    "predict('AMBIENCE')\n",
    "predict('SERVICE')\n",
    "predict('PRICES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_lex('FOOD')\n",
    "run_lex('AMBIENCE')\n",
    "run_lex('SERVICE')\n",
    "run_lex('PRICES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run('FOOD')\n",
    "run('AMBIENCE')\n",
    "run('SERVICE')\n",
    "run('PRICES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# run_lesk('FOOD')\n",
    "# run_lesk('AMBIENCE')\n",
    "# run_lesk('SERVICE')\n",
    "# run_lesk('PRICES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_hasil('lex_') # lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_hasil('') # sentiwordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_hasil('lesk_') # sentiwordnet + wsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'long' in op_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import os\n",
    "\n",
    "positive_lexicon = []\n",
    "negative_lexicon = []\n",
    "\n",
    "def read_lexicon():\n",
    "    global positive_lexicon;\n",
    "    global negative_lexicon;\n",
    "    \n",
    "    with open(os.path.join(os.path.abspath('opinion-lexicon-English/') , 'positive-words.txt'), 'r') as file:\n",
    "        line = file.readline();\n",
    "        while \";\" in line:\n",
    "            line = file.readline();\n",
    "         \n",
    "        positive_lexicon = file.readlines()\n",
    "    \n",
    "    with open(os.path.join(os.path.abspath('opinion-lexicon-English/') , 'negative-words.txt'), 'r', encoding = \"ISO-8859-1\") as file:\n",
    "        line = file.readline();\n",
    "        while \";\" in line:\n",
    "            line = file.readline();\n",
    "        \n",
    "        negative_lexicon = file.readlines()\n",
    "        \n",
    "    positive_lexicon = list(map(lambda word: word.rstrip(\"\\n\\r\"), positive_lexicon))\n",
    "    negative_lexicon = list(map(lambda word: word.rstrip(\"\\n\\r\"), negative_lexicon))\n",
    "    \n",
    "        \n",
    "read_lexicon()\n",
    "op_set = positive_lexicon + negative_lexicon\n",
    "\n",
    "negation = [\n",
    "    \"afraid\",\n",
    "    \"can't\",\n",
    "    \"cannot\",\n",
    "    \"deny\",\n",
    "    \"mean\",\n",
    "    \"negate\",\n",
    "    \"negation\",\n",
    "    \"negative\",\n",
    "    \"neither\",\n",
    "    \"never\",\n",
    "    \"no\",\n",
    "    \"non\",\n",
    "    \"none\",\n",
    "    \"nor\",\n",
    "    \"not\",\n",
    "    \"nothing\",\n",
    "    \"refusal\",\n",
    "    \"refuse\",\n",
    "    \"reject\",\n",
    "    \"rejection\"\n",
    "]\n",
    "\n",
    "def predict_lexicon(opinions, sent):\n",
    "    endscore = 0\n",
    "    for opinion in opinions:\n",
    "        if opinion in positive_lexicon:\n",
    "            endscore += 1\n",
    "        elif opinion in negative_lexicon:\n",
    "            endscore -= 1\n",
    "\n",
    "        words = sent.split(' ')\n",
    "        word_around = []\n",
    "        for x in range(0, len(words)):\n",
    "            if words[x] in string.punctuation:\n",
    "                continue\n",
    "            try:\n",
    "                if (words[x+1] == opinion) or (words[x+2] == opinion) or (words[x+3] == opinion) or (words[x+4] == opinion):\n",
    "                    word_around.append(words[x])\n",
    "                elif (words[x-1] == opinion):\n",
    "                    word_around.append(words[x])\n",
    "            except:\n",
    "                pass\n",
    "        for neg in negation:\n",
    "            if neg in word_around:\n",
    "                endscore *= (-1)\n",
    "                break\n",
    "\n",
    "    if endscore > 0:\n",
    "        polarity = 'positive'\n",
    "    else:\n",
    "        polarity = 'negative'\n",
    "           \n",
    "    return polarity\n",
    "\n",
    "def predict(category):\n",
    "    df = pd.read_csv(category + '.csv')\n",
    "    sf = pd.DataFrame(columns = ['id','review', 'opinion', 'label', 'predict'])\n",
    "    for index in range(0, len(df)):\n",
    "        opinion = []\n",
    "        if type(df['opinion'][index]) != float:\n",
    "            opinion = df['opinion'][index].split('|')\n",
    "            \n",
    "        prediction = predict_lexicon(opinion, preprocessing(df['review'][index]))\n",
    "        sf = sf.append({'id': df['id'][index], \n",
    "            'review': df['review'][index],\n",
    "            'opinion': df['opinion'][index],\n",
    "            'label': df['label'][index],\n",
    "            'predict': prediction\n",
    "           }, ignore_index=True)\n",
    "    \n",
    "    sf.to_csv(\"output_\"+ category +\".csv\")\n",
    "    sf.to_excel(\"output_\"+ category +\".xlsx\")\n",
    "\n",
    "predict('FOOD')\n",
    "predict('AMBIENCE')\n",
    "predict('SERVICE')\n",
    "predict('PRICES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'FOOD.csv' does not exist: b'FOOD.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-cb75f3afe9de>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[0msf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"output_\"\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mcategory\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;34m\".xlsx\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'FOOD'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'AMBIENCE'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'SERVICE'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-cb75f3afe9de>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(category)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcategory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcategory\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m     \u001b[0msf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'review'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'opinion'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'label'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'predict'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 429\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1121\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1122\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1123\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1124\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1852\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1855\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'FOOD.csv' does not exist: b'FOOD.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "\n",
    "negation = [\n",
    "    \"afraid\",\n",
    "    \"can't\",\n",
    "    \"cannot\",\n",
    "    \"deny\",\n",
    "    \"mean\",\n",
    "    \"negate\",\n",
    "    \"negation\",\n",
    "    \"negative\",\n",
    "    \"neither\",\n",
    "    \"never\",\n",
    "    \"no\",\n",
    "    \"non\",\n",
    "    \"none\",\n",
    "    \"nor\",\n",
    "    \"not\",\n",
    "    \"nothing\",\n",
    "    \"refusal\",\n",
    "    \"refuse\",\n",
    "    \"reject\",\n",
    "    \"rejection\"\n",
    "]\n",
    "\n",
    "def predict_sentiwordnet(opinions, sent):\n",
    "    endscore = 0\n",
    "    for opinion in opinions:\n",
    "        try:\n",
    "            score = swn.senti_synset(opinion + '.a.1')\n",
    "            if score.pos_score() > score.neg_score():\n",
    "                endscore += score.pos_score()\n",
    "            else:\n",
    "                endscore += score.neg_score() * (-1)\n",
    "\n",
    "            words = sent.split(' ')\n",
    "            word_around = []\n",
    "            for x in range(0, len(words)):\n",
    "                if words[x] in string.punctuation:\n",
    "                    continue\n",
    "                try:\n",
    "                    if (words[x+1] == opinion) or (words[x+2] == opinion) or (words[x+3] == opinion) or (words[x+4] == opinion):\n",
    "                        word_around.append(words[x])\n",
    "                    elif (words[x-1] == opinion):\n",
    "                        word_around.append(words[x])\n",
    "                except:\n",
    "                    pass\n",
    "            for neg in negation:\n",
    "                if neg in word_around:\n",
    "                    endscore *= (-1)\n",
    "                    break\n",
    "        except:\n",
    "            pass\n",
    "    if endscore > 0:\n",
    "        polarity = 'positive'\n",
    "    else:\n",
    "        polarity = 'negative'\n",
    "           \n",
    "    return polarity, endscore\n",
    "\n",
    "def predict(category):\n",
    "    df = pd.read_csv(category + '.csv')\n",
    "    sf = pd.DataFrame(columns = ['id','review', 'opinion', 'label', 'predict'])\n",
    "    for index in range(0, len(df)):\n",
    "        opinion = []\n",
    "        if type(df['opinion'][index]) != float:\n",
    "            opinion = df['opinion'][index].split('|')\n",
    "            \n",
    "        prediction = predict_sentiwordnet(opinion, preprocessing(df['review'][index]))\n",
    "        sf = sf.append({'id': df['id'][index], \n",
    "            'review': df['review'][index],\n",
    "            'opinion': df['opinion'][index],\n",
    "            'label': df['label'][index],\n",
    "            'predict': prediction\n",
    "           }, ignore_index=True)\n",
    "    \n",
    "    sf.to_csv(\"output_\"+ category +\".csv\")\n",
    "    sf.to_excel(\"output_\"+ category +\".xlsx\")\n",
    "\n",
    "predict('FOOD')\n",
    "predict('AMBIENCE')\n",
    "predict('SERVICE')\n",
    "predict('PRICES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
