{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ROOT\n",
      "  (S\n",
      "    (NP (`` ``) (NN iPod) ('' ''))\n",
      "    (VP (VBZ is)\n",
      "      (NP (DT the) (JJS best) (NN mp3) (NN player)))\n",
      "    (. .)))\n"
     ]
    }
   ],
   "source": [
    "from pycorenlp import StanfordCoreNLP\n",
    "import json\n",
    "\n",
    "nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "dependency_parser = nlp.annotate\n",
    "\n",
    "document = [\n",
    "#             'The phone has a good \"screen\".',\n",
    "            '\"iPod\" is the best mp3 player.',\n",
    "#             'Does the player play dvd with audio and \"video\"?',\n",
    "#             'The camera is amazing and \"easy\" to use.',\n",
    "#             'Canon \"G3\" has a great len.',\n",
    "#             'If you want to buy a sexy, \"cool\", accessory-available mp3 player, you can choose iPod.'\n",
    "           ]\n",
    "\n",
    "# Dependency relations describing relations between opinion words and targets\n",
    "# tak tambahi amod & nsubj\n",
    "MR = ['mod', 'pnmod', 'subj', 's', 'obj', 'obj2','desc', 'amod', 'nsubj']\n",
    "CONJ = ['conj', 'compound']\n",
    "\n",
    "same_mod = ['mod', 'pnmod', 'amod']\n",
    "same_bj = ['subj', 'obj', 'nsubj']\n",
    "\n",
    "# Initial Opinion Words\n",
    "opinion_words = [u'good', u'great', u'best', u'easy', u'cool']\n",
    "ow_set = set(opinion_words)\n",
    "\n",
    "# Initial Aspects (bukan dari paper, ini tambahan sendiri)\n",
    "aspects = [u'video', u'G3']\n",
    "a_set = set(aspects)\n",
    "\n",
    "\n",
    "# Loop 1\n",
    "for sentence in document:\n",
    "    # Parse\n",
    "    result = dependency_parser(sentence, properties={\n",
    "        \"outputFormat\": \"json\",\n",
    "        \"annotators\": \"pos, depparse, sentiment\"})\n",
    "    \n",
    "#     print(result)\n",
    "    for res in result['sentences']:\n",
    "        print(res['parse'])\n",
    "        # Rule 1.1\n",
    "#         for data in res['basicDependencies']:\n",
    "#             w1 = data['governorGloss']\n",
    "#             w2 = data['dependentGloss']\n",
    "#             w1_pos = res['tokens'][data['governor']-1]['pos']\n",
    "#             w2_pos = res['tokens'][data['dependent']-1]['pos']\n",
    "#             dep = data['dep']\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['iPod', 'mp3 player']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer()\n",
    "\n",
    "def process_content(sentence):\n",
    "    try:\n",
    "        tokenized = custom_sent_tokenizer.tokenize(sentence)\n",
    "        for i in tokenized:\n",
    "            hasil = []\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            chunkGram = r\"\"\"Chunk: {<DT>?<NN.*|JJ>?<IN>?<PRP.*>?<NN.*>}\"\"\"\n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            for subtree in chunked.subtrees(filter=lambda t: t.label() == 'Chunk'):\n",
    "                panjang = len(subtree)\n",
    "                rangkaian_kata = ''\n",
    "                for i in range(0,panjang):\n",
    "                    if i == 0:\n",
    "                        rangkaian_kata += subtree[i][0]\n",
    "                    else:\n",
    "                        rangkaian_kata += ' ' + subtree[i][0]\n",
    "                hasil.append(rangkaian_kata)\n",
    "            return hasil\n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        \n",
    "\n",
    "print(process_content('\"iPod\" is the best mp3 player.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycorenlp import StanfordCoreNLP\n",
    "import json\n",
    "\n",
    "nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "dependency_parser = nlp.annotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = dependency_parser('i buy it for $9', properties={\n",
    "        \"outputFormat\": \"json\",\n",
    "        \"annotators\": \"ner\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentences': [{'entitymentions': [{'characterOffsetBegin': 13,\n",
       "     'characterOffsetEnd': 15,\n",
       "     'docTokenBegin': 4,\n",
       "     'docTokenEnd': 6,\n",
       "     'ner': 'MONEY',\n",
       "     'normalizedNER': '$9.0',\n",
       "     'text': '$9',\n",
       "     'tokenBegin': 4,\n",
       "     'tokenEnd': 6}],\n",
       "   'index': 0,\n",
       "   'tokens': [{'after': ' ',\n",
       "     'before': '',\n",
       "     'characterOffsetBegin': 0,\n",
       "     'characterOffsetEnd': 1,\n",
       "     'index': 1,\n",
       "     'lemma': 'i',\n",
       "     'ner': 'O',\n",
       "     'originalText': 'i',\n",
       "     'pos': 'LS',\n",
       "     'word': 'i'},\n",
       "    {'after': ' ',\n",
       "     'before': ' ',\n",
       "     'characterOffsetBegin': 2,\n",
       "     'characterOffsetEnd': 5,\n",
       "     'index': 2,\n",
       "     'lemma': 'buy',\n",
       "     'ner': 'O',\n",
       "     'originalText': 'buy',\n",
       "     'pos': 'VB',\n",
       "     'word': 'buy'},\n",
       "    {'after': ' ',\n",
       "     'before': ' ',\n",
       "     'characterOffsetBegin': 6,\n",
       "     'characterOffsetEnd': 8,\n",
       "     'index': 3,\n",
       "     'lemma': 'it',\n",
       "     'ner': 'O',\n",
       "     'originalText': 'it',\n",
       "     'pos': 'PRP',\n",
       "     'word': 'it'},\n",
       "    {'after': ' ',\n",
       "     'before': ' ',\n",
       "     'characterOffsetBegin': 9,\n",
       "     'characterOffsetEnd': 12,\n",
       "     'index': 4,\n",
       "     'lemma': 'for',\n",
       "     'ner': 'O',\n",
       "     'originalText': 'for',\n",
       "     'pos': 'IN',\n",
       "     'word': 'for'},\n",
       "    {'after': '',\n",
       "     'before': ' ',\n",
       "     'characterOffsetBegin': 13,\n",
       "     'characterOffsetEnd': 14,\n",
       "     'index': 5,\n",
       "     'lemma': '$',\n",
       "     'ner': 'MONEY',\n",
       "     'normalizedNER': '$9.0',\n",
       "     'originalText': '$',\n",
       "     'pos': '$',\n",
       "     'word': '$'},\n",
       "    {'after': '',\n",
       "     'before': '',\n",
       "     'characterOffsetBegin': 14,\n",
       "     'characterOffsetEnd': 15,\n",
       "     'index': 6,\n",
       "     'lemma': '9',\n",
       "     'ner': 'MONEY',\n",
       "     'normalizedNER': '$9.0',\n",
       "     'originalText': '9',\n",
       "     'pos': 'CD',\n",
       "     'word': '9'}]}]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
