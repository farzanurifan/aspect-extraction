{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warming up PyWSD (takes ~10 secs)... took 6.164828300476074 secs.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "from pywsd.lesk import simple_lesk\n",
    "from pywsd.lesk import cosine_lesk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import requests\n",
    "import sys, os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "dependency_parser = nlp.annotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "breakdown = swn.senti_synset('good.n.01')\n",
    "print(breakdown.pos_score())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('love.n.01'),\n",
       " Synset('love.n.02'),\n",
       " Synset('beloved.n.01'),\n",
       " Synset('love.n.04'),\n",
       " Synset('love.n.05'),\n",
       " Synset('sexual_love.n.02'),\n",
       " Synset('love.v.01'),\n",
       " Synset('love.v.02'),\n",
       " Synset('love.v.03'),\n",
       " Synset('sleep_together.v.01')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('love')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_lexicon = []\n",
    "negative_lexicon = []\n",
    "\n",
    "def read_lexicon():\n",
    "    global positive_lexicon;\n",
    "    global negative_lexicon;\n",
    "    \n",
    "    with open(os.path.join(os.path.abspath('opinion-lexicon-English/') , 'positive-words.txt'), 'r') as file:\n",
    "        line = file.readline();\n",
    "        while \";\" in line:\n",
    "            line = file.readline();\n",
    "         \n",
    "        positive_lexicon = file.readlines()\n",
    "    \n",
    "    with open(os.path.join(os.path.abspath('opinion-lexicon-English/') , 'negative-words.txt'), 'r', encoding = \"ISO-8859-1\") as file:\n",
    "        line = file.readline();\n",
    "        while \";\" in line:\n",
    "            line = file.readline();\n",
    "        \n",
    "        negative_lexicon = file.readlines()\n",
    "        \n",
    "    positive_lexicon = list(map(lambda word: word.rstrip(\"\\n\\r\"), positive_lexicon))\n",
    "    negative_lexicon = list(map(lambda word: word.rstrip(\"\\n\\r\"), negative_lexicon))\n",
    "    \n",
    "        \n",
    "read_lexicon()\n",
    "op_set = positive_lexicon + negative_lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import re\n",
    "import pandas as pd\n",
    "import itertools, nltk, string \n",
    "\n",
    "def read_file(file):\n",
    "    f = open(file, 'r')\n",
    "\n",
    "    pattern_title = '\\[t\\]'\n",
    "    pattern_sentence = '(?<=##).+'\n",
    "    pattern_aspect = '.+(?=##)'\n",
    "\n",
    "    review = []\n",
    "    for a in f:\n",
    "        if re.search('##', a):\n",
    "            sentence = re.findall(pattern_sentence, a)[0]\n",
    "            aspect = re.findall(pattern_aspect, a)\n",
    "            if len(aspect) > 0:\n",
    "                aspect = aspect[0]\n",
    "            else:\n",
    "                aspect = ''\n",
    "            review.append((sentence, aspect))\n",
    "\n",
    "    df = pd.DataFrame(columns=['review','target'])\n",
    "    for r in review:\n",
    "        df = df.append({'review': r[0], 'target': r[1]}, ignore_index=True)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tag(sentence):\n",
    "    result = dependency_parser(sentence, properties={\"outputFormat\": \"json\", \"annotators\": \"pos\"})['sentences'][0]['tokens']\n",
    "    res = []\n",
    "    for pos in result:\n",
    "        res.append((pos['word'], pos['pos']))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(sentence, key, tagged_sentence):\n",
    "    flag = True\n",
    "    \n",
    "    tagged = ('','')\n",
    "    for p in tagged_sentence:\n",
    "        if p[0] == key:\n",
    "            tagged = p\n",
    "    if tagged == ('', ''):\n",
    "        flag = False\n",
    "\n",
    "    ambiguous = tagged[0]\n",
    "    tag = tagged[1]\n",
    "    pos = ''\n",
    "\n",
    "    if 'NN' in tag:\n",
    "        pos = 'n'\n",
    "    elif 'NNS' in tag:\n",
    "        pos = 'nns'\n",
    "    elif 'VB' in tag:\n",
    "        pos = 'v'\n",
    "    elif 'VBG' in tag:\n",
    "        pos = 'v'\n",
    "    elif 'JJ' in tag:\n",
    "        pos = 'a'\n",
    "    elif 'RB' in tag:\n",
    "        pos = 'r'\n",
    "    else:\n",
    "        flag = False\n",
    "\n",
    "    endscore = 0\n",
    "    if flag:\n",
    "        answer = 1#cosine_lesk(sentence, ambiguous, pos)\n",
    "        if answer:\n",
    "            try:\n",
    "                score = swn.senti_synset(ambiguous + '.' + 'a' + '.01')\n",
    "\n",
    "                if score.pos_score() > score.neg_score():\n",
    "                    endscore = score.pos_score()\n",
    "                else:\n",
    "                    endscore = score.neg_score() * (-1)\n",
    "            except:\n",
    "                endscore = 0\n",
    "    return endscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "negation = [\n",
    "    \"afraid\",\n",
    "    \"can't\",\n",
    "    \"cannot\",\n",
    "    \"deny\",\n",
    "    \"mean\",\n",
    "    \"negate\",\n",
    "    \"negation\",\n",
    "    \"negative\",\n",
    "    \"neither\",\n",
    "    \"never\",\n",
    "    \"no\",\n",
    "    \"non\",\n",
    "    \"none\",\n",
    "    \"nor\",\n",
    "    \"not\",\n",
    "    \"nothing\",\n",
    "    \"refusal\",\n",
    "    \"refuse\",\n",
    "    \"reject\",\n",
    "    \"rejection\"\n",
    "]\n",
    "\n",
    "\n",
    "def get_sentence_score(sentence):\n",
    "    scores = []\n",
    "    tagged_sentence = pos_tag(sentence)\n",
    "    for word in sentence.split(' '):\n",
    "        if word in op_set:\n",
    "            score = get_score(sentence, word, tagged_sentence)\n",
    "            scores.append(score)\n",
    "\n",
    "    if len(scores):\n",
    "        for neg in negation:\n",
    "            if neg in sentence:\n",
    "                return np.mean(scores) * (-1)\n",
    "        return np.mean(scores)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_program(review, target, filename):\n",
    "    scores = []\n",
    "    labels = []\n",
    "    index = 0\n",
    "    for sentence in review:\n",
    "        aspects = target[index].split(',')\n",
    "        label = 0\n",
    "        if aspects[0] != '':\n",
    "            for aspect in aspects:\n",
    "                try:\n",
    "                    tanda = aspect.split('[')[1][0]\n",
    "                    angka = aspect.split('[')[1][1]\n",
    "                    if tanda == '+':\n",
    "                        label += int(angka)\n",
    "                    else:\n",
    "                        label -= int(angka)\n",
    "                except:\n",
    "                    pass\n",
    "#                     print(aspect)\n",
    "            if label != 0:\n",
    "                score = get_sentence_score(sentence)\n",
    "\n",
    "                if label > 0:\n",
    "                    labels.append(1)\n",
    "                elif label < 0:\n",
    "                    labels.append(2)\n",
    "\n",
    "                if score >= 0:\n",
    "                    scores.append(1)\n",
    "                elif score < 0:\n",
    "                    scores.append(2)\n",
    "            \n",
    "        index += 1\n",
    "    \n",
    "#     data = { 'review':review, 'label': labels, 'prediction': scores}\n",
    "#     out = pd.DataFrame(data)\n",
    "#     out.to_csv(filename)\n",
    "    \n",
    "    return labels, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(sentences):\n",
    "    res = []\n",
    "    for sentence in sentences:\n",
    "        try:\n",
    "            res.append(sentence.replace(\"n't\", \"not\").replace(\"'m\", \"am\"))\n",
    "        except:\n",
    "            pass\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = [\n",
    "            'Apex AD2600 Progressive-scan DVD player',\n",
    "            'Canon G3',\n",
    "            'Creative Labs Nomad Jukebox Zen Xtra 40GB',\n",
    "            'Nikon coolpix 4300',\n",
    "            'Nokia 6610'\n",
    "]\n",
    "\n",
    "def run_all():\n",
    "    out_file = pd.DataFrame(columns=['name', 'TP', 'TN', 'FP', 'FN'])\n",
    "    for file in filename:\n",
    "        TP = 0\n",
    "        TN = 0\n",
    "        FP = 0\n",
    "        FN = 0\n",
    "        \n",
    "        df = read_file('dataset/bing_liu/' + file + '.txt')\n",
    "        label, pred = main_program(preprocessing(df['review']), df['target'], 'sentiwordnet.csv')\n",
    "        for x in range(0, len(label) - 1):\n",
    "            l = label[x]\n",
    "            p = pred[x]\n",
    "            if l == 1 and p == 1:\n",
    "                TP += 1\n",
    "            elif l == 2 and p == 2:\n",
    "                TN += 1\n",
    "            elif l == 1 and p == 2:\n",
    "                FP += 1\n",
    "            elif l == 2 and p == 1:\n",
    "                FN += 1\n",
    "        \n",
    "        out_file = out_file.append({'name': file, 'TP': TP, 'TN': TN, 'FP': FP, 'FN': FN}, ignore_index=True)\n",
    "        out_file.to_csv(\"result-sentiwordnet-v3.csv\")\n",
    "        out_file.to_excel(\"result-sentiwordnet-v3.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
