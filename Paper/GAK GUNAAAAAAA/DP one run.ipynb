{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import re\n",
    "import pandas as pd\n",
    "import itertools, nltk, string \n",
    "from nltk.parse.corenlp import CoreNLPDependencyParser\n",
    "\n",
    "parser = CoreNLPDependencyParser()\n",
    "\n",
    "def read_file(file):\n",
    "    f = open(file, 'r')\n",
    "    pattern_title = '\\[t\\]'\n",
    "    pattern_sentence = '(?<=##).+'\n",
    "    pattern_aspect = '.+(?=##)'\n",
    "    review = []\n",
    "    for a in f:\n",
    "        if re.search('##', a):\n",
    "            sentence = re.findall(pattern_sentence, a)[0]\n",
    "            aspect = re.findall(pattern_aspect, a)\n",
    "            flag = True\n",
    "            if len(aspect) > 0:\n",
    "                aspect = aspect[0]\n",
    "            else:\n",
    "                aspect = ''\n",
    "            if flag:\n",
    "                review.append((sentence, aspect))\n",
    "    df = pd.DataFrame(columns=['review','target'])\n",
    "    for r in review:\n",
    "        df = df.append({'review': r[0], 'target': r[1]}, ignore_index=True)\n",
    "    return df\n",
    "        \n",
    "positive_lexicon = []\n",
    "negative_lexicon = []\n",
    "\n",
    "def read_lexicon():\n",
    "    global positive_lexicon;\n",
    "    global negative_lexicon;    \n",
    "    with open(os.path.join(os.path.abspath('opinion-lexicon-English/') , 'positive-words.txt'), 'r') as file:\n",
    "        line = file.readline();\n",
    "        while \";\" in line:\n",
    "            line = file.readline();\n",
    "        positive_lexicon = file.readlines()\n",
    "    with open(os.path.join(os.path.abspath('opinion-lexicon-English/') , 'negative-words.txt'), 'r', encoding = \"ISO-8859-1\") as file:\n",
    "        line = file.readline();\n",
    "        while \";\" in line:\n",
    "            line = file.readline();\n",
    "        negative_lexicon = file.readlines()\n",
    "    positive_lexicon = list(map(lambda word: word.rstrip(\"\\n\\r\"), positive_lexicon))\n",
    "    negative_lexicon = list(map(lambda word: word.rstrip(\"\\n\\r\"), negative_lexicon))\n",
    "        \n",
    "read_lexicon()\n",
    "df = read_file('dataset/bing_liu/Nokia 6610.txt')\n",
    "\n",
    "import os\n",
    "os.environ[\"CORENLP_HOME\"] = r'C:\\stanford-corenlp-full-2018-10-05'\n",
    "\n",
    "import corenlp \n",
    "client = corenlp.CoreNLPClient()\n",
    "\n",
    "def preprocess(sentences, chunk = False):\n",
    "    res = []\n",
    "    for sentence in sentences:\n",
    "        text = sentence.lower()\n",
    "        if chunk:\n",
    "            try:\n",
    "#                 pattern = '[{pos:NN}][{pos:NN}][{pos:NN}] | [{pos:NN}][{pos:NN}]'\n",
    "                pattern = '[{pos:JJ}][{pos:NN}]'\n",
    "                matches = client.tokensregex(sentence, pattern)\n",
    "                response = matches['sentences'][0]\n",
    "                len_chunk = response['length']\n",
    "                if len_chunk > 0:\n",
    "                    for index in range(0, len_chunk):\n",
    "                        replacer = response[str(index)]['text']\n",
    "                        print(replacer)\n",
    "                        text = text.replace(replacer, '-'.join(replacer.split(' ')))\n",
    "                \n",
    "            except:\n",
    "                text = text\n",
    "        res.append(text)\n",
    "    return res\n",
    "\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN = ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "JJ = ['JJ', 'JJR', 'JJS']\n",
    "\n",
    "def rule_1_1(parse, f, o_expanded):\n",
    "    #Rule 1.1\n",
    "#     print(dep_DP)\n",
    "    for (w1, dep, w2) in list(parse.triples()):\n",
    "        if(dep in dep_DP):\n",
    "            # Rule 1.1\n",
    "            if(w1[0] in o_expanded):\n",
    "                if w2[1] in NN and w2[0] not in f:\n",
    "                    return True, w2[0], w1[0]\n",
    "            elif(w2[0] in o_expanded):\n",
    "                if w1[1] in NN and w1[0] not in f:\n",
    "                    return True, w1[0], w2[0]\n",
    "    return False, None, None\n",
    "\n",
    "def rule_1_2(parse, f, o_expanded):\n",
    "    # Rule 1.2\n",
    "    for (w1, dep, w2) in parse.triples():\n",
    "        if(dep in dep_DP):\n",
    "            H = ''\n",
    "            O = ''\n",
    "            if w1[0] in o_expanded:\n",
    "                H = w2[0]\n",
    "                O = w1\n",
    "            elif w2[0] in o_expanded:\n",
    "                H = w1[0]\n",
    "                O = w2\n",
    "\n",
    "            if H:\n",
    "                for (w1, dep, w2) in list(parse.triples()):\n",
    "                    if w1[0] == H and w2[0] != O[0]:\n",
    "                        if w2[1] in NN and w2[0] not in f:\n",
    "                            return True, w2[0], O[0]\n",
    "                    elif w2[0] == H  and w1[0] != O[0]:\n",
    "                        if w1[1] in NN and w1[0] not in f:\n",
    "                            return True, w1[0], O[0]\n",
    "    return False, None, None\n",
    "\n",
    "def rule_4_1(parse, f, o_expanded):\n",
    "    # Rule 4.1\n",
    "    for (w1, dep, w2) in list(parse.triples()):\n",
    "        if(dep in conj_DP):\n",
    "            if w1[0] in o_expanded:\n",
    "                if w2[1] in JJ and w2[0] not in o_expanded:\n",
    "                    return True, w2[0]\n",
    "\n",
    "            elif w2[0] in o_expanded:\n",
    "                if w1[1] in JJ and w1[0] not in o_expanded:\n",
    "                    return True, w1[0]\n",
    "\n",
    "    return False, None\n",
    "\n",
    "def rule_4_2(parse, f, o_expanded):\n",
    "    # Rule 4.2\n",
    "    for (w1, dep, w2) in parse.triples():\n",
    "        if(dep in dep_DP or dep in conj_DP):\n",
    "            H = ''\n",
    "            O = ''\n",
    "            if w1[0] in o_expanded:\n",
    "                H = w2[0]\n",
    "                O = w1\n",
    "            elif w2[0] in o_expanded:\n",
    "                H = w1[0]\n",
    "                O = w2\n",
    "\n",
    "            if H:\n",
    "                for (w1, dep, w2) in list(parse.triples()):\n",
    "                    if w1[0] == H and w2[0] != O[0]:\n",
    "                        if w2[1] in JJ and w2[0] not in o_expanded:\n",
    "                            return True, w2[0]\n",
    "\n",
    "                    elif w2[0] == H  and w1[0] != O[0]:\n",
    "                        if w1[1] in JJ and w1[0] not in o_expanded:\n",
    "                            return True, w1[0]\n",
    "    \n",
    "    return False, None\n",
    "\n",
    "def rule_3_1(parse, f):\n",
    "    #Rule 3.1\n",
    "    for (w1, dep, w2) in list(parse.triples()):\n",
    "        if(dep in conj_DP):\n",
    "            if(w1[0] in f): \n",
    "                if w2[1] in NN and w2[0] not in f:\n",
    "                    return True, w2[0], w1[0]\n",
    "            elif(w2[0] in f):          \n",
    "                if w1[1] in NN and w1[0] not in f:\n",
    "                    return True, w1[0], w2[0]\n",
    "                \n",
    "    return False, None, None\n",
    "\n",
    "def rule_3_2(parse, f):\n",
    "    # Rule 3.2\n",
    "    for (w1, dep, w2) in parse.triples():\n",
    "        if(dep in dep_DP or dep in conj_DP):\n",
    "            H = ''\n",
    "            O = ''\n",
    "            if w1[0] in f:\n",
    "                H = w2[0]\n",
    "                O = w1\n",
    "            elif w2[0] in f:\n",
    "                H = w1[0]\n",
    "                O = w2\n",
    "\n",
    "            if H:\n",
    "                for (w1, dep, w2) in list(parse.triples()):\n",
    "                    if w1[0] == H and w2[0] != O[0]:\n",
    "                        if w2[1] in NN and w2[0] not in f:\n",
    "                            return True, w2[0], O[0]\n",
    "                    elif w2[0] == H  and w1[0] != O[0]:\n",
    "                        if w1[1] in NN and w1[0] not in f:\n",
    "                            return True, w1[0], O[0]\n",
    "\n",
    "    return False, None, None\n",
    "\n",
    "def rule_2_1(parse, f, o_expanded):\n",
    "    # Rule 2.1\n",
    "    for (w1, dep, w2) in list(parse.triples()):\n",
    "        if(dep in dep_DP):\n",
    "            if w1[0] in f:\n",
    "                if w2[1] in JJ and w2[0] not in o_expanded:\n",
    "                    return True, w2[0]\n",
    "\n",
    "            elif w2[0] in f:\n",
    "                if w1[1] in JJ and w1[0] not in o_expanded:\n",
    "                    return True, w1[0]\n",
    "                \n",
    "    return False, None\n",
    "\n",
    "def rule_2_2(parse, f, o_expanded):      \n",
    "    # Rule 2.2\n",
    "    for (w1, dep, w2) in parse.triples():\n",
    "        if(dep in dep_DP):\n",
    "            H = ''\n",
    "            O = ''\n",
    "            if w1[0] in f:\n",
    "                H = w2[0]\n",
    "                O = w1\n",
    "            elif w2[0] in f:\n",
    "                H = w1[0]\n",
    "                O = w2\n",
    "\n",
    "            if H:\n",
    "                for (w1, dep, w2) in list(parse.triples()):\n",
    "                    if w1[0] == H and w2[0] != O[0]:\n",
    "                        if w2[1] in JJ and w2[0] not in o_expanded:    \n",
    "                            return True, w2[0]\n",
    "                    elif w2[0] == H  and w1[0] != O[0]:\n",
    "                        if w1[1] in JJ and w1[0] not in o_expanded:\n",
    "                            return True, w1[0]\n",
    "\n",
    "    return False, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "candidate_aspect = []\n",
    "new_opinion = []\n",
    "op_set = positive_lexicon + negative_lexicon\n",
    "\n",
    "def double_propagation(O: op_set, reviews, targets, output_file, save_to_file = False):\n",
    "    o_expanded = O\n",
    "    f = []\n",
    "    is_stop = False\n",
    "    flag_cycle = 0\n",
    "    \n",
    "    t_a_p = []\n",
    "    pair_a_o = []\n",
    "    a_p = []\n",
    "    r_p = []\n",
    "    \n",
    "    while (not is_stop):\n",
    "        f_i = []\n",
    "        o_i = []\n",
    "    \n",
    "        index = 0\n",
    "\n",
    "        for sentence in reviews:\n",
    "            aspect_sentence = []\n",
    "            if flag_cycle:\n",
    "                aspect_sentence = t_a_p[index].split('|')\n",
    "                \n",
    "            temp = []\n",
    "            pair = []\n",
    "                \n",
    "            try:\n",
    "                parse = next(parser.raw_parse(sentence))\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            # Rule 1.1\n",
    "            is_true, t_a, o = rule_1_1(parse, aspect_sentence, o_expanded)\n",
    "            if is_true:\n",
    "                f_i.append(t_a)\n",
    "                temp.append(t_a)\n",
    "                pair.append(t_a + '!' + o)\n",
    "\n",
    "            # Rule 1.2\n",
    "            is_true, t_a, o = rule_1_2(parse, aspect_sentence, o_expanded)\n",
    "            if is_true:\n",
    "                f_i.append(t_a)\n",
    "                temp.append(t_a)\n",
    "                pair.append(t_a + '!' + o)\n",
    "\n",
    "            # Rule 4.1\n",
    "            is_true, t_o = rule_4_1(parse, f, o_expanded)\n",
    "            if is_true:\n",
    "                o_i.append(t_o)\n",
    "\n",
    "            # Rule 4.2\n",
    "            is_true, t_o = rule_4_2(parse, f, o_expanded)\n",
    "            if is_true:\n",
    "                o_i.append(t_o)\n",
    "\n",
    "\n",
    "            if flag_cycle == 0:\n",
    "                r_p.append(sentence)\n",
    "                a_array = []\n",
    "                tes = []\n",
    "                try:\n",
    "                    tes = targets[index].split(', ')\n",
    "                except:\n",
    "                    tes = []\n",
    "                for x in tes:\n",
    "                    splitted = x.split('[')\n",
    "                    if len(splitted) > 1:\n",
    "                        a_array.append(splitted[0] + '!' + splitted[1][0])\n",
    "                    else:\n",
    "                        a_array.append(splitted[0])\n",
    "\n",
    "                a_p.append('|'.join(a_array))\n",
    "                t_a_p.append('|'.join(temp))\n",
    "                pair_a_o.append('|'.join(pair))\n",
    "            else:\n",
    "                if len(temp) != 0:\n",
    "                    t_a_p[index] += '|' + '|'.join(temp)\n",
    "                    pair_a_o[index] += '|' + '|'.join(pair)\n",
    "            index += 1\n",
    "            \n",
    "\n",
    "        #calculate target and opinion expanded\n",
    "        f = f + f_i \n",
    "        o_expanded = o_expanded + o_i\n",
    "\n",
    "        \n",
    "        #reread review, and run rule 3.1, 3.2, 2.1, and 2.2\n",
    "        index = 0\n",
    "        f_ii = []\n",
    "        o_ii = []\n",
    "        \n",
    "        for sentence in reviews:\n",
    "            \n",
    "            aspect_sentence = []\n",
    "            if flag_cycle:\n",
    "                aspect_sentence = t_a_p[index].split('|')\n",
    "                \n",
    "            temp = []\n",
    "            pair = []\n",
    "                \n",
    "            try:\n",
    "                parse = next(parser.raw_parse(sentence))\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            # Rule 3.1\n",
    "            is_true, t_a, o = rule_3_1(parse, aspect_sentence)\n",
    "            if is_true:\n",
    "                f_ii.append(t_a)\n",
    "                temp.append(t_a)\n",
    "                pair.append(t_a + '!' + o)\n",
    "\n",
    "            # Rule 3.2\n",
    "            is_true, t_a, o = rule_3_2(parse, aspect_sentence)\n",
    "            if is_true:\n",
    "                f_ii.append(t_a)\n",
    "                temp.append(t_a)\n",
    "                pair.append(t_a + '!' + o)\n",
    "\n",
    "\n",
    "            # Rule 2.1\n",
    "            is_true, t_o = rule_2_1(parse, f, o_expanded)\n",
    "            if is_true:\n",
    "                o_ii.append(t_o)\n",
    "\n",
    "            # Rule 2.2\n",
    "            is_true, t_o = rule_2_2(parse, f, o_expanded)\n",
    "            if is_true:\n",
    "                o_ii.append(t_o)\n",
    "\n",
    "            if len(temp) != 0:\n",
    "                t_a_p[index] += '|' + '|'.join(temp)\n",
    "                pair_a_o[index] += '|' + '|'.join(pair)\n",
    "\n",
    "            index += 1\n",
    "            \n",
    "        f_i = f_i + f_ii\n",
    "        o_i = o_i + o_ii\n",
    "        f = f + f_ii\n",
    "        o_expanded = o_expanded + o_ii     \n",
    "        \n",
    "        flag_cycle = 1\n",
    "        \n",
    "        if(len(f_i) == 0 and len(o_i) == 0):\n",
    "            if save_to_file == True:\n",
    "                out = pd.DataFrame(r_p)\n",
    "                out['aspect'] = a_p\n",
    "                out['prediction'] = pair_a_o\n",
    "                out.to_csv(output_file)\n",
    "            is_stop = True\n",
    "\n",
    "    return f, o_expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_frequency(aspects):\n",
    "    aspect_frequency = {}\n",
    "    \n",
    "    for aspect in aspects:\n",
    "        if(aspect in aspect_frequency):\n",
    "            aspect_frequency[aspect] += 1\n",
    "        else:\n",
    "            aspect_frequency[aspect] = 1\n",
    "            \n",
    "    return aspect_frequency\n",
    "\n",
    "k_DP = ['conj', 'compound']\n",
    "def pruning_based_on_clause(aspect_frequency, reviews, predictions):\n",
    "    pruning = []\n",
    "    index = 0\n",
    "    for review in reviews:\n",
    "        temp = predictions[index]\n",
    "        index += 1\n",
    "        if isinstance(temp, str):\n",
    "            prediction = temp.split('|')\n",
    "        else:\n",
    "            continue\n",
    "        parse = next(parser.raw_parse(review))\n",
    "        for (w1, dep, w2) in list(parse.triples()):\n",
    "            if(w1[0] in aspect_frequency and w2[0] in aspect_frequency):\n",
    "                if(dep not in k_DP):\n",
    "                    if(aspect_frequency[w1[0]] > aspect_frequency[w2[0]]):\n",
    "                        if w2[0] in prediction:\n",
    "                            pruning.append(w2[0])\n",
    "                    elif(aspect_frequency[w1[0]] < aspect_frequency[w2[0]]):\n",
    "                        if w1[0] in prediction:\n",
    "                            pruning.append(w1[0])\n",
    "                \n",
    "    return pruning\n",
    "def pruning_based_other_products_and_dealers(aspect_frequency, reviews,predictions, window=3):\n",
    "    pruning = []\n",
    "    ProductINDI = [\"compare to\", \"compare with\", \"better than\", \"worse than\"]\n",
    "    DealerINDI  = [\"shop with\", \"buy from\"]\n",
    "    count = 0\n",
    "    for review in reviews:\n",
    "        temp = predictions[count]\n",
    "        count += 1\n",
    "        if isinstance(temp, str):\n",
    "            prediction = temp.split('|')\n",
    "        else:\n",
    "            continue\n",
    "        if any(indication in review for indication in ProductINDI):\n",
    "            tokens = nltk.word_tokenize(review)\n",
    "            index = 0\n",
    "            while index < len(tokens) - 1:\n",
    "                if tokens[index] + \" \" + tokens[index + 1] in ProductINDI:\n",
    "                    index += 2\n",
    "                    for x in range(window):\n",
    "                        next_window = index + x + 1;\n",
    "                        if next_window < len(tokens) and tokens[next_window] in aspect_frequency:\n",
    "                            if tokens[next_window] in prediction:\n",
    "                                pruning.append(tokens[next_window])\n",
    "                else :\n",
    "                    index += 1\n",
    "                    \n",
    "        if any(indication in review for indication in DealerINDI):\n",
    "            tokens = nltk.word_tokenize(review)\n",
    "            index = 0\n",
    "            while index < len(tokens) - 1:\n",
    "                if tokens[index] + \" \" + tokens[index + 1] in DealerINDI:\n",
    "                    index += 2\n",
    "                    for x in range(window):\n",
    "                        next_window = index + x + 1;\n",
    "                        if next_window < len(tokens) and tokens[next_window] in aspect_frequency:\n",
    "                            if tokens[next_window] in prediction:\n",
    "                                pruning.append(tokens[next_window])\n",
    "                else :\n",
    "                    index += 1 \n",
    "                        \n",
    "    return pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def run(filename, save_file, chunking, path):\n",
    "    df = read_file(filename)\n",
    "        \n",
    "    dp_aspect, opinion_expand = double_propagation(op_set, preprocess(df['review'], chunking),\n",
    "                                                   df['target'], path + save_file, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nokia 6610\n",
      "mobile service\n",
      "great phone\n",
      "double check\n",
      "recent road\n",
      "northern kentucky\n",
      "perfect signal\n",
      "same route\n",
      "only feature\n",
      "previous nokia\n",
      "old phone\n",
      "t-mobile service\n",
      "horrible customer\n",
      "brief synopsis\n",
      "personal information\n",
      "good sound\n",
      "little memory\n",
      "full screen\n",
      "excellent hearing\n",
      "other person\n",
      "amazing battery\n",
      "exceptional service\n",
      "annoying habit\n",
      "same interaction\n",
      "good csr\n",
      "clear line\n",
      "expensive sanyo\n",
      "prepaid telephone\n",
      "apparent indestructibility\n",
      "other color\n",
      "personal touch\n",
      "nifty phone\n",
      "brisk jog\n",
      "flip phone\n",
      "frequent signal\n",
      "great care\n",
      "teeny phone\n",
      "big mechanism\n",
      "favorite phone\n",
      "great battery\n",
      "perfect size\n",
      "nice color\n",
      "major minus\n",
      "good server\n",
      "nice phone\n",
      "broken headphone\n",
      "much help\n",
      "t-mobile store\n",
      "unchangeable email\n",
      "online tech\n",
      "same problem\n",
      "half hour\n",
      "sound quality\n",
      "excellent fm\n",
      "poor visibility\n",
      "superb reception\n",
      "big deal\n",
      "useful feature\n",
      "other end\n",
      "decent reception\n",
      "normal radio\n",
      "beneficial feature\n",
      "only problem\n",
      "small glitch\n",
      "full bar\n",
      "international phone\n",
      "good chance\n",
      "rare occasion\n",
      "t-mobile store\n",
      "european phone\n",
      "european company\n",
      "extended period\n",
      "international roaming\n",
      "single bar\n",
      "perfect phone\n",
      "appealing package\n",
      "polyphonic ringing\n",
      "nice feature\n",
      "regular ring\n",
      "little phone\n",
      "sticky tho\n",
      "sexy piece\n",
      "functional device\n",
      "outstanding piece\n",
      "wireless telephone\n",
      "sound quality\n",
      "unaccustomed luxury\n",
      "t-mobile service\n",
      "little critter\n",
      "rectangular layout\n",
      "insurmoutable obstacle\n",
      "other gsm\n",
      "whole screen\n",
      "vibrate setting\n",
      "silent ringer\n",
      "good thing\n",
      "voice-activated dialing\n",
      "compact design\n",
      "high-quality construction\n",
      "well-backlit keypad\n",
      "outstanding signal\n",
      "sound quality\n",
      "visible screen\n",
      "long-lasting battery\n",
      "propietary headset\n",
      "super duper\n",
      "east bruvah\n",
      "key lock\n",
      "light weight\n",
      "much survey\n",
      "small size\n",
      "previous model\n",
      "free pc\n",
      "big nokia\n",
      "last phone\n",
      "samsung s105\n",
      "sound quality\n",
      "important thing\n",
      "sound quality\n",
      "sound quality\n",
      "sound quality\n",
      "samsung phone\n",
      "loud speaker\n",
      "overall design\n",
      "certain menu\n",
      "directional key\n",
      "nice feature\n",
      "horrible battery\n",
      "digital camera\n",
      "useful application\n",
      "big display\n",
      "infrared port\n",
      "nokia tech\n",
      "average phone\n",
      "good chance\n",
      "flip camera\n",
      "next year\n",
      "new account\n",
      "great reception\n",
      "sound quality\n",
      "only thing\n",
      "slick phone\n",
      "first night\n",
      "favorite music\n",
      "doesnt weight\n",
      "ultimate phone\n",
      "comaptible phone\n",
      "specific location\n",
      "great phone\n",
      "mobile phone\n",
      "little phone\n",
      "beatiful screen\n",
      "big seller\n",
      "big one\n",
      "right realation\n",
      "nonflip phone\n",
      "sound quality\n",
      "distorted sound\n",
      "samsung x105\n",
      "great phone\n",
      "light phone\n",
      "long battery\n",
      "new polymer\n",
      "hands-free speakerphone\n",
      "powerful headset\n",
      "public transport\n",
      "infrared port\n",
      "small phone\n",
      "mobile text\n",
      "built-in camera\n",
      "digital camera\n",
      "mobile phone\n",
      "perfect size\n",
      "high-tech phone\n",
      "nokia style\n",
      "delicate appearance\n",
      "little guy\n",
      "optional headset\n",
      "1-touch dialing\n",
      "decent size\n",
      "on/off key\n",
      "side-mounted volume\n",
      "significant force\n",
      "small phone\n",
      "other issue\n",
      "current customer\n",
      "little money\n",
      "old motorola\n",
      "right phone\n",
      "great battery\n",
      "excellent signal\n",
      "free speakerphone\n",
      "favorite function\n",
      "downloadable java\n",
      "tri-band gsm850\n",
      "great phone\n",
      "small size\n",
      "corporate email\n",
      "corporate email\n",
      "corporate network\n",
      "corporate email\n",
      "unlimited internet\n",
      "great phone\n",
      "excellent phone\n",
      "excellent phone\n",
      "great phone\n",
      "excellent network\n",
      "bulky phone\n",
      "coveted item\n",
      "high quality\n",
      "great deal\n",
      "great phone\n",
      "sound quality\n",
      "other end\n",
      "sound quality\n",
      "classic design\n",
      "cheap feel\n",
      "small size\n",
      "unlimited internet\n",
      "56k speed\n",
      "only problem\n",
      "real estate\n",
      "great amount\n",
      "free phone\n",
      "only gripe\n",
      "only thing\n",
      "great screen\n",
      "great sound\n",
      "great reception\n",
      "t-mobile coverage\n",
      "free-after-rebates phone\n",
      "only gripe\n",
      "key arrangement\n",
      "perfect phone\n",
      "new phone\n",
      "great phone\n",
      "comfortable handsfree\n",
      "previous phone\n",
      "first thing\n",
      "several color\n",
      "annoying series\n",
      "massive phonebook\n",
      "multiple phone\n",
      "high-speed internet\n",
      "compatible phone\n",
      "t-mobile service\n",
      "brief time\n",
      "cheerful representative\n",
      "old carrier\n",
      "great phone\n",
      "huge disappointment\n",
      "t-mobile lack\n",
      "different network\n",
      "local sim\n",
      "local wap\n",
      "bottom line\n",
      "tri-band feature\n",
      "manufacturer-unlocked form\n",
      "huge array\n",
      "last week\n",
      "good resolution\n",
      "light weight\n",
      "good signal\n",
      "main problem\n",
      "sound quality\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "filename = [\n",
    "#             'Apex AD2600 Progressive-scan DVD player',\n",
    "#             'Canon G3',\n",
    "#             'Creative Labs Nomad Jukebox Zen Xtra 40GB',\n",
    "#             'Nikon coolpix 4300',\n",
    "            'Nokia 6610'\n",
    "]\n",
    "\n",
    "conj_DP = ['conj']\n",
    "dep_DP = ['amod', 'prep', 'nsubj', 'csubj', 'xsubj', 'dobj', 'iobj']\n",
    "\n",
    "for file in filename:\n",
    "    print(file)\n",
    "#     run('dataset/bing_liu/' + file + '.txt', file + '.csv', False, 'hasil/')\n",
    "    run('dataset/bing_liu/' + file + '.txt', file + '.csv', True, 'hasil_chunk/')\n",
    "\n",
    "\n",
    "# dep_DP = ['amod', 'prep', 'nsubj', 'csubj', 'xsubj', 'dobj', 'iobj',\n",
    "#           'advmod', 'dep', 'cop', 'mark', 'nsubjpass', 'pobj', 'acomp', 'xcomp', 'csubjpass', 'poss']\n",
    "# for file in filename:\n",
    "#     print(file)\n",
    "#     run('dataset/bing_liu/' + file + '.txt', file + '.csv', False, 'hasil_plus/')\n",
    "#     run('dataset/bing_liu/' + file + '.txt', file + '.csv', True, 'hasil_chunk_plus/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
