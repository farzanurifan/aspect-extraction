{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import pickle\n",
    "import time\n",
    "import shelve\n",
    "\n",
    "import chainer\n",
    "from chainer import cuda\n",
    "from chainer import serializers\n",
    "import chainer.optimizers as O\n",
    "import numpy as np\n",
    "\n",
    "from lda2vec import utils\n",
    "from lda2vec import prepare_topics, print_top_words_per_topic, topic_coherence\n",
    "from lda2vec import LDA2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu_id = int(os.getenv('CUDA_GPU', 0))\n",
    "# cuda.get_device(gpu_id).use()\n",
    "# print(\"Using GPU:\" + str(gpu_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_dir = os.getenv('data_dir', '../data/')\n",
    "fn_vocab = 'vocab.pkl'\n",
    "fn_corpus = 'corpus.pkl'\n",
    "fn_flatnd = 'flattened.npy'\n",
    "fn_docids = 'doc_ids.npy'\n",
    "fn_vectors = 'vectors.npy'\n",
    "vocab = pickle.load(open(fn_vocab, 'rb'))\n",
    "corpus = pickle.load(open(fn_corpus, 'rb'))\n",
    "flattened = np.load(fn_flatnd)\n",
    "doc_ids = np.load(fn_docids)\n",
    "vectors = np.load(fn_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Parameters\n",
    "# Number of documents\n",
    "n_docs = doc_ids.max() + 1\n",
    "# Number of unique words in the vocabulary\n",
    "n_vocab = flattened.max() + 1\n",
    "# 'Strength' of the dircihlet prior; 200.0 seems to work well\n",
    "clambda = 200.0\n",
    "# Number of topics to fit\n",
    "n_topics = int(os.getenv('n_topics', 20))\n",
    "batchsize = 4096\n",
    "# Power for neg sampling\n",
    "power = float(os.getenv('power', 0.75))\n",
    "# Intialize with pretrained word vectors\n",
    "pretrained = bool(int(os.getenv('pretrained', True)))\n",
    "# Sampling temperature\n",
    "temperature = float(os.getenv('temperature', 1.0))\n",
    "# Number of dimensions in a single word vector\n",
    "n_units = int(os.getenv('n_units', 300))\n",
    "# Get the string representation for every compact key\n",
    "words = corpus.word_list(vocab)[:n_vocab]\n",
    "# How many tokens are in each document\n",
    "doc_idx, lengths = np.unique(doc_ids, return_counts=True)\n",
    "doc_lengths = np.zeros(doc_ids.max() + 1, dtype='int32')\n",
    "doc_lengths[doc_idx] = lengths\n",
    "# Count all token frequencies\n",
    "tok_idx, freq = np.unique(flattened, return_counts=True)\n",
    "term_frequency = np.zeros(n_vocab, dtype='int32')\n",
    "term_frequency[tok_idx] = freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key in sorted(locals().keys()):\n",
    "#     val = locals()[key]\n",
    "#     if len(str(val)) < 100 and '<' not in str(val):\n",
    "#         print(key, val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(65, 300)\n",
      "[[0.37454012 0.95071431 0.73199394 ... 0.21582103 0.62289048 0.08534746]\n",
      " [0.05168172 0.53135463 0.54063512 ... 0.17231987 0.19228902 0.04086862]\n",
      " [0.16893506 0.27859034 0.17701048 ... 0.89633582 0.01300192 0.08550853]\n",
      " ...\n",
      " [0.3340789  0.54680203 0.10241312 ... 0.57782151 0.11376172 0.47696789]\n",
      " [0.35622689 0.49103847 0.92186644 ... 0.32599792 0.54188095 0.17021463]\n",
      " [0.02222064 0.33698312 0.8653708  ... 0.91455828 0.41887066 0.65653307]]\n"
     ]
    }
   ],
   "source": [
    "model = LDA2Vec(n_documents=n_docs, n_document_topics=n_topics,\n",
    "                n_units=n_units, n_vocab=n_vocab, counts=term_frequency,\n",
    "                n_samples=20, power=power, temperature=temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if os.path.exists('lda2vec.hdf5'):\n",
    "#     print(\"Reloading from saved\")\n",
    "#     serializers.load_hdf5(\"lda2vec.hdf5\", model)\n",
    "    \n",
    "if pretrained:\n",
    "    model.sampler.W.data[:, :] = vectors[:n_vocab, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.to_gpu()\n",
    "optimizer = O.Adam()\n",
    "optimizer.setup(model)\n",
    "clip = chainer.optimizer.GradientClipping(5.0)\n",
    "optimizer.add_hook(clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "epoch = 0\n",
    "fraction = batchsize * 1.0 / flattened.shape[0]\n",
    "progress = shelve.open('progress.shelve')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words in topic 0 price bagel amazing pizza nice rice atmosphere drink ever thai\n",
      "Top words in topic 1 out_of_vocabulary never staff time out_of_vocabulary ever place portion made bagel\n",
      "Top words in topic 2 price ambience view fish atmosphere nice value dish cheese amazing\n",
      "Top words in topic 3 bagel ambience restaurant never attentive recommend absolutely like nice relaxed\n",
      "Top words in topic 4 friendly well ever atmosphere <SKIP> reasonably out_of_vocabulary portion rice made\n",
      "Top words in topic 5 moderate atmosphere wine even price restaurant drink reasonably cheese flavorful\n",
      "Top words in topic 6 appropriate price fish flavorful rice never ever reasonably fast recommend\n",
      "Top words in topic 7 plentiful fish out_of_vocabulary <SKIP> thai appropriate price fast drink rice\n",
      "Top words in topic 8 salad recommend bagel attentive dish rice meal delicious menu <SKIP>\n",
      "Top words in topic 9 cheese sushi salad price high absolutely best bagel value wait\n",
      "Top words in topic 10 bagel staff casual wait decor menu drink recommend friendly attentive\n",
      "Top words in topic 11 salad out_of_vocabulary special high would going rice made absolutely fresh\n",
      "Top words in topic 12 staff portion thing drink value excellent thai absolutely good plentiful\n",
      "Top words in topic 13 plentiful delicious good <SKIP> great flavorful excellent fresh tasty nice\n",
      "Top words in topic 14 nice atmosphere service great amazing excellent thing meal good flavorful\n",
      "Top words in topic 15 dish meal sushi wine delicious attentive rice pizza salad flavorful\n",
      "Top words in topic 16 restaurant service drink friendly menu staff place food delicious bagel\n",
      "Top words in topic 17 made ever plentiful absolutely never even thing casual service bagel\n",
      "Top words in topic 18 menu ambience special fast atmosphere staff relaxed attentive nice good\n",
      "Top words in topic 19 view decor menu restaurant drink thai wine sushi appropriate service\n",
      "0\n",
      "after partial fitting: 938.78937\n",
      "J:00000 E:00000 L:9.388e+02 P:-3.980e+04 R:4.204e+03\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1):\n",
    "    data = prepare_topics(cuda.to_cpu(model.mixture.weights.W.data).copy(),\n",
    "                          cuda.to_cpu(model.mixture.factors.W.data).copy(),\n",
    "                          cuda.to_cpu(model.sampler.W.data).copy(),\n",
    "                          words)\n",
    "    top_words = print_top_words_per_topic(data)\n",
    "    if j % 100 == 0 and j > 100:\n",
    "        coherence = topic_coherence(top_words)\n",
    "        for j in range(n_topics):\n",
    "            print(j, coherence[(j, 'cv')])\n",
    "        kw = dict(top_words=top_words, coherence=coherence, epoch=epoch)\n",
    "        progress[str(epoch)] = pickle.dumps(kw)\n",
    "    data['doc_lengths'] = doc_lengths\n",
    "    data['term_frequency'] = term_frequency\n",
    "    np.savez('topics.pyldavis', **data)\n",
    "    print(epoch)\n",
    "    for d, f in utils.chunks(batchsize, doc_ids, flattened):\n",
    "        t0 = time.time()\n",
    "        model.cleargrads()\n",
    "        #optimizer.use_cleargrads(use=False)\n",
    "        l = model.fit_partial(d.copy(), f.copy())\n",
    "        print(\"after partial fitting:\", l)\n",
    "        prior = model.prior()\n",
    "        loss = prior * fraction\n",
    "        loss.backward()\n",
    "        optimizer.update()\n",
    "        msg = (\"J:{j:05d} E:{epoch:05d} L:{loss:1.3e} \"\n",
    "               \"P:{prior:1.3e} R:{rate:1.3e}\")\n",
    "        prior.to_cpu()\n",
    "        loss.to_cpu()\n",
    "        t1 = time.time()\n",
    "        dt = t1 - t0\n",
    "        rate = batchsize / dt\n",
    "        logs = dict(loss=float(l), epoch=epoch, j=j,\n",
    "                    prior=float(prior.data), rate=rate)\n",
    "        print(msg.format(**logs))\n",
    "        j += 1\n",
    "    serializers.save_hdf5(\"lda2vec.hdf5\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<SKIP>', 'absolutely', 'amazing', 'ambience', 'appropriate', 'atmosphere', 'attentive', 'bagel', 'best', 'casual', 'cheese', 'decor', 'delicious', 'dish', 'drink', 'even', 'ever', 'excellent', 'fast', 'fish', 'flavorful', 'food', 'fresh', 'friendly', 'going', 'good', 'great', 'high', 'like', 'made', 'meal', 'menu', 'moderate', 'never', 'nice', 'out_of_vocabulary', 'pizza', 'place', 'plentiful', 'portion', 'price', 'reasonably', 'recommend', 'relaxed', 'restaurant', 'rice', 'salad', 'service', 'special', 'staff', 'sushi', 'tasty', 'thai', 'thing', 'time', 'value', 'view', 'wait', 'well', 'wine', 'would']\n",
      "61\n"
     ]
    }
   ],
   "source": [
    "all_topics = []\n",
    "for row in top_words:\n",
    "    for word in row:\n",
    "        all_topics.append(word)\n",
    "print(sorted(list(dict.fromkeys(all_topics))))\n",
    "print(len(sorted(list(dict.fromkeys(all_topics)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>reviewID</th>\n",
       "      <th>sentenceID</th>\n",
       "      <th>review</th>\n",
       "      <th>category</th>\n",
       "      <th>polarity</th>\n",
       "      <th>entity</th>\n",
       "      <th>preprocessed_sentence</th>\n",
       "      <th>type_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>RL#3</td>\n",
       "      <td>RL#3:1</td>\n",
       "      <td>I am not necessarily fanatical about this plac...</td>\n",
       "      <td>VALUE#PRICES</td>\n",
       "      <td>positive</td>\n",
       "      <td>VALUE</td>\n",
       "      <td>i am not necessarily fanatical about this plac...</td>\n",
       "      <td>compound_sentence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>TR#2</td>\n",
       "      <td>TR#2:2</td>\n",
       "      <td>The high prices you're going to pay is for the...</td>\n",
       "      <td>VALUE#PRICES</td>\n",
       "      <td>negative</td>\n",
       "      <td>VALUE</td>\n",
       "      <td>the high prices you 're going to pay is for th...</td>\n",
       "      <td>complex_sentence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>TR#2</td>\n",
       "      <td>TR#2:2</td>\n",
       "      <td>The high prices you're going to pay is for the...</td>\n",
       "      <td>VALUE#PRICES</td>\n",
       "      <td>negative</td>\n",
       "      <td>VALUE</td>\n",
       "      <td>the high prices you 're going to pay is for th...</td>\n",
       "      <td>complex_sentence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>TR#2</td>\n",
       "      <td>TR#2:2</td>\n",
       "      <td>The high prices you're going to pay is for the...</td>\n",
       "      <td>VALUE#PRICES</td>\n",
       "      <td>negative</td>\n",
       "      <td>VALUE</td>\n",
       "      <td>the high prices you 're going to pay is for th...</td>\n",
       "      <td>complex_sentence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>TR#2</td>\n",
       "      <td>TR#2:2</td>\n",
       "      <td>The high prices you're going to pay is for the...</td>\n",
       "      <td>VALUE#PRICES</td>\n",
       "      <td>negative</td>\n",
       "      <td>VALUE</td>\n",
       "      <td>the high prices you 're going to pay is for th...</td>\n",
       "      <td>complex_sentence</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id reviewID sentenceID                                             review  \\\n",
       "0   0     RL#3     RL#3:1  I am not necessarily fanatical about this plac...   \n",
       "1   2     TR#2     TR#2:2  The high prices you're going to pay is for the...   \n",
       "2   3     TR#2     TR#2:2  The high prices you're going to pay is for the...   \n",
       "3   4     TR#2     TR#2:2  The high prices you're going to pay is for the...   \n",
       "4   5     TR#2     TR#2:2  The high prices you're going to pay is for the...   \n",
       "\n",
       "       category  polarity entity  \\\n",
       "0  VALUE#PRICES  positive  VALUE   \n",
       "1  VALUE#PRICES  negative  VALUE   \n",
       "2  VALUE#PRICES  negative  VALUE   \n",
       "3  VALUE#PRICES  negative  VALUE   \n",
       "4  VALUE#PRICES  negative  VALUE   \n",
       "\n",
       "                               preprocessed_sentence      type_sentence  \n",
       "0  i am not necessarily fanatical about this plac...  compound_sentence  \n",
       "1  the high prices you 're going to pay is for th...   complex_sentence  \n",
       "2  the high prices you 're going to pay is for th...   complex_sentence  \n",
       "3  the high prices you 're going to pay is for th...   complex_sentence  \n",
       "4  the high prices you 're going to pay is for th...   complex_sentence  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../res_mul_all.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def aspect_topic(tipe, all_topics):\n",
    "    sf = pd.DataFrame(columns=['id','review','category','term'])\n",
    "    count = 0\n",
    "    index = 0\n",
    "    res = []\n",
    "    for sentence in df['review']:\n",
    "        lowercased = sentence.lower()\n",
    "        term = []\n",
    "        category = []\n",
    "        for cat in df['category'][index].split(','):\n",
    "            splitted = cat.split('#')\n",
    "            if splitted[1] == 'PRICES':\n",
    "                category.append('VALUE')\n",
    "            else:\n",
    "                category.append(splitted[0])\n",
    "        id_name = df['id'][index]\n",
    "        for topic in all_topics:\n",
    "            if topic in lowercased:\n",
    "                term.append(topic)\n",
    "#         print(term)\n",
    "        if len(term) == 0:\n",
    "            print(lowercased)\n",
    "            count += 1\n",
    "        sf = sf.append({'id': id_name, 'review': sentence.strip().lower().replace('  ', ' '), 'category': '|'.join(category), 'term': '|'.join(term)}, ignore_index=True)\n",
    "        index += 1\n",
    "    print(count)\n",
    "    sf.to_csv(\"lda2vec\"+ tipe +\".csv\")\n",
    "    sf.to_excel(\"lda2vec\"+ tipe +\".xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chow fun was dry; pork shu mai was more than usually greasy and had to share a table with loud and rude family. \n",
      "the lava cake dessert was terrible.\n",
      "once you step into cosette, you're miraculously in a small, off-the-beaten path parisian bistro.\n",
      "my wife had the fried shrimp which are huge and loved it.\n",
      "the hostess is rude to the point of being offensive.\n",
      "first went here to enjoy their garden terrace.\n",
      "took my mom for mother's day, and the maitre d' was pretty rude.\n",
      "the tuna and wasabe potatoes are bad.\n",
      "my chow fun and chow see was really bland and oily.\n",
      "ingredients are organic which is a real plus for me.\n",
      "their tuna tartar appetizer is to die for.\n",
      "the characters really make for an enjoyable experience.\n",
      "thius is a must for anyone who loves shabu-shabu.\n",
      "taxan horrible!\n",
      "whether it's the parmesean porcini souffle or the lamb glazed with balsamic vinegar, you will surely be transported to northern italy with one bite.\n",
      "i had their eggs benedict for brunch, which were the worst in my entire life, i tried removing the hollondaise sauce completely that was how failed it was.\n",
      "the seats are uncomfortable if you are sitting against the wall on wooden benches.\n",
      "(the asparagus, truffle oil, parmesan bruschetta is a winner!)\n",
      "and the tom kha soup was pathetic.\n",
      "the back garden sitting area is very pleasant, where you can see their personal herb garden.\n",
      "we had the lobster sandwich and it was fantastic.\n",
      "mizu is home to creative and unique rolls not to found anywhere else.\n",
      "i ordered the smoked salmon and roe appetizer and it was off flavor.\n",
      "the entree was bland and small, dessert was not inspired.\n",
      "their calzones are horrific, bad, vomit-inducing, yuck.\n",
      "unique apppetizers.\n",
      "the turkey burgers are scary!\n",
      "the location is perfect.\n",
      "if you're daring, try the balsamic vinegar over icecream, it's wonderful!\n",
      "terrible, terrible management - deserves to be shut-down.\n",
      "the lamb was tender so full of flavor, the dessert was divine!!\n",
      "dessert is a joke...dont bother\n",
      "cozy romantic atomosphere with only around 15 tables at most.\n",
      "delicate spices, onions, eggs and a kick-ass roti.\n",
      "toons has recently been redone, so it's now a very attractive space.\n",
      "we recently decided to try this location, and to our delight, they have outdoor seating, perfect since i had my yorkie with me.\n",
      "indoor was very cozy and cute.\n",
      "i fell in love with the egg noodles in the beef broth with shrimp dumplings and slices of bbq roast pork.\n",
      "the only problem is that the manager is a complete incompetent.\n",
      "personal pans are the perfect size for those hungry nights.\n",
      "there is a downside if you're ordering in -- the delivery guys have major attitude.\n",
      " perfect location for those traveling in/out of the city by auto or bus\n",
      " the 8th ave location was very convenient and while busy, wasn't packed\n",
      "the location in the heart of manhattan adjacent to the port authority makes this an easy spot to grab a bite to eat\n",
      "location is convienient to businesses, hotels and theaters\n",
      "  boucherie is our new favorite neighborhood spot.\n",
      "46\n"
     ]
    }
   ],
   "source": [
    "aspect_topic(\"-20\",list(dict.fromkeys(all_topics)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
