{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os.path\n",
    "import pickle\n",
    "import time\n",
    "import shelve\n",
    "\n",
    "import chainer\n",
    "from chainer import cuda\n",
    "from chainer import serializers\n",
    "import chainer.optimizers as O\n",
    "import numpy as np\n",
    "\n",
    "from lda2vec import utils\n",
    "from lda2vec import prepare_topics, print_top_words_per_topic, topic_coherence\n",
    "from lda2vec import LDA2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu_id = int(os.getenv('CUDA_GPU', 0))\n",
    "# cuda.get_device(gpu_id).use()\n",
    "# print(\"Using GPU:\" + str(gpu_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_dir = os.getenv('data_dir', '../data/')\n",
    "fn_vocab = 'vocab.pkl'\n",
    "fn_corpus = 'corpus.pkl'\n",
    "fn_flatnd = 'flattened.npy'\n",
    "fn_docids = 'doc_ids.npy'\n",
    "fn_vectors = 'vectors.npy'\n",
    "vocab = pickle.load(open(fn_vocab, 'rb'))\n",
    "corpus = pickle.load(open(fn_corpus, 'rb'))\n",
    "flattened = np.load(fn_flatnd)\n",
    "doc_ids = np.load(fn_docids)\n",
    "vectors = np.load(fn_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Parameters\n",
    "# Number of documents\n",
    "n_docs = doc_ids.max() + 1\n",
    "# Number of unique words in the vocabulary\n",
    "n_vocab = flattened.max() + 1\n",
    "# 'Strength' of the dircihlet prior; 200.0 seems to work well\n",
    "clambda = 200.0\n",
    "# Number of topics to fit\n",
    "n_topics = int(os.getenv('n_topics', 30))\n",
    "batchsize = 4096\n",
    "# Power for neg sampling\n",
    "power = float(os.getenv('power', 0.75))\n",
    "# Intialize with pretrained word vectors\n",
    "pretrained = bool(int(os.getenv('pretrained', True)))\n",
    "# Sampling temperature\n",
    "temperature = float(os.getenv('temperature', 1.0))\n",
    "# Number of dimensions in a single word vector\n",
    "n_units = int(os.getenv('n_units', 300))\n",
    "# Get the string representation for every compact key\n",
    "words = corpus.word_list(vocab)[:n_vocab]\n",
    "# How many tokens are in each document\n",
    "doc_idx, lengths = np.unique(doc_ids, return_counts=True)\n",
    "doc_lengths = np.zeros(doc_ids.max() + 1, dtype='int32')\n",
    "doc_lengths[doc_idx] = lengths\n",
    "# Count all token frequencies\n",
    "tok_idx, freq = np.unique(flattened, return_counts=True)\n",
    "term_frequency = np.zeros(n_vocab, dtype='int32')\n",
    "term_frequency[tok_idx] = freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key in sorted(locals().keys()):\n",
    "#     val = locals()[key]\n",
    "#     if len(str(val)) < 100 and '<' not in str(val):\n",
    "#         print(key, val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39, 300)\n",
      "[[0.37454012 0.95071431 0.73199394 ... 0.21582103 0.62289048 0.08534746]\n",
      " [0.05168172 0.53135463 0.54063512 ... 0.17231987 0.19228902 0.04086862]\n",
      " [0.16893506 0.27859034 0.17701048 ... 0.89633582 0.01300192 0.08550853]\n",
      " ...\n",
      " [0.35121702 0.83724    0.06679708 ... 0.82626915 0.7313451  0.98933329]\n",
      " [0.27254765 0.8749895  0.76662267 ... 0.68798402 0.3228145  0.54548815]\n",
      " [0.14727381 0.4525645  0.88119734 ... 0.92717484 0.90138686 0.08987828]]\n"
     ]
    }
   ],
   "source": [
    "model = LDA2Vec(n_documents=n_docs, n_document_topics=n_topics,\n",
    "                n_units=n_units, n_vocab=n_vocab, counts=term_frequency,\n",
    "                n_samples=20, power=power, temperature=temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if os.path.exists('lda2vec.hdf5'):\n",
    "#     print(\"Reloading from saved\")\n",
    "#     serializers.load_hdf5(\"lda2vec.hdf5\", model)\n",
    "    \n",
    "if pretrained:\n",
    "    model.sampler.W.data[:, :] = vectors[:n_vocab, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.to_gpu()\n",
    "optimizer = O.Adam()\n",
    "optimizer.setup(model)\n",
    "clip = chainer.optimizer.GradientClipping(5.0)\n",
    "optimizer.add_hook(clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "epoch = 0\n",
    "fraction = batchsize * 1.0 / flattened.shape[0]\n",
    "progress = shelve.open('progress.shelve')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words in topic 0 plentiful high service price atmosphere value flavorful fresh food appropriate\n",
      "Top words in topic 1 sushi flavorful plentiful relaxed friendly casual good place like delicious\n",
      "Top words in topic 2 sushi menu great appropriate going restaurant delicious like friendly view\n",
      "Top words in topic 3 good reasonably fresh delicious high nice flavorful relaxed dish sushi\n",
      "Top words in topic 4 casual place atmosphere friendly great plentiful always good menu relaxed\n",
      "Top words in topic 5 nice restaurant menu casual sushi would dish meal absolutely plentiful\n",
      "Top words in topic 6 food relaxed atmosphere restaurant casual fresh high staff <SKIP> good\n",
      "Top words in topic 7 plentiful price sushi view service flavorful food out_of_vocabulary staff best\n",
      "Top words in topic 8 service food price out_of_vocabulary value out_of_vocabulary appropriate place view restaurant\n",
      "Top words in topic 9 fresh price going would amazing staff high great absolutely service\n",
      "Top words in topic 10 fresh food menu value <SKIP> out_of_vocabulary meal always dish restaurant\n",
      "Top words in topic 11 casual plentiful nice sushi staff <SKIP> menu friendly going service\n",
      "Top words in topic 12 excellent great amazing delicious really absolutely flavorful always plentiful menu\n",
      "Top words in topic 13 atmosphere flavorful excellent good always absolutely really would staff out_of_vocabulary\n",
      "Top words in topic 14 food pizza sushi menu place restaurant view <SKIP> price always\n",
      "Top words in topic 15 price plentiful menu best <SKIP> food casual staff going would\n",
      "Top words in topic 16 meal absolutely out_of_vocabulary place dish delicious plentiful pizza sushi flavorful\n",
      "Top words in topic 17 atmosphere casual flavorful relaxed fresh like always would friendly delicious\n",
      "Top words in topic 18 price value absolutely best dish would out_of_vocabulary view going relaxed\n",
      "Top words in topic 19 relaxed appropriate best excellent good plentiful amazing like sushi reasonably\n",
      "Top words in topic 20 appropriate relaxed friendly price casual flavorful nice place menu out_of_vocabulary\n",
      "Top words in topic 21 price restaurant value service menu sushi meal pizza would best\n",
      "Top words in topic 22 amazing excellent good service menu <SKIP> fresh place out_of_vocabulary great\n",
      "Top words in topic 23 reasonably atmosphere relaxed <SKIP> appropriate casual absolutely out_of_vocabulary restaurant really\n",
      "Top words in topic 24 price appropriate flavorful dish value menu service excellent out_of_vocabulary <SKIP>\n",
      "Top words in topic 25 amazing reasonably plentiful out_of_vocabulary like excellent nice <SKIP> really friendly\n",
      "Top words in topic 26 appropriate flavorful value delicious reasonably relaxed price place fresh great\n",
      "Top words in topic 27 appropriate best place reasonably out_of_vocabulary high price fresh dish friendly\n",
      "Top words in topic 28 value staff would friendly atmosphere <SKIP> out_of_vocabulary price food fresh\n",
      "Top words in topic 29 menu flavorful pizza amazing good relaxed great plentiful like excellent\n",
      "0\n",
      "after partial fitting: 427.0766\n",
      "J:00000 E:00000 L:4.271e+02 P:-6.896e+04 R:5.001e+03\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1):\n",
    "    data = prepare_topics(cuda.to_cpu(model.mixture.weights.W.data).copy(),\n",
    "                          cuda.to_cpu(model.mixture.factors.W.data).copy(),\n",
    "                          cuda.to_cpu(model.sampler.W.data).copy(),\n",
    "                          words)\n",
    "    top_words = print_top_words_per_topic(data)\n",
    "    if j % 100 == 0 and j > 100:\n",
    "        coherence = topic_coherence(top_words)\n",
    "        for j in range(n_topics):\n",
    "            print(j, coherence[(j, 'cv')])\n",
    "        kw = dict(top_words=top_words, coherence=coherence, epoch=epoch)\n",
    "        progress[str(epoch)] = pickle.dumps(kw)\n",
    "    data['doc_lengths'] = doc_lengths\n",
    "    data['term_frequency'] = term_frequency\n",
    "    np.savez('topics.pyldavis', **data)\n",
    "    print(epoch)\n",
    "    for d, f in utils.chunks(batchsize, doc_ids, flattened):\n",
    "        t0 = time.time()\n",
    "        model.cleargrads()\n",
    "        #optimizer.use_cleargrads(use=False)\n",
    "        l = model.fit_partial(d.copy(), f.copy())\n",
    "        print(\"after partial fitting:\", l)\n",
    "        prior = model.prior()\n",
    "        loss = prior * fraction\n",
    "        loss.backward()\n",
    "        optimizer.update()\n",
    "        msg = (\"J:{j:05d} E:{epoch:05d} L:{loss:1.3e} \"\n",
    "               \"P:{prior:1.3e} R:{rate:1.3e}\")\n",
    "        prior.to_cpu()\n",
    "        loss.to_cpu()\n",
    "        t1 = time.time()\n",
    "        dt = t1 - t0\n",
    "        rate = batchsize / dt\n",
    "        logs = dict(loss=float(l), epoch=epoch, j=j,\n",
    "                    prior=float(prior.data), rate=rate)\n",
    "        print(msg.format(**logs))\n",
    "        j += 1\n",
    "    serializers.save_hdf5(\"lda2vec.hdf5\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<SKIP>', 'absolutely', 'always', 'amazing', 'appropriate', 'atmosphere', 'best', 'casual', 'delicious', 'dish', 'excellent', 'flavorful', 'food', 'fresh', 'friendly', 'going', 'good', 'great', 'high', 'like', 'meal', 'menu', 'nice', 'out_of_vocabulary', 'pizza', 'place', 'plentiful', 'price', 'really', 'reasonably', 'relaxed', 'restaurant', 'service', 'staff', 'sushi', 'value', 'view', 'would']\n",
      "38\n"
     ]
    }
   ],
   "source": [
    "all_topics = []\n",
    "for row in top_words:\n",
    "    for word in row:\n",
    "        all_topics.append(word)\n",
    "print(sorted(list(dict.fromkeys(all_topics))))\n",
    "print(len(sorted(list(dict.fromkeys(all_topics)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>reviewID</th>\n",
       "      <th>sentenceID</th>\n",
       "      <th>review</th>\n",
       "      <th>category</th>\n",
       "      <th>polarity</th>\n",
       "      <th>entity</th>\n",
       "      <th>preprocessed_sentence</th>\n",
       "      <th>type_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>RL#3</td>\n",
       "      <td>RL#3:1</td>\n",
       "      <td>I am not necessarily fanatical about this plac...</td>\n",
       "      <td>VALUE#PRICES</td>\n",
       "      <td>positive</td>\n",
       "      <td>VALUE</td>\n",
       "      <td>i am not necessarily fanatical about this plac...</td>\n",
       "      <td>compound_sentence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>TR#2</td>\n",
       "      <td>TR#2:2</td>\n",
       "      <td>The high prices you're going to pay is for the...</td>\n",
       "      <td>VALUE#PRICES</td>\n",
       "      <td>negative</td>\n",
       "      <td>VALUE</td>\n",
       "      <td>the high prices you 're going to pay is for th...</td>\n",
       "      <td>complex_sentence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>TR#2</td>\n",
       "      <td>TR#2:2</td>\n",
       "      <td>The high prices you're going to pay is for the...</td>\n",
       "      <td>VALUE#PRICES</td>\n",
       "      <td>negative</td>\n",
       "      <td>VALUE</td>\n",
       "      <td>the high prices you 're going to pay is for th...</td>\n",
       "      <td>complex_sentence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>TR#2</td>\n",
       "      <td>TR#2:2</td>\n",
       "      <td>The high prices you're going to pay is for the...</td>\n",
       "      <td>VALUE#PRICES</td>\n",
       "      <td>negative</td>\n",
       "      <td>VALUE</td>\n",
       "      <td>the high prices you 're going to pay is for th...</td>\n",
       "      <td>complex_sentence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>TR#2</td>\n",
       "      <td>TR#2:2</td>\n",
       "      <td>The high prices you're going to pay is for the...</td>\n",
       "      <td>VALUE#PRICES</td>\n",
       "      <td>negative</td>\n",
       "      <td>VALUE</td>\n",
       "      <td>the high prices you 're going to pay is for th...</td>\n",
       "      <td>complex_sentence</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id reviewID sentenceID                                             review  \\\n",
       "0   0     RL#3     RL#3:1  I am not necessarily fanatical about this plac...   \n",
       "1   2     TR#2     TR#2:2  The high prices you're going to pay is for the...   \n",
       "2   3     TR#2     TR#2:2  The high prices you're going to pay is for the...   \n",
       "3   4     TR#2     TR#2:2  The high prices you're going to pay is for the...   \n",
       "4   5     TR#2     TR#2:2  The high prices you're going to pay is for the...   \n",
       "\n",
       "       category  polarity entity  \\\n",
       "0  VALUE#PRICES  positive  VALUE   \n",
       "1  VALUE#PRICES  negative  VALUE   \n",
       "2  VALUE#PRICES  negative  VALUE   \n",
       "3  VALUE#PRICES  negative  VALUE   \n",
       "4  VALUE#PRICES  negative  VALUE   \n",
       "\n",
       "                               preprocessed_sentence      type_sentence  \n",
       "0  i am not necessarily fanatical about this plac...  compound_sentence  \n",
       "1  the high prices you 're going to pay is for th...   complex_sentence  \n",
       "2  the high prices you 're going to pay is for th...   complex_sentence  \n",
       "3  the high prices you 're going to pay is for th...   complex_sentence  \n",
       "4  the high prices you 're going to pay is for th...   complex_sentence  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../res_mul_all.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def aspect_topic(tipe, all_topics):\n",
    "    sf = pd.DataFrame(columns=['id','review','category','term'])\n",
    "    count = 0\n",
    "    index = 0\n",
    "    res = []\n",
    "    for sentence in df['review']:\n",
    "        lowercased = sentence.lower()\n",
    "        term = []\n",
    "        category = []\n",
    "        for cat in df['category'][index].split(','):\n",
    "            splitted = cat.split('#')\n",
    "            if splitted[1] == 'PRICES':\n",
    "                category.append('VALUE')\n",
    "            else:\n",
    "                category.append(splitted[0])\n",
    "        id_name = df['id'][index]\n",
    "        for topic in all_topics:\n",
    "            if topic in lowercased:\n",
    "                term.append(topic)\n",
    "#         print(term)\n",
    "        if len(term) == 0:\n",
    "            print(lowercased)\n",
    "            count += 1\n",
    "        sf = sf.append({'id': id_name, 'review': sentence.strip().lower().replace('  ', ' '), 'category': '|'.join(category), 'term': '|'.join(term)}, ignore_index=True)\n",
    "        index += 1\n",
    "    print(count)\n",
    "    sf.to_csv(\"lda2vec\"+ tipe +\".csv\")\n",
    "    sf.to_excel(\"lda2vec\"+ tipe +\".xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chow fun was dry; pork shu mai was more than usually greasy and had to share a table with loud and rude family. \n",
      "i had the duck breast special on my last visit and it was not incredible.\n",
      "the only thing i moderately enjoyed was their grilled chicken special with edamame puree.\n",
      "i had never had edamame pureed before but i thought it was innovative and tasty (could've used a bit more salt).\n",
      "the lava cake dessert was terrible.\n",
      "once you step into cosette, you're miraculously in a small, off-the-beaten path parisian bistro.\n",
      "my wife had the fried shrimp which are huge and loved it.\n",
      "the hostess is rude to the point of being offensive.\n",
      "there was a small wait, but shorter than i expected.\n",
      "first went here to enjoy their garden terrace.\n",
      "took my mom for mother's day, and the maitre d' was pretty rude.\n",
      "i loved everythig about it-especially the shows and actors.\n",
      "the tuna and wasabe potatoes are bad.\n",
      "the bagel was small.\n",
      "salads were bad.\n",
      "ingredients are organic which is a real plus for me.\n",
      "we even had a visit from the manager who wanted to make sure we were enjoying ourselves.\n",
      "fish was overdone.\n",
      "someone else recommended the dessert - we also left that.\n",
      "their tuna tartar appetizer is to die for.\n",
      "we ordered the special, grilled branzino, that was so infused with bone, it was difficult to eat.\n",
      "delivery is fast too.\n",
      "thius is a must for anyone who loves shabu-shabu.\n",
      "taxan horrible!\n",
      "i had the worst ravioli ever.\n",
      "whether it's the parmesean porcini souffle or the lamb glazed with balsamic vinegar, you will surely be transported to northern italy with one bite.\n",
      "the hostess and the waitress were incredibly rude and did everything they could to rush us out.\n",
      "i had their eggs benedict for brunch, which were the worst in my entire life, i tried removing the hollondaise sauce completely that was how failed it was.\n",
      "the seats are uncomfortable if you are sitting against the wall on wooden benches.\n",
      "truly the mark of an attentive waiter.\n",
      "i recommend the jelly fish, drunken chicken and the soupy dumplings, certainly the stir fry blue crab.\n",
      "(the asparagus, truffle oil, parmesan bruschetta is a winner!)\n",
      "after dinner the manager grabbed my boyfriend, asked him: where are you from...maybe you dont know how things work in america...and in the end stormed away almost teareyed yelling that tips are the only thing they survive on.\n",
      "we did tip, i guess the model/waitress just wanted more and complained to the manager.\n",
      "decor is charming.\n",
      "we ate out in the back patio, which is worth it as it's cool and the music is hear well there.\n",
      "and the tom kha soup was pathetic.\n",
      "the back garden sitting area is very pleasant, where you can see their personal herb garden.\n",
      "we had the lobster sandwich and it was fantastic.\n",
      "mizu is home to creative and unique rolls not to found anywhere else.\n",
      "i ordered the smoked salmon and roe appetizer and it was off flavor.\n",
      "the entree was bland and small, dessert was not inspired.\n",
      "their calzones are horrific, bad, vomit-inducing, yuck.\n",
      "the dosas are skimpy, unattractive and drip with grease, and personally i'd drink popcorn topping before i'd eat another one of these.\n",
      "unique apppetizers.\n",
      "the cream cheeses are out of this world and i love that coffee!!\n",
      "the turkey burgers are scary!\n",
      "the rice was poor quality and was cooked so badly it was hard.\n",
      "the fish was adequate, but inexpertly sliced.\n",
      "the location is perfect.\n",
      "i love their thai\n",
      "if you're daring, try the balsamic vinegar over icecream, it's wonderful!\n",
      "terrible, terrible management - deserves to be shut-down.\n",
      "the lamb was tender so full of flavor, the dessert was divine!!\n",
      "the waiter was attentive.\n",
      "dessert is a joke...dont bother\n",
      "cozy romantic atomosphere with only around 15 tables at most.\n",
      "i can't wait for summer, when they serve outside on their gigantic patio.\n",
      "the mussles were the fishiest things i've ever tasted, the seabass was bland, the goat cheese salad was missing the goat cheese, the penne w/ chicken had bones in it... it was disgusting.\n",
      "delicate spices, onions, eggs and a kick-ass roti.\n",
      "toons has recently been redone, so it's now a very attractive space.\n",
      "we recently decided to try this location, and to our delight, they have outdoor seating, perfect since i had my yorkie with me.\n",
      "indoor was very cozy and cute.\n",
      "the decor is very simple but comfortable.\n",
      "i fell in love with the egg noodles in the beef broth with shrimp dumplings and slices of bbq roast pork.\n",
      "yakitori (bbq meats) is tasty too.\n",
      "we were seated outside and the waiter spilled red wine and hot tea on myself and my date.\n",
      "the only problem is that the manager is a complete incompetent.\n",
      "personal pans are the perfect size for those hungry nights.\n",
      "there is a downside if you're ordering in -- the delivery guys have major attitude.\n",
      " perfect location for those traveling in/out of the city by auto or bus\n",
      " the 8th ave location was very convenient and while busy, wasn't packed\n",
      "the location in the heart of manhattan adjacent to the port authority makes this an easy spot to grab a bite to eat\n",
      "location is convienient to businesses, hotels and theaters\n",
      "  boucherie is our new favorite neighborhood spot.\n",
      "75\n"
     ]
    }
   ],
   "source": [
    "aspect_topic(\"-20\",list(dict.fromkeys(all_topics)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
