{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Farza\n",
      "[nltk_data]     Nurifan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Farza Nurifan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "Warming up PyWSD (takes ~10 secs)... took 5.071583271026611 secs.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "import math\n",
    "from math import exp, expm1, log, log10\n",
    "import numpy as np\n",
    "import turtle\n",
    "import pandas as pd\n",
    "from nltk.wsd import lesk\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "from pywsd.lesk import adapted_lesk\n",
    "from nltk.corpus import stopwords\n",
    "stopWords = set(stopwords.words('english'))\n",
    "import sys, os\n",
    "\n",
    "\n",
    "nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "dependency_parser = nlp.annotate\n",
    "positive_lexicon = []\n",
    "negative_lexicon = []\n",
    "\n",
    "def read_lexicon():\n",
    "    global positive_lexicon;\n",
    "    global negative_lexicon;\n",
    "    \n",
    "    with open(os.path.join(os.path.abspath('../opinion-lexicon-English/') , 'positive-words.txt'), 'r') as file:\n",
    "        line = file.readline();\n",
    "        while \";\" in line:\n",
    "            line = file.readline();\n",
    "         \n",
    "        positive_lexicon = file.readlines()\n",
    "    \n",
    "    with open(os.path.join(os.path.abspath('../opinion-lexicon-English/') , 'negative-words.txt'), 'r', encoding = \"ISO-8859-1\") as file:\n",
    "        line = file.readline();\n",
    "        while \";\" in line:\n",
    "            line = file.readline();\n",
    "        \n",
    "        negative_lexicon = file.readlines()\n",
    "        \n",
    "    positive_lexicon = list(map(lambda word: word.rstrip(\"\\n\\r\"), positive_lexicon))\n",
    "    negative_lexicon = list(map(lambda word: word.rstrip(\"\\n\\r\"), negative_lexicon))\n",
    "    \n",
    "        \n",
    "read_lexicon()\n",
    "op_set = positive_lexicon + negative_lexicon\n",
    "\n",
    "negation = [\n",
    "    \"afraid\",\n",
    "    \"can't\",\n",
    "    \"cannot\",\n",
    "    \"deny\",\n",
    "    \"mean\",\n",
    "    \"negate\",\n",
    "    \"negation\",\n",
    "    \"negative\",\n",
    "    \"neither\",\n",
    "    \"never\",\n",
    "    \"no\",\n",
    "    \"non\",\n",
    "    \"none\",\n",
    "    \"nor\",\n",
    "    \"not\",\n",
    "    \"nothing\",\n",
    "    \"refusal\",\n",
    "    \"refuse\",\n",
    "    \"reject\",\n",
    "    \"rejection\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def geometric_medianX(points, method='auto', options={}):\n",
    "    \"\"\"\n",
    "    Calculates the geometric median of an array of points.\n",
    "    method specifies which algorithm to use:\n",
    "        * 'auto' -- uses a heuristic to pick an algorithm\n",
    "        * 'minimize' -- scipy.optimize the sum of distances\n",
    "        * 'weiszfeld' -- Weiszfeld's algorithm\n",
    "    \"\"\"\n",
    "\n",
    "    points = np.asarray(points)\n",
    "\n",
    "    if len(points.shape) == 1:\n",
    "        # geometric_median((0, 0)) has too much potential for error.\n",
    "        # Did the user intend a single 2D point or two scalars?\n",
    "        # Use np.median if you meant the latter.\n",
    "        raise ValueError(\"Expected 2D array\")\n",
    "\n",
    "    if method == 'auto':\n",
    "        if points.shape[1] > 2:\n",
    "            # weiszfeld tends to converge faster in higher dimensions\n",
    "            method = 'weiszfeld'\n",
    "        else:\n",
    "            method = 'minimize'\n",
    "\n",
    "    return _methods[method](points, options)\n",
    "\n",
    "\n",
    "def minimize_method(points, options={}):\n",
    "    \"\"\"\n",
    "    Geometric median as a convex optimization problem.\n",
    "    \"\"\"\n",
    "\n",
    "    # objective function\n",
    "    def aggregate_distance(x):\n",
    "        return cdist([x], points).sum()\n",
    "\n",
    "    # initial guess: centroid\n",
    "    centroid = points.mean(axis=0)\n",
    "\n",
    "    optimize_result = minimize(aggregate_distance, centroid, method='COBYLA')\n",
    "\n",
    "    return optimize_result.x\n",
    "\n",
    "\n",
    "def weiszfeld_method(points, options={}):\n",
    "    \"\"\"\n",
    "    Weiszfeld's algorithm as described on Wikipedia.\n",
    "    \"\"\"\n",
    "\n",
    "    default_options = {'maxiter': 1000, 'tol': 1e-7}\n",
    "    default_options.update(options)\n",
    "    options = default_options\n",
    "\n",
    "    def distance_func(x):\n",
    "        return cdist([x], points)\n",
    "\n",
    "    # initial guess: centroid\n",
    "    guess = points.mean(axis=0)\n",
    "\n",
    "    iters = 0\n",
    "\n",
    "    while iters < options['maxiter']:\n",
    "        distances = distance_func(guess).T\n",
    "\n",
    "        # catch divide by zero\n",
    "        # TODO: Wikipedia cites how to deal with distance 0\n",
    "        distances = np.where(distances == 0, 1, distances)\n",
    "\n",
    "        guess_next = (points/distances).sum(axis=0) / (1./distances).sum(axis=0)\n",
    "\n",
    "        guess_movement = np.sqrt(((guess - guess_next)**2).sum())\n",
    "\n",
    "        guess = guess_next\n",
    "\n",
    "        if guess_movement <= options['tol']:\n",
    "            break\n",
    "\n",
    "        iters += 1\n",
    "\n",
    "    return guess\n",
    "\n",
    "\n",
    "_methods = {\n",
    "    'minimize': minimize_method,\n",
    "    'weiszfeld': weiszfeld_method,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph(radii, theta, key):\n",
    "#     fig = plt.figure()\n",
    "#     ax = fig.add_subplot(111)\n",
    "#     plt.plot([0,0],[-2,2])\n",
    "#     plt.plot([-2,2],[0,0])\n",
    "#     plt.axis([-1,1,-1,1])\n",
    "    i=0\n",
    "    a=[]\n",
    "    b=[]\n",
    "    rad=list(radii.values())\n",
    "    theta1=list(theta.values())\n",
    "\n",
    "    while(i<len(radii)):\n",
    "        a.append(rad[i]*math.cos(theta1[i]))\n",
    "        b.append(rad[i]*math.sin(theta1[i]))\n",
    "        i+=1 \n",
    "        \n",
    "#     plt.scatter(a,b,label=\"circles\",color=\"r\",marker=\"o\",s=10)\n",
    "#     q = 0\n",
    "#     rad2=list(radii.keys())\n",
    "#     for i,j in zip(a,b):\n",
    "#         ax.annotate('%s' %rad2[q], xy=(i,j), xytext=(15,0), textcoords='offset points')\n",
    "#         q = q+1    \n",
    "        \n",
    "        \n",
    "#     points = []\n",
    "#     for x,y in zip(a,b):\n",
    "#         points.append([x, y])\n",
    "\n",
    "#     if(len(points) == 0):\n",
    "#         points.append([0, 0])\n",
    "\n",
    "#     senti_x, senti_y = geometric_medianX(points)\n",
    "#     plt.scatter(senti_x,senti_y,label=\"circles\",color=\"b\",marker=\"o\",s=10)\n",
    "#     ax.annotate('%s' % '', xy=(senti_x,senti_y), xytext=(15,0), textcoords='offset points')\n",
    "    \n",
    "    \n",
    "    \n",
    "#     ax.add_artist(plt.Circle((0,0),1.0,color='b',fill=False))\n",
    "#     plt.xlabel('Sentiment Strength')\n",
    "#     plt.ylabel('Orientation')\n",
    "#     plt.title(key)\n",
    "#     plt.savefig(\"graph.png\")\n",
    "    \n",
    "    return a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_file(key, lines):    \n",
    "    radii = get_TDOC(lines, key)    \n",
    "    return radii\n",
    "\n",
    "def get_TDOC(lines, key):\n",
    "    freq = {'Init': 0}              #Number of times context term occurs with key\n",
    "    freq.clear()\n",
    "    prohib = [''] #stopWords\n",
    "    for line in lines:\n",
    "        words = line.split(\" \")\n",
    "        if key in words:\n",
    "            for context in words:\n",
    "                flag=0\n",
    "                for i in prohib:\n",
    "                    if i == context:\n",
    "                        flag=1\n",
    "                        break\n",
    "                if flag==0 and context!=key and context in op_set:\n",
    "                    freq.setdefault(context, 0)\n",
    "                    freq[context] = freq.get(context) + 1\n",
    "                                           \n",
    "    N = 0                           #Total Number of terms in Document\n",
    "    for line in lines:\n",
    "        words = line.split(\" \")\n",
    "        N += len(words)\n",
    "\n",
    "    Nci = {'Init': 0}               #Total terms that occur with context term\n",
    "    Nci.clear()\n",
    "    for context in freq.keys():\n",
    "        for line in lines:\n",
    "            words = line.split(\" \")\n",
    "            if context in words:\n",
    "                Nci.setdefault(context, 0)\n",
    "                Nci[context] += len(words)\n",
    "\n",
    "    radii = {'Init': 0}             #Get Radius of context term with TDOC formula\n",
    "    radii.clear()\n",
    "    \n",
    "    df = pd.DataFrame(columns=['c', 'm', 'N', 'Nc', 'f', 'N/Nc', 'log(N/Nc)', 'fxlog(N/Nc)', '/4'])\n",
    "    max_value = 0\n",
    "    for term in freq.keys():\n",
    "        radii[term] = (freq[term]*(log(N/Nci[term])))\n",
    "        \n",
    "        if radii[term] > max_value:\n",
    "            max_value = radii[term]\n",
    "        \n",
    "    for term in freq.keys():\n",
    "        radii[term] = radii[term]/max_value\n",
    "        \n",
    "        df = df.append({'c': term,\n",
    "                'm': key,\n",
    "                'N': N,\n",
    "                'Nc': Nci[term],\n",
    "                'f': freq[term],\n",
    "                'N/Nc': \"{0:.2f}\".format(N/Nci[term]),\n",
    "                'log(N/Nc)': \"{0:.2f}\".format(log(N/Nci[term])),\n",
    "                'fxlog(N/Nc)': \"{0:.2f}\".format(freq[term]*(log(N/Nci[term]))),\n",
    "                'normalisasi': \"{0:.2f}\".format((freq[term]*(log(N/Nci[term])))/max_value)\n",
    "               }, ignore_index=True)\n",
    "    \n",
    "#     df.to_excel(\"tdoc.xlsx\")\n",
    "    return radii                    #Returns entire set of context terms related to key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_theta(key, sentences):\n",
    "    scores = []\n",
    "    for sentence in sentences:\n",
    "        flag = True\n",
    "        endscore = 0\n",
    "#         if key in positive_lexicon:\n",
    "#             endscore += 0.75\n",
    "#         elif key in negative_lexicon:\n",
    "#             endscore -= 0.75\n",
    "            \n",
    "#         if key == 'cramped':\n",
    "#             endscore -= 0.2\n",
    "#         elif key == 'unappealing':\n",
    "#             endscore += 0.1\n",
    "#         else:\n",
    "#             endscore -= 0.2\n",
    "        try:\n",
    "            score = swn.senti_synset(key + '.a.1')\n",
    "#             print(key)\n",
    "            if score.pos_score() > score.neg_score():\n",
    "                endscore += score.pos_score()\n",
    "            else:\n",
    "                endscore += score.neg_score() * (-1)\n",
    "        except:\n",
    "            print(key)\n",
    "            \n",
    "        words = sentence.split(' ')\n",
    "        word_around = []\n",
    "        for x in range(0, len(words)):\n",
    "            try:\n",
    "                if (words[x+1] == key) or (words[x+2] == key) or (words[x+3]== key) or (words[x+4]== key):\n",
    "                    word_around.append(words[x])\n",
    "                elif (words[x-1] == key):\n",
    "                    word_around.append(words[x])\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        for neg in negation:\n",
    "            if neg in sentence:\n",
    "#                 endscore *= (-1)\n",
    "                break\n",
    "\n",
    "        scores.append(endscore)\n",
    "            \n",
    "    final_score = np.average(scores)\n",
    "    return np.pi * final_score\n",
    "\n",
    "def prior_sentiment(radii, key, all_sentences):\n",
    "    theta = {'Init': 0}\n",
    "    theta.clear()\n",
    "    for word in radii.keys():\n",
    "        sentences = []\n",
    "        for sentence in all_sentences:\n",
    "            words = sentence.split(' ')\n",
    "            if (word in words) and (key in words):\n",
    "                sentences.append(sentence)\n",
    "                \n",
    "        filter = get_theta(word, sentences)            #if function returns 0 word does not exist in lexicon\n",
    "        theta[word] = filter\n",
    "        \n",
    "    return theta\n",
    "\n",
    "def senti(key, lines):\n",
    "    radii = analyse_file(key, lines)\n",
    "    theta = prior_sentiment(radii, key, lines)\n",
    "    a, b = graph(radii, theta, key)\n",
    "    points = []\n",
    "    for x,y in zip(a,b):\n",
    "        points.append([x, y])\n",
    "\n",
    "    if(len(points) == 0):\n",
    "        points.append([0, 0])\n",
    "    senti_x, senti_y = geometric_medianX(points)\n",
    "#     print(theta)\n",
    "    return senti_x, senti_y\n",
    "#     return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def rem_punct(word):\n",
    "    return word.replace('  ', ' ').strip().lower().translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def run(tipe):\n",
    "    df = pd.read_csv('Results/'+ tipe +'.csv')\n",
    "    sf = pd.DataFrame(columns = ['opini','sentimen'])\n",
    "    \n",
    "    data = []\n",
    "    for sent in df['review']:\n",
    "        data.append(rem_punct(sent))\n",
    "\n",
    "    opinions_all = []\n",
    "    for opinions in df['opinion']:\n",
    "        if type(opinions) != float:\n",
    "            opinion = opinions.split('|')\n",
    "            for o in opinion:\n",
    "                if o == '-':\n",
    "                    continue\n",
    "                opinions_all.append(o)\n",
    "    opinions_sorted = sorted(list(dict.fromkeys(opinions_all)))\n",
    "\n",
    "    for opinion in opinions_sorted:\n",
    "        a, b = senti(opinion, data)\n",
    "        sentimen = ''\n",
    "        if b > 0:\n",
    "            sentimen = 'positive'\n",
    "        else:\n",
    "            sentimen = 'negative'\n",
    "#         print(sentimen, '\\t', opinion)\n",
    "        sf = sf.append({'opini': opinion,\n",
    "                        'sentimen': sentimen\n",
    "                       }, ignore_index=True)\n",
    "    sf.to_csv('Results/Opinion/'+ tipe + '-swn.csv')\n",
    "    sf.to_excel('Results/Opinion/'+ tipe + '-swn.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fairly\n",
      "unusually\n",
      "delight\n",
      "enjoyed\n",
      "outrageously\n",
      "bargain\n",
      "fun\n",
      "perfection\n",
      "reasonably\n",
      "haze\n",
      "reasonably\n",
      "fairly\n",
      "allergies\n",
      "fun\n",
      "reasonably\n",
      "reasonably\n",
      "enjoy\n",
      "consistently\n",
      "outrageously\n",
      "allergies\n",
      "enjoy\n",
      "consistently\n",
      "outrageously\n",
      "unusually\n",
      "gem\n",
      "fun\n",
      "delight\n",
      "bargain\n",
      "fun\n",
      "reasonably\n",
      "recommend\n",
      "unusually\n",
      "allergies\n",
      "recommend\n",
      "gem\n",
      "allergies\n",
      "fairly\n",
      "fun\n",
      "fairly\n",
      "allergies\n",
      "fairly\n",
      "fun\n",
      "enjoyed\n",
      "gem\n",
      "allergies\n",
      "fun\n",
      "perfection\n",
      "recommend\n",
      "unusually\n",
      "fun\n",
      "unusually\n",
      "gem\n",
      "fairly\n",
      "recommend\n",
      "delightfully\n",
      "gem\n",
      "love\n",
      "vent\n",
      "kills\n",
      "gem\n",
      "fun\n",
      "gem\n",
      "attraction\n",
      "hang\n",
      "fun\n",
      "recommend\n",
      "hate\n",
      "enjoy\n",
      "recommend\n",
      "vent\n",
      "love\n",
      "complain\n",
      "delight\n",
      "love\n",
      "dearth\n",
      "recommend\n",
      "love\n",
      "delight\n",
      "miraculously\n",
      "recommend\n",
      "fun\n",
      "gimmick\n",
      "dearth\n",
      "gem\n",
      "warmer\n",
      "absurdly\n",
      "work\n",
      "recommend\n",
      "enjoy\n",
      "fairly\n",
      "fun\n",
      "exceeded\n",
      "absurdly\n",
      "work\n",
      "enjoy\n",
      "absurdly\n",
      "work\n",
      "enjoy\n",
      "delight\n",
      "bargain\n",
      "fun\n",
      "love\n",
      "recommend\n",
      "love\n",
      "fairly\n",
      "absurdly\n",
      "work\n",
      "fairly\n",
      "absurdly\n",
      "work\n",
      "exceeded\n",
      "delight\n",
      "bargain\n",
      "fun\n",
      "incredibly\n",
      "intimidate\n",
      "absurdly\n",
      "work\n",
      "perfection\n",
      "smile\n",
      "perfection\n",
      "haze\n",
      "absurdly\n",
      "work\n",
      "smile\n",
      "enjoy\n",
      "enjoy\n",
      "absurdly\n",
      "work\n",
      "died\n",
      "incredibly\n",
      "recommend\n",
      "intimidate\n",
      "enjoy\n",
      "recommend\n",
      "promptly\n",
      "absurdly\n",
      "work\n",
      "fun\n",
      "charm\n",
      "incredibly\n",
      "dissappointed\n",
      "absurdly\n",
      "work\n",
      "perfection\n",
      "died\n",
      "recommend\n",
      "charm\n",
      "dissappointed\n",
      "dearth\n",
      "dissappointed\n",
      "dearth\n"
     ]
    }
   ],
   "source": [
    "run('FOOD')\n",
    "run('AMBIENCE')\n",
    "run('PRICES')\n",
    "run('SERVICE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Results/'+ 'FOOD' +'.csv')\n",
    "\n",
    "data = []\n",
    "for sent in df['review']:\n",
    "    data.append(rem_punct(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expensive\n",
      "great\n",
      "ridiculous\n"
     ]
    }
   ],
   "source": [
    "a, b = senti('best', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.003979001408701357"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = swn.senti_synset('good.a.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score.pos_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score.neg_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['quicker',\n",
       " 'insightfully',\n",
       " 'lucid',\n",
       " 'enviousness',\n",
       " 'sensibly',\n",
       " 'enchant',\n",
       " 'breathtaking',\n",
       " 'joyous',\n",
       " 'unabashed',\n",
       " 'backbone',\n",
       " 'upbeat',\n",
       " 'cushy',\n",
       " 'lustrous',\n",
       " 'beutifully',\n",
       " 'brilliances',\n",
       " 'energy-saving',\n",
       " 'avidly',\n",
       " 'complement',\n",
       " 'reasonably',\n",
       " 'luckier',\n",
       " 'jubilation',\n",
       " 'elevate',\n",
       " 'comely',\n",
       " 'attune',\n",
       " 'hardier',\n",
       " 'eloquently',\n",
       " 'exciting',\n",
       " 'innovative',\n",
       " 'instrumental',\n",
       " 'earnest',\n",
       " 'impartial',\n",
       " 'ameliorate',\n",
       " 'manageable',\n",
       " 'truthfully',\n",
       " 'trendy',\n",
       " 'cohesive',\n",
       " 'ingenuity',\n",
       " 'fruitful',\n",
       " 'tingle',\n",
       " 'freshest',\n",
       " 'adaptable',\n",
       " 'cleverly',\n",
       " 'dashing',\n",
       " 'mastery',\n",
       " 'uplifting',\n",
       " 'sensibly',\n",
       " 'winning',\n",
       " 'refunded',\n",
       " 'blockbuster',\n",
       " 'deserving',\n",
       " 'heartily',\n",
       " 'succeed',\n",
       " 'fortune',\n",
       " 'worthy',\n",
       " 'remarkably',\n",
       " 'razor-sharp',\n",
       " 'autonomous',\n",
       " 'breathtakingly',\n",
       " 'adorable',\n",
       " 'unassailable',\n",
       " 'prestige',\n",
       " 'spiritual',\n",
       " 'acumen',\n",
       " 'warmer',\n",
       " 'supreme',\n",
       " 'large-capacity',\n",
       " 'gleefully',\n",
       " 'overtaken',\n",
       " 'encourage',\n",
       " 'great',\n",
       " 'earnestness',\n",
       " 'tingle',\n",
       " 'genial',\n",
       " 'polite',\n",
       " 'unparalleled',\n",
       " 'adjustable',\n",
       " 'homage',\n",
       " 'interesting',\n",
       " 'solid',\n",
       " 'stable',\n",
       " 'propitious',\n",
       " 'goodly',\n",
       " 'rewardingly',\n",
       " 'unfazed',\n",
       " 'neat',\n",
       " 'staunch',\n",
       " 'well-made',\n",
       " 'admirably',\n",
       " 'thumb-up',\n",
       " 'gems',\n",
       " 'complimentary',\n",
       " 'celebration',\n",
       " 'wonderous',\n",
       " 'luckiness',\n",
       " 'savings',\n",
       " 'boundless',\n",
       " 'standout',\n",
       " 'edify',\n",
       " 'ultra-crisp',\n",
       " 'awed']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "positive = []\n",
    "for i in range(0, 100):\n",
    "    positive.append(positive_lexicon[random.randint(0,len(positive_lexicon))])\n",
    "positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
