{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import re\n",
    "import pandas as pd\n",
    "import itertools, nltk, string \n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "nltk_words = set(nltk.corpus.stopwords.words('english'))\n",
    "stop_words = []\n",
    "for word in nltk_words:\n",
    "    stop_words.append(word.translate(str.maketrans('', '', string.punctuation)))\n",
    "\n",
    "def preprocess(sentence):\n",
    "    res = sentence.lower()\n",
    "    res = res.translate(str.maketrans('', '', string.punctuation))\n",
    "    tokenized_words = nltk.word_tokenize(res)\n",
    "    res = [word for word in tokenized_words if word not in stop_words]\n",
    "    res = [lemmatizer.lemmatize(r) for r in res]\n",
    "    res = [re.sub(r\"[^A-Za-z]+\", '', r) for r in res]\n",
    "    return res\n",
    "\n",
    "positive_lexicon = []\n",
    "negative_lexicon = []\n",
    "\n",
    "def read_lexicon():\n",
    "    global positive_lexicon;\n",
    "    global negative_lexicon;\n",
    "    \n",
    "    with open(os.path.join(os.path.abspath('opinion-lexicon-English/') , 'positive-words.txt'), 'r') as file:\n",
    "        line = file.readline();\n",
    "        while \";\" in line:\n",
    "            line = file.readline();\n",
    "         \n",
    "        positive_lexicon = file.readlines()\n",
    "    \n",
    "    with open(os.path.join(os.path.abspath('opinion-lexicon-English/') , 'negative-words.txt'), 'r', encoding = \"ISO-8859-1\") as file:\n",
    "        line = file.readline();\n",
    "        while \";\" in line:\n",
    "            line = file.readline();\n",
    "        \n",
    "        negative_lexicon = file.readlines()\n",
    "        \n",
    "    positive_lexicon = list(map(lambda word: word.rstrip(\"\\n\\r\"), positive_lexicon))\n",
    "    negative_lexicon = list(map(lambda word: word.rstrip(\"\\n\\r\"), negative_lexicon))    \n",
    "        \n",
    "read_lexicon()\n",
    "opinion_lexicon = positive_lexicon + negative_lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CORENLP_HOME\"] = r'C:\\stanford-corenlp-full-2018-10-05'\n",
    "\n",
    "import corenlp \n",
    "client = corenlp.CoreNLPClient()\n",
    "\n",
    "to_be = ['be', 'am', 'is', 'are', 'was', 'were', 'been', 'being']\n",
    "\n",
    "def get_adjective(sentence):\n",
    "    text = sentence.lower()\n",
    "    res = []\n",
    "    try:\n",
    "        pattern = '[{pos:JJ}] | [{pos:JJR}] | [{pos:JJS}]'\n",
    "        matches = client.tokensregex(sentence, pattern)\n",
    "        response = matches['sentences'][0]\n",
    "        len_chunk = response['length']\n",
    "        if len_chunk > 0:\n",
    "            for index in range(0, len_chunk):\n",
    "                res.append(response[str(index)]['text'])\n",
    "    except:\n",
    "        print('err : ', sentence)\n",
    "        res = []\n",
    "    return res\n",
    "\n",
    "def get_verb(sentence):\n",
    "    text = sentence.lower()\n",
    "    res = []\n",
    "    try:\n",
    "        pattern = '[{pos:/VB.*/}]'\n",
    "        matches = client.tokensregex(sentence, pattern)\n",
    "        response = matches['sentences'][0]\n",
    "        len_chunk = response['length']\n",
    "        if len_chunk > 0:\n",
    "            for index in range(0, len_chunk):\n",
    "                word = response[str(index)]['text']\n",
    "                if word not in to_be:\n",
    "                    res.append(word)\n",
    "    except:\n",
    "        print('err : ', sentence)\n",
    "        res = []\n",
    "    return res\n",
    "\n",
    "def get_op_word(middle, opinions):\n",
    "    temp = 9999\n",
    "    res = ''\n",
    "    for word, index in opinions:\n",
    "        jarak = abs(index - middle)\n",
    "        if jarak <= temp:\n",
    "            res = word\n",
    "            temp = jarak\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def midpoint(p1, p2):\n",
    "    return int((p1+p2)/2)\n",
    "\n",
    "def rem_punct(word):\n",
    "    return word.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "def create_file(category):\n",
    "    sf = pd.DataFrame(columns = ['id','review', 'opinion', 'label', 'preprocessed'])\n",
    "    buang = 0\n",
    "    for index in range(0, len(df)):\n",
    "        if type(df['predict'][index]) == float:\n",
    "            continue\n",
    "        labels = df['label'][index].split(',')\n",
    "        predicts = df['predict'][index].split('|')\n",
    "        if category in labels and category in predicts:\n",
    "            kata_kata = []\n",
    "            tokens = df['review'][index].replace('-',' ').replace('  ', ' ').strip().lower().split(' ')\n",
    "            terms = df['term'][index].split('|')\n",
    "            if \"neutral\" in df['polarity'][index]:\n",
    "#                 print('neutral')\n",
    "                continue\n",
    "            \n",
    "            polarities = df['polarity'][index].split(',')\n",
    "\n",
    "            new_polar = []\n",
    "            for x, label in enumerate(labels):\n",
    "                if category == label:\n",
    "                    new_polar.append(polarities[x])\n",
    "            \n",
    "            new_terms = []\n",
    "            for x, predict in enumerate(predicts):\n",
    "                if category == predict:\n",
    "                    new_terms.append(terms[x])\n",
    "            \n",
    "            for term in new_terms:\n",
    "                i = 0\n",
    "                for token in tokens:\n",
    "                    if term in token:\n",
    "                        kata_kata.append(i)\n",
    "                    i += 1\n",
    "            middle = midpoint(min(kata_kata), max(kata_kata))\n",
    "            \n",
    "            opinion_pos = []\n",
    "            i = 0\n",
    "            for token in tokens:\n",
    "                if rem_punct(token) in opinion_lexicon:\n",
    "                    opinion_pos.append((rem_punct(token), i))\n",
    "                i += 1\n",
    "            \n",
    "            opinion_adj = []\n",
    "            if len(opinion_pos) == 0:\n",
    "                opinion_adj = get_adjective(df['review'][index].replace('-',' ').replace('  ', ' ').strip().lower())\n",
    "                i = 0\n",
    "                for token in tokens:\n",
    "                    for op in opinion_adj:\n",
    "                        if op in token:\n",
    "                            opinion_pos.append((rem_punct(token), i))\n",
    "                    i += 1\n",
    "                    \n",
    "            \n",
    "            opinion_vb = []\n",
    "            if len(opinion_pos) == 0:\n",
    "                opinion_vb = get_verb(df['review'][index].replace('-',' ').replace('  ', ' ').strip().lower())\n",
    "                i = 0\n",
    "                for token in tokens:\n",
    "                    for op in opinion_vb:\n",
    "                        if op in token:\n",
    "                            opinion_pos.append((rem_punct(token), i))\n",
    "                    i += 1\n",
    "                    \n",
    "            final_opinion = get_op_word(middle, opinion_pos)\n",
    "            \n",
    "            label_polar = ''\n",
    "            cp, cn = 0, 0\n",
    "            for polar in new_polar:\n",
    "                if polar == 'positive':\n",
    "                    cp += 1\n",
    "                elif polar == 'negative':\n",
    "                    cn += 1\n",
    "            if cp > cn:\n",
    "                label_polar = 'positive'\n",
    "            elif cp < cn:\n",
    "                label_polar = 'negative'\n",
    "            else:\n",
    "                buang += 1\n",
    "                continue\n",
    "            \n",
    "            sf = sf.append({'id': df['id'][index], \n",
    "                    'review': df['review'][index],\n",
    "                    'opinion': final_opinion,\n",
    "                    'label': label_polar,\n",
    "                    'preprocessed': preprocess(df['review'][index]),\n",
    "                   }, ignore_index=True)\n",
    "    print(buang)\n",
    "    sf.to_csv(\"Results/\"+ category +\".csv\")\n",
    "    sf.to_excel(\"Results/\"+ category +\".xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('Results/Categorization/lda2vec-partial-ner-elmo-money.csv')\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "err :  the signs, the specials menus, food, and even all the waitstaff are all totally japanese.\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "create_file('FOOD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "create_file('AMBIENCE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "create_file('SERVICE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "create_file('PRICES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'pizza',\n",
       " 'and',\n",
       " 'wine',\n",
       " 'were',\n",
       " 'excellent-the',\n",
       " 'service',\n",
       " 'too',\n",
       " '--',\n",
       " 'but',\n",
       " 'what',\n",
       " 'really',\n",
       " 'made',\n",
       " 'this',\n",
       " 'place',\n",
       " 'was',\n",
       " 'the',\n",
       " 'backyard',\n",
       " 'dining',\n",
       " 'area',\n",
       " '.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.parse.corenlp import CoreNLPParser\n",
    "st = CoreNLPParser()\n",
    "tokenized_sent = list(st.tokenize('the pizza and wine were excellent-the service too--but what really made this place was the backyard dining area.'))\n",
    "tokenized_sent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
