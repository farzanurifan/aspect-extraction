{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warming up PyWSD (takes ~10 secs)... took 17.34176516532898 secs.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import itertools, nltk, string \n",
    "import requests, re\n",
    "from nltk import Tree\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "stopWords = set(stopwords.words('english'))\n",
    "import os\n",
    "import spacy\n",
    "from gensim.models.wrappers import FastText\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from math import exp, expm1, log, log10\n",
    "from scipy.spatial.distance import euclidean\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "import ast\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import average_precision_score,accuracy_score, balanced_accuracy_score, f1_score, precision_score, recall_score\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import math\n",
    "import statistics\n",
    "from pywsd.lesk import simple_lesk\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.parse.corenlp import CoreNLPDependencyParser\n",
    "parser = CoreNLPDependencyParser()\n",
    "\n",
    "\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "snowball_stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_lexicon = []\n",
    "negative_lexicon = []\n",
    "\n",
    "def read_lexicon():\n",
    "    global positive_lexicon;\n",
    "    global negative_lexicon;\n",
    "    \n",
    "    with open(os.path.join(os.path.abspath('../opinion-lexicon-English/') , 'positive-words.txt'), 'r') as file:\n",
    "        line = file.readline();\n",
    "        while \";\" in line:\n",
    "            line = file.readline();\n",
    "         \n",
    "        positive_lexicon = file.readlines()\n",
    "    \n",
    "    with open(os.path.join(os.path.abspath('../opinion-lexicon-English/') , 'negative-words.txt'), 'r', encoding = \"ISO-8859-1\") as file:\n",
    "        line = file.readline();\n",
    "        while \";\" in line:\n",
    "            line = file.readline();\n",
    "        \n",
    "        negative_lexicon = file.readlines()\n",
    "        \n",
    "    positive_lexicon = list(map(lambda word: word.rstrip(\"\\n\\r\"), positive_lexicon))\n",
    "    negative_lexicon = list(map(lambda word: word.rstrip(\"\\n\\r\"), negative_lexicon))\n",
    "\n",
    "read_lexicon()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load function\n",
    "\n",
    "linking_verbs_be = [\n",
    "    'be',\n",
    "    'is',\n",
    "    'are',\n",
    "    'am',\n",
    "    'was',\n",
    "    'were',\n",
    "    'can be',\n",
    "    'could be',\n",
    "    'will be',\n",
    "    'would be',\n",
    "    'shall be',\n",
    "    'should be',\n",
    "    'may be',\n",
    "    'might be',\n",
    "    'must be',\n",
    "    'has been',\n",
    "    'have been',\n",
    "    'had been'\n",
    "];\n",
    "\n",
    "linking_verbs_v = [\n",
    "    'feel',\n",
    "    'look',\n",
    "    'smell',\n",
    "    'sound',\n",
    "    'taste',\n",
    "    'act',\n",
    "    'appear',\n",
    "    'become',\n",
    "    'get',\n",
    "    'grow',\n",
    "    'prove',\n",
    "    'remain', \n",
    "    'seem',\n",
    "    'stay',\n",
    "    'turn'\n",
    "];\n",
    "\n",
    "def check_is_noun(pos):\n",
    "    return re.match('NN.*', pos)\n",
    "\n",
    "def check_is_prp(pos):\n",
    "    return re.match('PRP.*', pos)\n",
    "\n",
    "def check_is_verb(pos):\n",
    "    return re.match('VB.*', pos)\n",
    "\n",
    "def check_is_adjective(pos):\n",
    "    return re.match('JJ.*', pos)\n",
    "\n",
    "def check_is_adverb(pos):\n",
    "    return re.match('RB.*', pos)\n",
    "\n",
    "def lemmatize(word, pos):\n",
    "    tag = None#wn.NOUN\n",
    "    if(check_is_noun(pos)):\n",
    "        tag = wn.NOUN\n",
    "    elif(check_is_verb(pos)):\n",
    "        tag = wn.VERB\n",
    "    elif(check_is_adjective(pos)):\n",
    "        tag = wn.ADJ\n",
    "    elif(check_is_adverb(pos)):\n",
    "        tag = wn.ADV\n",
    "    if tag:        \n",
    "        lemma = wordnet_lemmatizer.lemmatize(word, tag)\n",
    "    else:\n",
    "        lemma = word\n",
    "    return lemma\n",
    "\n",
    "\n",
    "def preprocessing(sentence):\n",
    "    res = sentence.replace(\"'m\", \"am\").replace(\"n't\", \" not\").replace(\"dont\", \"do not\").replace(\"’s\", ' is').replace(\"'s\", ' is').replace(\"'ve\", \" have\").lower()\n",
    "    res = re.sub(r'\\b\\d+\\b', 'NUM', res)\n",
    "    res = re.sub(r'(.)\\1{3,}', '', res)\n",
    "    to_delete_punc = set(string.punctuation) - {',', '.', '(', ')', '-'} # remove comma and fullstop\n",
    "    clean_tokens = [x for x in nltk.word_tokenize(res) if x not in to_delete_punc]\n",
    "\n",
    "    return \" \".join(clean_tokens)\n",
    "\n",
    "def preprocessing_with_lemma(sentence):\n",
    "    res = re.sub(r'[^\\w\\s]',' ', sentence.replace(\"'m\", \"am\").replace(\"n't\", \"not\").replace(\"dont\", \"do not\").replace(\"'ve\", \" have\").replace(\"'s\", ' is').replace(\"’s\", ' is')).lower()\n",
    "    res = re.sub(r'(.)\\1{3,}', '', res)\n",
    "   \n",
    "    #checking for parallel clauses\n",
    "    #splitted = res.split(', and but')\n",
    "    res = re.sub(r'\\b\\d+\\b', '', res)\n",
    "    tagged_words = pos_tag(res.encode())\n",
    "    lemma = []\n",
    "    for word, pos in tagged_words:\n",
    "        if word not in stopWords:\n",
    "            lemma.append(lemmatize(word, pos)) \n",
    "        \n",
    "    return \" \".join(lemma)\n",
    "    \n",
    "def pos_tag(sentence):\n",
    "    #url = \"http://localhost:9000\"\n",
    "    #request_params = {\"annotators\": \"pos\"}\n",
    "    #r = requests.post(url, data=sentence.encode('utf-8'), params=request_params, timeout=120)\n",
    "    #try:\n",
    "    #    results = r.json()['sentences'][0]['tokens']\n",
    "    #    res = []\n",
    "    #    for pos in results:\n",
    "    #        res.append((pos['word'], pos['pos']))\n",
    "    #    return res\n",
    "    #except Exception as e:\n",
    "    #    print(e)\n",
    "    #    return []\n",
    "    \n",
    "    return nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "    \n",
    "def get_tregex(text, tregex):\n",
    "    url = \"http://localhost:9000/tregex\"\n",
    "    request_params = {\"pattern\": tregex}\n",
    "    r = requests.post(url, data=text, params=request_params, timeout=120)\n",
    "    try:\n",
    "        return r.json()['sentences'][0]\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "def sentence_from_tree(s):\n",
    "    pattern = r'(?<= )[a-zA-Z,.].*?(?=\\))'\n",
    "    replaced = s.replace('\\r\\n', '')\n",
    "    res = ' '.join(re.findall(pattern, replaced))\n",
    "    return res\n",
    "        \n",
    "def sentence_type(clauses):\n",
    "    IC = 0\n",
    "    DC = 0\n",
    "    for clause in clauses:\n",
    "        if(clause[1] == 'IC'):\n",
    "            IC += 1\n",
    "        elif(clause[1] == 'DC'):\n",
    "            DC += 1\n",
    "\n",
    "    if IC == 1 and DC == 0:\n",
    "        return 'simple_sentence'\n",
    "    elif IC >= 2 and DC == 0:\n",
    "        return 'compound_sentence'\n",
    "    elif IC ==1 and DC >= 1:\n",
    "        return 'complex_sentence'\n",
    "    elif IC > 1 and DC >= 1:\n",
    "        return 'compound_complex_sentence'\n",
    "    else:\n",
    "        return 'phrase'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geometric_medianX(points, method='auto', options={}):\n",
    "    \"\"\"\n",
    "    Calculates the geometric median of an array of points.\n",
    "    method specifies which algorithm to use:\n",
    "        * 'auto' -- uses a heuristic to pick an algorithm\n",
    "        * 'minimize' -- scipy.optimize the sum of distances\n",
    "        * 'weiszfeld' -- Weiszfeld's algorithm\n",
    "    \"\"\"\n",
    "\n",
    "    points = np.asarray(points)\n",
    "\n",
    "    if len(points.shape) == 1:\n",
    "        # geometric_median((0, 0)) has too much potential for error.\n",
    "        # Did the user intend a single 2D point or two scalars?\n",
    "        # Use np.median if you meant the latter.\n",
    "        raise ValueError(\"Expected 2D array\")\n",
    "\n",
    "    if method == 'auto':\n",
    "        if points.shape[1] > 2:\n",
    "            # weiszfeld tends to converge faster in higher dimensions\n",
    "            method = 'weiszfeld'\n",
    "        else:\n",
    "            method = 'minimize'\n",
    "\n",
    "    return _methods[method](points, options)\n",
    "\n",
    "\n",
    "def minimize_method(points, options={}):\n",
    "    \"\"\"\n",
    "    Geometric median as a convex optimization problem.\n",
    "    \"\"\"\n",
    "\n",
    "    # objective function\n",
    "    def aggregate_distance(x):\n",
    "        return cdist([x], points).sum()\n",
    "\n",
    "    # initial guess: centroid\n",
    "    centroid = points.mean(axis=0)\n",
    "\n",
    "    optimize_result = minimize(aggregate_distance, centroid, method='COBYLA')\n",
    "\n",
    "    return optimize_result.x\n",
    "\n",
    "\n",
    "def weiszfeld_method(points, options={}):\n",
    "    \"\"\"\n",
    "    Weiszfeld's algorithm as described on Wikipedia.\n",
    "    \"\"\"\n",
    "\n",
    "    default_options = {'maxiter': 1000, 'tol': 1e-7}\n",
    "    default_options.update(options)\n",
    "    options = default_options\n",
    "\n",
    "    def distance_func(x):\n",
    "        return cdist([x], points)\n",
    "\n",
    "    # initial guess: centroid\n",
    "    guess = points.mean(axis=0)\n",
    "\n",
    "    iters = 0\n",
    "\n",
    "    while iters < options['maxiter']:\n",
    "        distances = distance_func(guess).T\n",
    "\n",
    "        # catch divide by zero\n",
    "        # TODO: Wikipedia cites how to deal with distance 0\n",
    "        distances = np.where(distances == 0, 1, distances)\n",
    "\n",
    "        guess_next = (points/distances).sum(axis=0) / (1./distances).sum(axis=0)\n",
    "\n",
    "        guess_movement = np.sqrt(((guess - guess_next)**2).sum())\n",
    "\n",
    "        guess = guess_next\n",
    "\n",
    "        if guess_movement <= options['tol']:\n",
    "            break\n",
    "\n",
    "        iters += 1\n",
    "\n",
    "    return guess\n",
    "\n",
    "\n",
    "_methods = {\n",
    "    'minimize': minimize_method,\n",
    "    'weiszfeld': weiszfeld_method,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph(radii, theta, key, show_pic=False):\n",
    "    i=0\n",
    "    a=[]\n",
    "    b=[]\n",
    "    rad=list(radii.values())\n",
    "    theta1=list(theta.values())\n",
    "    index = []\n",
    "    while(i<len(radii)):\n",
    "        if(theta1[i] != 0):\n",
    "            a.append(rad[i]*math.cos(math.radians(theta1[i])))\n",
    "            b.append(rad[i]*math.sin(math.radians(theta1[i])))\n",
    "            index.append(i)\n",
    "        i+=1 \n",
    "        \n",
    "    if show_pic:\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "        plt.plot([0,0],[-1,1])\n",
    "        plt.plot([-1,1],[0,0])\n",
    "        plt.axis([-1,1,-1,1])\n",
    "\n",
    "        plt.scatter(a,b,label=\"circles\",color=\"r\",marker=\"o\",s=10)\n",
    "        q = 0\n",
    "        rad2=list(radii.keys())\n",
    "        for i,j,z in zip(a,b, index):\n",
    "            ax.annotate('%s' %rad2[z], xy=(i,j), xytext=(15,0), textcoords='offset points')\n",
    "            #ax.annotate('%s' %'', xy=(i,j), xytext=(15,0), textcoords='offset points')\n",
    "            \n",
    "            q = q+1    \n",
    "        \n",
    "        points = []\n",
    "        for x,y in zip(a,b):\n",
    "            points.append([x, y])\n",
    "        \n",
    "        if(len(points) == 0):\n",
    "            points.append([0, 0])\n",
    "            \n",
    "        senti_x, senti_y = geometric_medianX(points)\n",
    "        plt.scatter(senti_x,senti_y,label=\"circles\",color=\"b\",marker=\"o\",s=10)\n",
    "        ax.annotate('%s' % '', xy=(senti_x,senti_y), xytext=(15,0), textcoords='offset points')\n",
    "           \n",
    "        ax.add_artist(plt.Circle((0,0),1.0,color='b',fill=False))\n",
    "        plt.xlabel('Sentiment Strength')\n",
    "        plt.ylabel('Orientation')\n",
    "        plt.title(key)\n",
    "        plt.savefig(\"graph.png\")\n",
    "    \n",
    "    return a,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "negation = [\n",
    "    \"afraid\",\n",
    "    \"can't\",\n",
    "    \"cannot\",\n",
    "    \"deny\",\n",
    "    \"mean\",\n",
    "    \"negate\",\n",
    "    \"negation\",\n",
    "    \"but\",\n",
    "    \"negative\",\n",
    "    \"neither\",\n",
    "    \"never\",\n",
    "    \"no\",\n",
    "    \"non\",\n",
    "    \"none\",\n",
    "    \"nor\",\n",
    "    \"not\",\n",
    "    \"nothing\",\n",
    "    \"refusal\",\n",
    "    \"refuse\",\n",
    "    \"reject\",\n",
    "    \"rejection\",\n",
    "  #  'even',\n",
    "  #  'although'\n",
    "]\n",
    "\n",
    "\n",
    "def analyse_file(key, lines, lemma_lines, usingSynonym=False, usingCoocurrence=False):    \n",
    "    radii, synonym_keys, coocurrence_keys = get_TDOC(lines, key, lemma_lines, usingSynonym, usingCoocurrence)    \n",
    "    return radii, synonym_keys, coocurrence_keys\n",
    "\n",
    "def get_TDOC(lines, key, lemma_lines, usingSynonym=False, usingCoocurrence=False, synonym_keys={}, coocurrence_keys={}):\n",
    "    freq = {'Init': 0}              #Number of times context term occurs with key\n",
    "    freq.clear()\n",
    "    prohib = ['even', 'main'] #stopWords\n",
    "    for line in lemma_lines:\n",
    "        words = line.split(\" \")\n",
    "        tagged = pos_tag(line)\n",
    "        if key in words:\n",
    "            if usingSynonym:\n",
    "                syn_c=[]\n",
    "                for context, pos in tagged:\n",
    "                    try:\n",
    "                        synonym_keys.setdefault(context, [])\n",
    "                                    \n",
    "                        for i, syn in enumerate(wn.synsets(context)):\n",
    "                            if i > 4:\n",
    "                                break\n",
    "                            \n",
    "                            if(syn.pos() != wn.NOUN):\n",
    "                                for l in syn.lemmas():\n",
    "                                    if not '-' in l.name():\n",
    "                                        if(l.name() not in synonym_keys[context]):\n",
    "                                            synonym_keys[context].append(l.name())\n",
    "                                        syn_c.append(l.name())\n",
    "\n",
    "                    except:\n",
    "                        #print(context)\n",
    "                        #print('error')\n",
    "                        pass\n",
    "                for c in syn_c:\n",
    "                    if c not in words:\n",
    "                        words.append(c)\n",
    "            for context in words:\n",
    "                flag=0\n",
    "                for i in prohib:\n",
    "                    if i == context:\n",
    "                        flag=1\n",
    "                        break\n",
    "                if flag==0 and context!=key:# and context in op_set:\n",
    "                    freq.setdefault(context, 0)\n",
    "                    freq[context] = freq.get(context) + 1\n",
    "                                           \n",
    "    N = 0                           #Total Number of terms in Document\n",
    "    for line in lemma_lines:\n",
    "        words = line.split(\" \")\n",
    "        N += len(words)\n",
    "    if usingCoocurrence:\n",
    "        di = {}\n",
    "        coocurrence_keys.setdefault(key, [])\n",
    "        it = 0\n",
    "        for key_n in freq.keys():\n",
    "            tg = pos_tag(key_n)\n",
    "            #if it > 5:\n",
    "            #    break;\n",
    "            if( check_is_adjective(tg[0][1]) or (key_n in positive_lexicon or key_n in negative_lexicon)):\n",
    "                for line in lemma_lines:\n",
    "                    words = line.split(\" \")\n",
    "                    tagged = pos_tag(line)\n",
    "                    \n",
    "                    if key_n in words:\n",
    "                        for x in range(0, len(words)):\n",
    "                            try:\n",
    "                                #if (words[x+1] == key_n) or (words[x+2] == key_n) or (words[x+3] == key_n) or (words[x-1] == key_n):\n",
    "                                if check_is_adjective(tagged[x][1]):\n",
    "                                    di.setdefault(words[x], 0)\n",
    "                                    di[words[x]] = di.get(words[x]) + 1\n",
    "                                    if words[x] not in coocurrence_keys[key]:\n",
    "                                        coocurrence_keys[key].append(words[x])\n",
    "                            except:\n",
    "                                pass\n",
    "               \n",
    "        freq.update(di)\n",
    "        \n",
    "    Nci = {'Init': 0}               #Total terms that occur with context term\n",
    "    Nci.clear()\n",
    "    for context in freq.keys():\n",
    "        for line in lemma_lines:\n",
    "            words = line.split(\" \")\n",
    "            if context in words:\n",
    "                Nci.setdefault(context, 0)\n",
    "                Nci[context] += len(words)\n",
    "                \n",
    "    if usingSynonym:\n",
    "        for context in freq.keys():\n",
    "            if context not in Nci.keys():\n",
    "                Nci.setdefault(context, 1)\n",
    "\n",
    "    radii = {'Init': 0}             #Get Radius of context term with TDOC formula\n",
    "    radii.clear()\n",
    "    df = pd.DataFrame(columns=['c', 'm', 'N', 'Nc', 'f', 'N/Nc', 'log(N/Nc)', 'fxlog(N/Nc)', '/4'])\n",
    "    max_value = 0\n",
    "    for term in freq.keys():\n",
    "        radii[term] = (freq[term]*(log(N/Nci[term])))\n",
    "        \n",
    "        if radii[term] > max_value:\n",
    "            max_value = radii[term]\n",
    "    for term in freq.keys():\n",
    "        radii[term] = radii[term]/max_value\n",
    "        \n",
    "        df = df.append({'c': term,\n",
    "                'm': key,\n",
    "                'N': N,\n",
    "                'Nc': Nci[term],\n",
    "                'f': freq[term],\n",
    "                'N/Nc': \"{0:.2f}\".format(N/Nci[term]),\n",
    "                'log(N/Nc)': \"{0:.2f}\".format(log(N/Nci[term])),\n",
    "                'fxlog(N/Nc)': \"{0:.2f}\".format(freq[term]*(log(N/Nci[term]))),\n",
    "                'normalisasi': \"{0:.2f}\".format((freq[term]*(log(N/Nci[term])))/max_value)\n",
    "               }, ignore_index=True)\n",
    "    \n",
    "    #df.to_excel(\"tdoc2.xlsx\")\n",
    "    return radii, synonym_keys, coocurrence_keys                  #Returns entire set of context terms related to key\n",
    "\n",
    "def get_theta(key, sentences):\n",
    "    scores = []\n",
    "    for sentence in sentences:\n",
    "        flag = True\n",
    "        \n",
    "        pp_tagged = pos_tag(sentence)\n",
    "        tagged = ('','')\n",
    "        for p in pp_tagged:\n",
    "            if p[0] == key:\n",
    "                tagged = p\n",
    "        if tagged == ('', ''):\n",
    "            flag = False\n",
    "        \n",
    "        ambiguous = tagged[0]\n",
    "        tag = tagged[1]\n",
    "        pos = ''\n",
    "\n",
    "        if 'NN' in tag or 'NNS' in tag:\n",
    "            pos = 'n'\n",
    "        elif 'VB' in tag:\n",
    "            pos = 'v'\n",
    "        elif 'VBG' in tag:\n",
    "            pos = 'v'\n",
    "        elif 'JJ' in tag:\n",
    "            pos = 'a'\n",
    "        elif 'RB' in tag:\n",
    "            pos = 'r'\n",
    "        else:\n",
    "            flag = False\n",
    "\n",
    "        if flag:\n",
    "            if ambiguous in negation or pos == 'n' or pos == 'v' or pos == 'r':\n",
    "                scores.append(0)\n",
    "                continue\n",
    "             \n",
    "            try:\n",
    "                 answer = simple_lesk(sentence, ambiguous, pos)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                answer = None\n",
    "                \n",
    "            if answer:\n",
    "                score = swn.senti_synset(answer.name())\n",
    "                endscore = 0\n",
    "                if score.pos_score() > score.neg_score():\n",
    "                    endscore = score.pos_score()\n",
    "                elif score.neg_score() > score.pos_score():\n",
    "                    endscore = score.neg_score() * (-1)\n",
    "                else:\n",
    "                    endscore = 0\n",
    "                \n",
    "                words = sentence.split(' ')\n",
    "                word_around = []\n",
    "                for x in range(0, len(words)):\n",
    "                    if words[x] in string.punctuation:\n",
    "                        continue\n",
    "                    try:\n",
    "                        if (words[x+1] == key) or (words[x+2] == key) or (words[x+3] == key) or (words[x+4] == key):\n",
    "                            word_around.append(words[x])\n",
    "                        elif (words[x-1] == key):\n",
    "                            word_around.append(words[x])\n",
    "                    except:\n",
    "                        pass\n",
    "                for neg in negation:\n",
    "                    if neg in word_around:\n",
    "                       # print('masuk', endscore, key, score)\n",
    "                        endscore *= (-1)\n",
    "                        break\n",
    "                        \n",
    "                scores.append(endscore)\n",
    "            else:\n",
    "                scores.append(0)\n",
    "        else:\n",
    "            scores.append(0)\n",
    "    final_score = np.average(scores)\n",
    "    \n",
    "    return np.pi * final_score\n",
    "\n",
    "def prior_sentiment(radii, key, all_sentences, sentence_lemmas, usingSynonym=False, usingCoocurrence=False, synonym_keys={}, coocurrence_keys={}):\n",
    "    theta = {'Init': 0}\n",
    "    theta.clear()\n",
    "    #print('masuk prior', synonym_keys, usingSynonym)\n",
    "    for word in radii.keys():\n",
    "        sentences = []\n",
    "        for sentence, sentence_lemma in zip(all_sentences, sentence_lemmas):\n",
    "            words = sentence_lemma.split(' ')\n",
    "            if key in words:\n",
    "                \n",
    "                if usingSynonym:\n",
    "                    child_syns = synonym_keys[key]\n",
    "                    \n",
    "                    if word in child_syns:\n",
    "                        words.append(word)\n",
    "                        sentence = sentence.replace(key, word)\n",
    "                        \n",
    "                   \n",
    "                if (word in words):\n",
    "                    sentences.append(sentence)\n",
    "                            \n",
    "            if usingCoocurrence:\n",
    "             \n",
    "                if word in words and word != key:\n",
    "                   # print(sentence, k)\n",
    "                    if sentence not in sentences:\n",
    "                        sentences.append(sentence)\n",
    "            #      \n",
    "            #            print(sentences, k, key)\n",
    "            #            sentences.append(sentence)\n",
    "                        \n",
    "        \n",
    "        #print(\"+++\", sentences, \"+++\")\n",
    "        if(len(sentences) > 0):\n",
    "           # print(word, sentences)\n",
    "            filter = get_theta(word, sentences)            #if function returns 0 word does not exist in lexicon\n",
    "            #print('hore')\n",
    "        else:\n",
    "            #print('kok bisa', word)\n",
    "            filter = 0.0\n",
    "        theta[word] = filter\n",
    "        #print('---', theta[word], '---')\n",
    "        \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def senti(keys, lines, lemma_lines, usingSynonym=False, usingCoocurrence=False):\n",
    "    points_k = []\n",
    "    for key in keys:\n",
    "        if key in memoize:\n",
    "            radii = memoize[key]['radii']\n",
    "            theta = memoize[key]['theta']\n",
    "        else:\n",
    "            radii, synonym_keys, coocurrence_keys = analyse_file(key, lines, lemma_lines, usingSynonym, usingCoocurrence)\n",
    "            #print(synonym_keys)\n",
    "            theta = prior_sentiment(radii, key, lines, lemma_lines, usingSynonym, usingCoocurrence, synonym_keys, coocurrence_keys)\n",
    "            memoize.setdefault(key, {})\n",
    "                   \n",
    "            memoize[key]['radii'] = radii\n",
    "            memoize[key]['theta'] = theta\n",
    "            \n",
    "        a,b = graph(radii, theta, key)\n",
    "        points = []\n",
    "        for x,y in zip(a,b):\n",
    "            points.append([x, y])\n",
    "        \n",
    "        if(len(points) == 0):\n",
    "            points.append([0, 0])\n",
    "        \n",
    "        senti_x, senti_y = geometric_medianX(points)\n",
    "        \n",
    "        points_k.append([senti_x, senti_y])\n",
    "    \n",
    "    sentiment_strength, sentiment_orientation = geometric_medianX(points_k)\n",
    "    #print(sentiment_strength, sentiment_orientation)\n",
    "    if sentiment_orientation > 0:\n",
    "        return 'positive', sentiment_orientation\n",
    "    else:\n",
    "        return 'negative', sentiment_orientation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_dataset(category, dataset):\n",
    "    return dataset[dataset.apply(lambda x: set([category]).issubset(ast.literal_eval(x['entity_prediction'])), axis=1)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_metrix_sentiment_senticircle(selected, dataset):\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0 \n",
    "    \n",
    "    for i, predictions in enumerate(dataset['opinion_prediction']):\n",
    "        target = dataset['Polarity'][i]\n",
    "        \n",
    "        entity_predictions =  ast.literal_eval(dataset['entity_prediction'][i])\n",
    "        #print(entity_predictions, predictions)\n",
    "        #print(entity_predictions)\n",
    "        entities = dataset['Kategori'][i].split(',')\n",
    "        pred_selected = None\n",
    "        actual_selected = None\n",
    "        \n",
    "        for index, pred in enumerate(predictions):\n",
    "            if entity_predictions[index].lower() == selected.lower():\n",
    "                if pred == 'positive':\n",
    "                    pred_selected = True\n",
    "                else:\n",
    "                    pred_selected = False\n",
    "        \n",
    "        for index, ent in enumerate(target.split(',')):\n",
    "            if entities[index].lower() == selected.lower():\n",
    "                if ent == 'positive':\n",
    "                    actual_selected = True\n",
    "                else:\n",
    "                    actual_selected = False\n",
    "        if pred_selected is None or actual_selected is None:\n",
    "            continue\n",
    "            \n",
    "        #print(pred_selected, actual_selected, entity_predictions, entities, target, predictions)\n",
    "        if pred_selected and actual_selected:\n",
    "            tp +=1\n",
    "        elif pred_selected and not actual_selected:   \n",
    "            fp += 1\n",
    "        elif not pred_selected and actual_selected:\n",
    "            fn += 1\n",
    "        elif not pred_selected and not actual_selected:\n",
    "            tn += 1\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    return tp, tn, fp, fn, precision, recall, ((precision * recall) / (precision + recall)) * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ac3 = pd.read_csv('ac3_tripadvisor.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_food = categorize_dataset('FOOD', data_ac3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_price = categorize_dataset('PRICE', data_ac3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ambience = categorize_dataset('AMBIENCE', data_ac3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_service = categorize_dataset('SERVICE', data_ac3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SA1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n"
     ]
    }
   ],
   "source": [
    "op_pred = []\n",
    "op_score = []\n",
    "memoize = {}\n",
    "for i, opinion_sents in enumerate(data_food['opinion_term_prediction']):\n",
    "    temp = []\n",
    "    temp1 = []\n",
    "    opinion_sents = ast.literal_eval(opinion_sents)\n",
    "    for opinions in opinion_sents:\n",
    "        polarity, senti_score = senti(opinions, data_food['preprocessed_sentence'].tolist(), data_food['preprocessed_sentence_lemma'].tolist())\n",
    "        temp.append(polarity)\n",
    "        temp1.append(senti_score)\n",
    "    op_pred.append(temp)\n",
    "    op_score.append(temp1)\n",
    "    #    op_pred.append(prediction_entity(sim_combined, category))\n",
    "    #y_pred.append(aspect_predictions)\n",
    "\n",
    "data_food['opinion_prediction'] = op_pred\n",
    "data_food['senti_score'] = op_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n"
     ]
    }
   ],
   "source": [
    "op_pred = []\n",
    "op_score = []\n",
    "memoize = {}\n",
    "for i, opinion_sents in enumerate(data_ambience['opinion_term_prediction']):\n",
    "    temp = []\n",
    "    temp1 = []\n",
    "    opinion_sents = ast.literal_eval(opinion_sents)\n",
    "    for opinions in opinion_sents:\n",
    "        polarity, senti_score = senti(opinions, data_ambience['preprocessed_sentence'].tolist(), data_ambience['preprocessed_sentence_lemma'].tolist())\n",
    "        temp.append(polarity)\n",
    "        temp1.append(senti_score)\n",
    "    op_pred.append(temp)\n",
    "    op_score.append(temp1)\n",
    "\n",
    "    #    op_pred.append(prediction_entity(sim_combined, category))\n",
    "    #y_pred.append(aspect_predictions)\n",
    "\n",
    "data_ambience['opinion_prediction'] = op_pred\n",
    "data_ambience['senti_score'] = op_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n"
     ]
    }
   ],
   "source": [
    "op_pred = []\n",
    "memoize = {}\n",
    "op_score = []\n",
    "\n",
    "\n",
    "for i, opinion_sents in enumerate(data_service['opinion_term_prediction']):\n",
    "    temp = []\n",
    "    temp1 = []\n",
    "    opinion_sents = ast.literal_eval(opinion_sents)\n",
    "    for opinions in opinion_sents:\n",
    "        polarity, senti_score = senti(opinions, data_service['preprocessed_sentence'].tolist(), data_service['preprocessed_sentence_lemma'].tolist())\n",
    "        temp.append(polarity)\n",
    "        temp1.append(senti_score)\n",
    "    op_pred.append(temp)\n",
    "    op_score.append(temp1)\n",
    "\n",
    "\n",
    "    #    op_pred.append(prediction_entity(sim_combined, category))\n",
    "    #y_pred.append(aspect_predictions)\n",
    "\n",
    "data_service['opinion_prediction'] = op_pred\n",
    "data_service['senti_score'] = op_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "op_pred = []\n",
    "memoize = {}\n",
    "op_score = []\n",
    "\n",
    "\n",
    "for i, opinion_sents in enumerate(data_price['opinion_term_prediction']):\n",
    "    temp = []\n",
    "    temp1 = []\n",
    "    opinion_sents = ast.literal_eval(opinion_sents)\n",
    "    for opinions in opinion_sents:\n",
    "        polarity, senti_score = senti(opinions, data_price['preprocessed_sentence'].tolist(), data_price['preprocessed_sentence_lemma'].tolist())\n",
    "        temp.append(polarity)\n",
    "        temp1.append(senti_score)\n",
    "    op_pred.append(temp)\n",
    "    op_score.append(temp1)\n",
    "\n",
    "\n",
    "    #    op_pred.append(prediction_entity(sim_combined, category))\n",
    "    #y_pred.append(aspect_predictions)\n",
    "\n",
    "data_price['opinion_prediction'] = op_pred\n",
    "data_price['senti_score'] = op_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "sentiment_price = (confusion_metrix_sentiment_senticircle('PRICE', data_price)[6]) #+ confusion_metrix_sentiment_senticircle('negative', data_value)[6] ) / 2\n",
    "print(sentiment_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8712871287128713\n"
     ]
    }
   ],
   "source": [
    "sentiment_service = (confusion_metrix_sentiment_senticircle('SERVICE', data_service)[6]) #+ confusion_metrix_sentiment_senticircle('negative', data_value)[6] ) / 2\n",
    "print(sentiment_service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9454545454545454\n"
     ]
    }
   ],
   "source": [
    "sentiment_ambience = (confusion_metrix_sentiment_senticircle('AMBIENCE', data_ambience)[6]) #+ confusion_metrix_sentiment_senticircle('negative', data_value)[6] ) / 2\n",
    "print(sentiment_ambience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8695652173913043\n"
     ]
    }
   ],
   "source": [
    "sentiment_food = (confusion_metrix_sentiment_senticircle('FOOD', data_food)[6]) #+ confusion_metrix_sentiment_senticircle('negative', data_value)[6] ) / 2\n",
    "print(sentiment_food)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7965767228896802"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sentiment_price + sentiment_ambience + sentiment_food + sentiment_service) / 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SA2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n"
     ]
    }
   ],
   "source": [
    "op_pred = []\n",
    "op_score = []\n",
    "memoize = {}\n",
    "for i, opinion_sents in enumerate(data_food['opinion_term_prediction']):\n",
    "    temp = []\n",
    "    temp1 = []\n",
    "    opinion_sents = ast.literal_eval(opinion_sents)\n",
    "    for opinions in opinion_sents:\n",
    "        polarity, senti_score =  senti(opinions, data_food['preprocessed_sentence'].tolist(), data_food['preprocessed_sentence_lemma'].tolist(), True)\n",
    "        temp.append(polarity)\n",
    "        temp1.append(senti_score)\n",
    "    op_pred.append(temp)\n",
    "    op_score.append(temp1)\n",
    "\n",
    "    #    op_pred.append(prediction_entity(sim_combined, category))\n",
    "    #y_pred.append(aspect_predictions)\n",
    "\n",
    "data_food['opinion_prediction'] = op_pred\n",
    "data_food['senti_score'] = op_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n"
     ]
    }
   ],
   "source": [
    "op_pred = []\n",
    "op_score = []\n",
    "\n",
    "memoize = {}\n",
    "for i, opinion_sents in enumerate(data_ambience['opinion_term_prediction']):\n",
    "    temp = []\n",
    "    temp1 = []\n",
    "    \n",
    "    opinion_sents = ast.literal_eval(opinion_sents)\n",
    "    for opinions in opinion_sents:\n",
    "        polarity, senti_score = senti(opinions, data_ambience['preprocessed_sentence'].tolist(), data_ambience['preprocessed_sentence_lemma'].tolist(), True)\n",
    "        temp.append(polarity)\n",
    "        temp1.append(senti_score)\n",
    "    op_pred.append(temp)\n",
    "    op_score.append(temp1)\n",
    "\n",
    "    #    op_pred.append(prediction_entity(sim_combined, category))\n",
    "    #y_pred.append(aspect_predictions)\n",
    "\n",
    "data_ambience['opinion_prediction'] = op_pred\n",
    "data_ambience['senti_score'] = op_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n"
     ]
    }
   ],
   "source": [
    "op_pred = []\n",
    "op_score = []\n",
    "\n",
    "memoize = {}\n",
    "for i, opinion_sents in enumerate(data_price['opinion_term_prediction']):\n",
    "    temp = []\n",
    "    temp1 = []\n",
    "    \n",
    "    opinion_sents = ast.literal_eval(opinion_sents)\n",
    "    for opinions in opinion_sents:\n",
    "        polarity, senti_score = senti(opinions, data_price['preprocessed_sentence'].tolist(), data_price['preprocessed_sentence_lemma'].tolist(), True)\n",
    "        temp.append(polarity)\n",
    "        temp1.append(senti_score)\n",
    "    op_pred.append(temp)\n",
    "    op_score.append(temp1)\n",
    "    #    op_pred.append(prediction_entity(sim_combined, category))\n",
    "    #y_pred.append(aspect_predictions)\n",
    "\n",
    "data_price['opinion_prediction'] = op_pred\n",
    "data_price['senti_score'] = op_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n"
     ]
    }
   ],
   "source": [
    "op_pred = []\n",
    "op_score = []\n",
    "\n",
    "\n",
    "memoize = {}\n",
    "for i, opinion_sents in enumerate(data_service['opinion_term_prediction']):\n",
    "    temp = []\n",
    "    temp1 = []\n",
    "    opinion_sents = ast.literal_eval(opinion_sents)\n",
    "    for opinions in opinion_sents:\n",
    "        polarity, senti_score = senti(opinions, data_service['preprocessed_sentence'].tolist(), data_service['preprocessed_sentence_lemma'].tolist(), True)\n",
    "        temp.append(polarity)\n",
    "        temp1.append(senti_score)\n",
    "    op_pred.append(temp)\n",
    "    op_score.append(temp1)\n",
    "    #    op_pred.append(prediction_entity(sim_combined, category))\n",
    "    #y_pred.append(aspect_predictions)\n",
    "\n",
    "data_service['opinion_prediction'] = op_pred\n",
    "data_service['senti_score'] = op_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7142857142857143\n"
     ]
    }
   ],
   "source": [
    "sentiment_price = (confusion_metrix_sentiment_senticircle('PRICE', data_price)[6]) #+ confusion_metrix_sentiment_senticircle('negative', data_value)[6] ) / 2\n",
    "print(sentiment_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9357798165137614\n"
     ]
    }
   ],
   "source": [
    "sentiment_ambience = (confusion_metrix_sentiment_senticircle('AMBIENCE', data_ambience)[6])# + confusion_metrix_sentiment_senticircle('negative', data_ambience)[6] ) / 2\n",
    "print(sentiment_ambience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8828125\n"
     ]
    }
   ],
   "source": [
    "sentiment_food = (confusion_metrix_sentiment_senticircle('FOOD', data_food)[6])# + confusion_metrix_sentiment_senticircle('negative', data_food)[6] ) / 2\n",
    "print(sentiment_food)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8979591836734694\n"
     ]
    }
   ],
   "source": [
    "sentiment_service = (confusion_metrix_sentiment_senticircle('SERVICE', data_service)[6])# + confusion_metrix_sentiment_senticircle('negative', data_service)[6] ) / 2\n",
    "print(sentiment_service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8577093036182363"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sentiment_price + sentiment_ambience + sentiment_food + sentiment_service) / 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SA3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n"
     ]
    }
   ],
   "source": [
    "op_pred = []\n",
    "op_score = []\n",
    "memoize = {}\n",
    "for i, opinion_sents in enumerate(data_food['opinion_term_prediction']):\n",
    "    temp = []\n",
    "    temp_score = []\n",
    "    opinion_sents = ast.literal_eval(opinion_sents)\n",
    "    for opinions in opinion_sents:\n",
    "        polarity, senti_score = senti(opinions, data_food['preprocessed_sentence'].tolist(), data_food['preprocessed_sentence_lemma'].tolist(), True, True)\n",
    "        temp.append(polarity)\n",
    "        temp_score.append(senti_score)\n",
    "    op_pred.append(temp)\n",
    "    op_score.append(temp_score)\n",
    "\n",
    "    #    op_pred.append(prediction_entity(sim_combined, category))\n",
    "    #y_pred.append(aspect_predictions)\n",
    "\n",
    "data_food['opinion_prediction'] = op_pred\n",
    "data_food['senti_score'] = op_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n"
     ]
    }
   ],
   "source": [
    "op_pred = []\n",
    "op_score = []\n",
    "memoize = {}\n",
    "for i, opinion_sents in enumerate(data_service['opinion_term_prediction']):\n",
    "    temp = []\n",
    "    temp_score = []\n",
    "    opinion_sents = ast.literal_eval(opinion_sents)\n",
    "    for opinions in opinion_sents:\n",
    "        polarity, senti_score = senti(opinions, data_service['preprocessed_sentence'].tolist(), data_service['preprocessed_sentence_lemma'].tolist(), True, True)\n",
    "        temp.append(polarity)\n",
    "        temp_score.append(senti_score)\n",
    "    op_pred.append(temp)\n",
    "    op_score.append(temp_score)\n",
    "\n",
    "    #    op_pred.append(prediction_entity(sim_combined, category))\n",
    "    #y_pred.append(aspect_predictions)\n",
    "\n",
    "data_service['opinion_prediction'] = op_pred\n",
    "data_service['senti_score'] = op_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-104-ab97324cfe49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mopinion_sents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mliteral_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopinion_sents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mopinions\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mopinion_sents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mpolarity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msenti_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msenti\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopinions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_ambience\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'preprocessed_sentence'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_ambience\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'preprocessed_sentence_lemma'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mtemp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolarity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mtemp_score\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msenti_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-eca1b77c0013>\u001b[0m in \u001b[0;36msenti\u001b[0;34m(keys, lines, lemma_lines, usingSynonym, usingCoocurrence)\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mtheta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemoize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'theta'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0mradii\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynonym_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoocurrence_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalyse_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlemma_lines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musingSynonym\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musingCoocurrence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0;31m#print(synonym_keys)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mtheta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprior_sentiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mradii\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlemma_lines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musingSynonym\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musingCoocurrence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynonym_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoocurrence_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-c7ffb56b7434>\u001b[0m in \u001b[0;36manalyse_file\u001b[0;34m(key, lines, lemma_lines, usingSynonym, usingCoocurrence)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0manalyse_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlemma_lines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musingSynonym\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musingCoocurrence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mradii\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynonym_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoocurrence_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_TDOC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlemma_lines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musingSynonym\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musingCoocurrence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mradii\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynonym_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoocurrence_keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-c7ffb56b7434>\u001b[0m in \u001b[0;36mget_TDOC\u001b[0;34m(lines, key, lemma_lines, usingSynonym, usingCoocurrence, synonym_keys, coocurrence_keys)\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlemma_lines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                     \u001b[0mtagged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mkey_n\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-101-7cc4b5110c5c>\u001b[0m in \u001b[0;36mpos_tag\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m#    return []\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_tregex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtregex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36mpos_tag\u001b[0;34m(tokens, tagset, lang)\u001b[0m\n\u001b[1;32m    160\u001b[0m     \"\"\"\n\u001b[1;32m    161\u001b[0m     \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_tagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36m_pos_tag\u001b[0;34m(tokens, tagset, tagger, lang)\u001b[0m\n\u001b[1;32m    117\u001b[0m         )\n\u001b[1;32m    118\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mtagged_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Maps to the specified tagset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlang\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'eng'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/nltk/tag/perceptron.py\u001b[0m in \u001b[0;36mtag\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m                 \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m                 \u001b[0mtag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0mprev2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprev\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/nltk/tag/perceptron.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;31m# Do a secondary alphabetic sort, for stability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mguess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "op_pred = []\n",
    "op_score = []\n",
    "memoize = {}\n",
    "for i, opinion_sents in enumerate(data_ambience['opinion_term_prediction']):\n",
    "    temp = []\n",
    "    temp_score = []\n",
    "    opinion_sents = ast.literal_eval(opinion_sents)\n",
    "    for opinions in opinion_sents:\n",
    "        polarity, senti_score = senti(opinions, data_ambience['preprocessed_sentence'].tolist(), data_ambience['preprocessed_sentence_lemma'].tolist(), True, True)\n",
    "        temp.append(polarity)\n",
    "        temp_score.append(senti_score)\n",
    "    op_pred.append(temp)\n",
    "    op_score.append(temp_score)\n",
    "\n",
    "    #    op_pred.append(prediction_entity(sim_combined, category))\n",
    "    #y_pred.append(aspect_predictions)\n",
    "\n",
    "data_ambience['opinion_prediction'] = op_pred\n",
    "data_ambience['senti_score'] = op_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op_pred = []\n",
    "op_score = []\n",
    "memoize = {}\n",
    "for i, opinion_sents in enumerate(data_price['opinion_term_prediction']):\n",
    "    temp = []\n",
    "    temp_score = []\n",
    "    opinion_sents = ast.literal_eval(opinion_sents)\n",
    "    for opinions in opinion_sents:\n",
    "        polarity, senti_score = senti(opinions, data_price['preprocessed_sentence'].tolist(), data_price['preprocessed_sentence_lemma'].tolist(), True, True)\n",
    "        temp.append(polarity)\n",
    "        temp_score.append(senti_score)\n",
    "    op_pred.append(temp)\n",
    "    op_score.append(temp_score)\n",
    "\n",
    "    #    op_pred.append(prediction_entity(sim_combined, category))\n",
    "    #y_pred.append(aspect_predictions)\n",
    "\n",
    "data_price['opinion_prediction'] = op_pred\n",
    "data_price['senti_score'] = op_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_price = (confusion_metrix_sentiment_senticircle('PRICE', data_price)[6]) #+ confusion_metrix_sentiment_senticircle('negative', data_value)[6] ) / 2\n",
    "print(sentiment_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_ambience = (confusion_metrix_sentiment_senticircle('AMBIENCE', data_ambience)[6])# + confusion_metrix_sentiment_senticircle('negative', data_ambience)[6] ) / 2\n",
    "print(sentiment_ambience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_food = (confusion_metrix_sentiment_senticircle('FOOD', data_food)[6])# + confusion_metrix_sentiment_senticircle('negative', data_food)[6] ) / 2\n",
    "print(sentiment_food)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_service = (confusion_metrix_sentiment_senticircle('SERVICE', data_service)[6])# + confusion_metrix_sentiment_senticircle('negative', data_service)[6] ) / 2\n",
    "print(sentiment_service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(sentiment_price + sentiment_ambience + sentiment_food + sentiment_service) / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((confusion_metrix_sentiment_senticircle('PRICE', data_price)[4]) + (confusion_metrix_sentiment_senticircle('FOOD', data_food)[4]) + (confusion_metrix_sentiment_senticircle('AMBIENCE', data_ambience)[4]) + (confusion_metrix_sentiment_senticircle('SERVICE', data_service)[4])) / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((confusion_metrix_sentiment_senticircle('PRICE', data_price)[5]) + (confusion_metrix_sentiment_senticircle('FOOD', data_food)[5]) + (confusion_metrix_sentiment_senticircle('AMBIENCE', data_ambience)[5]) + (confusion_metrix_sentiment_senticircle('SERVICE', data_service)[5])) / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_food.to_csv('SA3_food_tripadvisor.csv')\n",
    "data_ambience.to_csv('SA3_ambience_tripadvisor.csv')\n",
    "data_price.to_csv('SA3_price_tripadvisor.csv')\n",
    "data_service.to_csv('SA3_service_tripadvisor.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sentiwordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiwordnet(opinions, sent):\n",
    "    endscore = 0\n",
    "    for opinion in opinions:\n",
    "        try:\n",
    "            score = swn.senti_synset(opinion + '.a.1')\n",
    "            if score.pos_score() > score.neg_score():\n",
    "                endscore += score.pos_score()\n",
    "            else:\n",
    "                endscore += score.neg_score() * (-1)\n",
    "            \n",
    "            words = sent.split(' ')\n",
    "            word_around = []\n",
    "            for x in range(0, len(words)):\n",
    "                if words[x] in string.punctuation:\n",
    "                    continue\n",
    "                try:\n",
    "                    if (words[x+1] == opinion) or (words[x+2] == opinion) or (words[x+3] == opinion) or (words[x+4] == opinion):\n",
    "                        word_around.append(words[x])\n",
    "                    elif (words[x-1] == opinion):\n",
    "                        word_around.append(words[x])\n",
    "                except:\n",
    "                    pass\n",
    "            for neg in negation:\n",
    "                if neg in word_around:\n",
    "                    endscore *= (-1)\n",
    "                    break\n",
    "        except:\n",
    "            pass\n",
    "    if endscore > 0:\n",
    "        polarity = 'positive'\n",
    "    else:\n",
    "        polarity = 'negative'\n",
    "           \n",
    "    return polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "op_pred = []\n",
    "for i, opinion_sents in enumerate(data_food['opinion_term_prediction']):\n",
    "    temp = []\n",
    "    review = data_food['Review'][i]\n",
    "    opinion_sents = ast.literal_eval(opinion_sents)\n",
    "    \n",
    "    for opinions in opinion_sents:\n",
    "        temp.append(predict_sentiwordnet(opinions, review))\n",
    "    op_pred.append(temp)\n",
    "\n",
    "    #    op_pred.append(prediction_entity(sim_combined, category))\n",
    "    #y_pred.append(aspect_predictions)\n",
    "\n",
    "data_food['opinion_prediction'] = op_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "op_pred = []\n",
    "for i, opinion_sents in enumerate(data_service['opinion_term_prediction']):\n",
    "    temp = []\n",
    "    review = data_service['Review'][i]\n",
    "    opinion_sents = ast.literal_eval(opinion_sents)\n",
    "    \n",
    "    for opinions in opinion_sents:\n",
    "        temp.append(predict_sentiwordnet(opinions, review))\n",
    "    op_pred.append(temp)\n",
    "\n",
    "    #    op_pred.append(prediction_entity(sim_combined, category))\n",
    "    #y_pred.append(aspect_predictions)\n",
    "\n",
    "data_service['opinion_prediction'] = op_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "op_pred = []\n",
    "for i, opinion_sents in enumerate(data_ambience['opinion_term_prediction']):\n",
    "    temp = []\n",
    "    review = data_ambience['Review'][i]\n",
    "    opinion_sents = ast.literal_eval(opinion_sents)\n",
    "    \n",
    "    for opinions in opinion_sents:\n",
    "        temp.append(predict_sentiwordnet(opinions, review))\n",
    "    op_pred.append(temp)\n",
    "\n",
    "    #    op_pred.append(prediction_entity(sim_combined, category))\n",
    "    #y_pred.append(aspect_predictions)\n",
    "\n",
    "data_ambience['opinion_prediction'] = op_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "op_pred = []\n",
    "for i, opinion_sents in enumerate(data_price['opinion_term_prediction']):\n",
    "    temp = []\n",
    "    review = data_price['Review'][i]\n",
    "    opinion_sents = ast.literal_eval(opinion_sents)\n",
    "    \n",
    "    for opinions in opinion_sents:\n",
    "        temp.append(predict_sentiwordnet(opinions, review))\n",
    "    op_pred.append(temp)\n",
    "\n",
    "    #    op_pred.append(prediction_entity(sim_combined, category))\n",
    "    #y_pred.append(aspect_predictions)\n",
    "\n",
    "data_price['opinion_prediction'] = op_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8546349434082576"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((confusion_metrix_sentiment_senticircle('PRICE', data_price)[4]) + (confusion_metrix_sentiment_senticircle('FOOD', data_food)[4]) + (confusion_metrix_sentiment_senticircle('AMBIENCE', data_ambience)[4]) + (confusion_metrix_sentiment_senticircle('SERVICE', data_service)[4])) / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7046889966475096"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((confusion_metrix_sentiment_senticircle('PRICE', data_price)[5]) + (confusion_metrix_sentiment_senticircle('FOOD', data_food)[5]) + (confusion_metrix_sentiment_senticircle('AMBIENCE', data_ambience)[5]) + (confusion_metrix_sentiment_senticircle('SERVICE', data_service)[5])) / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7701639255278727"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((confusion_metrix_sentiment_senticircle('PRICE', data_price)[6]) + (confusion_metrix_sentiment_senticircle('FOOD', data_food)[6]) + (confusion_metrix_sentiment_senticircle('AMBIENCE', data_ambience)[6]) + (confusion_metrix_sentiment_senticircle('SERVICE', data_service)[6])) / 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SentiWordnet + WSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiwordnet_lesk(opinions, sent):\n",
    "    endscore = 0\n",
    "    for opinion in opinions:\n",
    "        try:\n",
    "            op_with_tag = pos_tag(opinion)[0]\n",
    "            if check_is_noun(op_with_tag[1]):\n",
    "                score = swn.senti_synset(simple_lesk(sent, opinion, pos='n').name())\n",
    "            elif check_is_adjective(op_with_tag[1]):\n",
    "                score = swn.senti_synset(simple_lesk(sent, opinion, pos='a').name())\n",
    "            elif check_is_verb(op_with_tag[1]):\n",
    "                score = swn.senti_synset(simple_lesk(sent, opinion, pos='v').name())\n",
    "            elif check_is_adverb(op_with_tag[1]):\n",
    "                score = swn.senti_synset(simple_lesk(sent, opinion, pos='r').name())\n",
    "            else:\n",
    "                score = swn.senti_synset(simple_lesk(sent, opinion, pos='n').name())\n",
    "                \n",
    "            if score.pos_score() > score.neg_score():\n",
    "                endscore += score.pos_score()\n",
    "            else:\n",
    "                endscore += score.neg_score() * (-1)\n",
    "                \n",
    "            words = sent.split(' ')\n",
    "            word_around = []\n",
    "            for x in range(0, len(words)):\n",
    "                if words[x] in string.punctuation:\n",
    "                    continue\n",
    "                try:\n",
    "                    if (words[x+1] == opinion) or (words[x+2] == opinion) or (words[x+3] == opinion) or (words[x+4] == opinion):\n",
    "                        word_around.append(words[x])\n",
    "                    elif (words[x-1] == opinion):\n",
    "                        word_around.append(words[x])\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            for neg in negation:\n",
    "                if neg in word_around:\n",
    "                    endscore *= (-1)\n",
    "                    break\n",
    "           \n",
    "        except Exception as e:\n",
    "            #print(e)\n",
    "            #print(opinion, sent, op_with_tag)\n",
    "            pass\n",
    "            \n",
    "    if endscore > 0:\n",
    "        polarity = 'positive'\n",
    "    else:\n",
    "        polarity = 'negative'\n",
    "           \n",
    "    return polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "op_pred = []\n",
    "for i, opinion_sents in enumerate(data_food['opinion_term_prediction']):\n",
    "    temp = []\n",
    "    review = data_food['Review'][i]\n",
    "    opinion_sents = ast.literal_eval(opinion_sents)\n",
    "    \n",
    "    for opinions in opinion_sents:\n",
    "        temp.append(predict_sentiwordnet_lesk(opinions, review))\n",
    "    op_pred.append(temp)\n",
    "\n",
    "    #    op_pred.append(prediction_entity(sim_combined, category))\n",
    "    #y_pred.append(aspect_predictions)\n",
    "\n",
    "data_food['opinion_prediction'] = op_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "op_pred = []\n",
    "for i, opinion_sents in enumerate(data_service['opinion_term_prediction']):\n",
    "    temp = []\n",
    "    review = data_service['Review'][i]\n",
    "    opinion_sents = ast.literal_eval(opinion_sents)\n",
    "    \n",
    "    for opinions in opinion_sents:\n",
    "        temp.append(predict_sentiwordnet_lesk(opinions, review))\n",
    "    op_pred.append(temp)\n",
    "\n",
    "    #    op_pred.append(prediction_entity(sim_combined, category))\n",
    "    #y_pred.append(aspect_predictions)\n",
    "\n",
    "data_service['opinion_prediction'] = op_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "op_pred = []\n",
    "for i, opinion_sents in enumerate(data_ambience['opinion_term_prediction']):\n",
    "    temp = []\n",
    "    review = data_ambience['Review'][i]\n",
    "    opinion_sents = ast.literal_eval(opinion_sents)\n",
    "    \n",
    "    for opinions in opinion_sents:\n",
    "        temp.append(predict_sentiwordnet_lesk(opinions, review))\n",
    "    op_pred.append(temp)\n",
    "\n",
    "    #    op_pred.append(prediction_entity(sim_combined, category))\n",
    "    #y_pred.append(aspect_predictions)\n",
    "\n",
    "data_ambience['opinion_prediction'] = op_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "op_pred = []\n",
    "for i, opinion_sents in enumerate(data_price['opinion_term_prediction']):\n",
    "    temp = []\n",
    "    review = data_price['Review'][i]\n",
    "    opinion_sents = ast.literal_eval(opinion_sents)\n",
    "    \n",
    "    for opinions in opinion_sents:\n",
    "        temp.append(predict_sentiwordnet_lesk(opinions, review))\n",
    "    op_pred.append(temp)\n",
    "\n",
    "    #    op_pred.append(prediction_entity(sim_combined, category))\n",
    "    #y_pred.append(aspect_predictions)\n",
    "\n",
    "data_price['opinion_prediction'] = op_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80324263502455"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((confusion_metrix_sentiment_senticircle('PRICE', data_price)[4]) + (confusion_metrix_sentiment_senticircle('FOOD', data_food)[4]) + (confusion_metrix_sentiment_senticircle('AMBIENCE', data_ambience)[4]) + (confusion_metrix_sentiment_senticircle('SERVICE', data_service)[4])) / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6801364942528736"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((confusion_metrix_sentiment_senticircle('PRICE', data_price)[5]) + (confusion_metrix_sentiment_senticircle('FOOD', data_food)[5]) + (confusion_metrix_sentiment_senticircle('AMBIENCE', data_ambience)[5]) + (confusion_metrix_sentiment_senticircle('SERVICE', data_service)[5])) / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7343189224441821"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((confusion_metrix_sentiment_senticircle('PRICE', data_price)[6]) + (confusion_metrix_sentiment_senticircle('FOOD', data_food)[6]) + (confusion_metrix_sentiment_senticircle('AMBIENCE', data_ambience)[6]) + (confusion_metrix_sentiment_senticircle('SERVICE', data_service)[6])) / 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## perhitungan aspect bases sentiment analisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sa3_food = pd.read_csv('SA3_food_tripadvisor.csv')\n",
    "data_sa3_ambience = pd.read_csv('SA3_ambience_tripadvisor.csv')\n",
    "data_sa3_service = pd.read_csv('SA3_service_tripadvisor.csv')\n",
    "data_sa3_price = pd.read_csv('SA3_price_tripadvisor.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "food"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_pos = 0\n",
    "total_neg = 0\n",
    "\n",
    "for i, predictions in enumerate(data_sa3_food['opinion_prediction']):\n",
    "    entity_predictions =  ast.literal_eval(data_sa3_food['entity_prediction'][i])\n",
    "    entities = data_sa3_food['entity'][i].split(',')\n",
    "    predictions = ast.literal_eval(predictions)\n",
    "    for index, pred in enumerate(predictions):\n",
    "       # print(pred)\n",
    "        if entity_predictions[index] == 'FOOD':\n",
    "            if pred == 'positive':\n",
    "                total_pos += 1\n",
    "            else:\n",
    "                total_neg += 1\n",
    "\n",
    "print(total_pos, total_neg)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
